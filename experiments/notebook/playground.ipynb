{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m sys\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mappend(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mabspath(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../..\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01margparse\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msfl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FLConfig\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msfl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexp\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_model_and_tokenizer\n\u001B[1;32m      9\u001B[0m config \u001B[38;5;241m=\u001B[39m FLConfig(\n\u001B[1;32m     10\u001B[0m     collect_intermediates\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     11\u001B[0m     global_round\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     22\u001B[0m     client_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m700\u001B[39m\n\u001B[1;32m     23\u001B[0m )\n",
      "File \u001B[0;32m~/project/pythonProject/SFL-LLM/sfl/config.py:3\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdataclasses\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dataclass\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PretrainedConfig\n\u001B[1;32m      5\u001B[0m data_root \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/data/stupidtree/data\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      7\u001B[0m dataset_cache_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_root\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/sfl/datasets/\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "import argparse\n",
    "from sfl.config import FLConfig\n",
    "from sfl.utils.exp import get_model_and_tokenizer\n",
    "\n",
    "config = FLConfig(\n",
    "    collect_intermediates=False,\n",
    "    global_round=10,\n",
    "    client_evaluate_freq=500,\n",
    "    client_epoch=1,  # 每轮联邦每个Client训2轮\n",
    "    split_point_1=6,\n",
    "    split_point_2=30,  # [0,1 | 2,3,.... 29| 30, 31]\n",
    "    use_lora_at_trunk=True,  # 在trunk部分使用LoRA\n",
    "    use_lora_at_top=False,\n",
    "    use_lora_at_bottom=False,\n",
    "    use_lora_at_embed=True,\n",
    "    top_and_bottom_from_scratch='True',\n",
    "    attack_mode='b2tr',\n",
    "    client_steps=700\n",
    ")\n",
    "\n",
    "args = {\n",
    "    'dataset_train_frac': 1.0,\n",
    "    'dataset_test_frac': 0.1,\n",
    "    'dataset': 'piqa',\n",
    "    'model_name': 'llama2',\n",
    "    'save_checkpoint': True,\n",
    "    'task_type': 'lm',\n",
    "    'attacker_freq': 10,\n",
    "    'attacker_samples':2,\n",
    "    'log_to_wandb': False\n",
    "}\n",
    "# convert to namespace\n",
    "args = argparse.Namespace(**args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "md,tok = get_model_and_tokenizer('gpt2-large')\n",
    "\n",
    "md.config_sfl(config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sfl/lib/python3.11/site-packages/peft/tuners/lora.py:299: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# md.print_split_model()\n",
    "md_lr = md.convert_to_lora_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================Split-/root/autodl-tmp/sfl/models/gpt2-large=================\n",
      "==================Top Layers==================\n",
      "\n",
      "transformer.h.30:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.31:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.32:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.33:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.34:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.35:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.ln_f.weight:[: (1280,)]\n",
      "\n",
      "transformer.ln_f.bias:[: (1280,)]\n",
      "==================Trunk Layers==================\n",
      "\n",
      "transformer.h.6:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.7:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.8:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.9:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.10:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.11:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.12:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.13:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.14:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.15:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.16:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.17:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.18:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.19:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.20:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.21:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.22:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.23:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.24:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.25:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.26:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.27:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.28:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.29:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "==================Bottom Layers==================\n",
      "\n",
      "transformer.wte.weight:[: (50257, 1280)]\n",
      "\n",
      "transformer.wte.lora_embedding_A.default:[: (8, 50257)]\n",
      "\n",
      "transformer.wte.lora_embedding_B.default:[: (1280, 8)]\n",
      "\n",
      "transformer.wpe.weight:[: (1024, 1280)]\n",
      "\n",
      "transformer.wpe.lora_embedding_A.default:[: (8, 1024)]\n",
      "\n",
      "transformer.wpe.lora_embedding_B.default:[: (1280, 8)]\n",
      "\n",
      "transformer.h.0:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.1:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.2:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.3:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.4:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.5:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "md_lr.print_split_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight torch.Size([32000, 4096])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.0.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.1.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.2.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.3.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.4.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.5.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.6.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.7.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.8.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.9.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.10.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.11.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.12.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.13.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.14.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.15.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.16.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.16.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.16.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.16.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.16.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.17.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.17.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.17.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.17.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.17.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.18.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.18.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.18.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.18.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.18.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.19.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.19.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.19.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.19.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.19.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.20.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.20.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.20.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.20.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.20.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.21.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.21.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.21.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.21.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.21.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.22.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.22.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.22.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.22.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.22.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.23.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.23.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.23.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.23.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.23.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.24.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.24.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.24.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.24.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.24.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.25.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.25.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.25.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.25.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.25.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.26.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.26.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.26.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.26.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.26.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.27.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.27.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.27.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.27.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.27.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.28.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.28.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.28.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.28.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.28.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.28.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.28.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.29.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.29.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.29.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096])\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 8])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.29.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.29.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.29.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.29.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.30.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.30.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.30.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.30.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.30.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.30.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.30.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.31.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.31.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.31.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "base_model.model.model.layers.31.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.31.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "base_model.model.model.layers.31.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "base_model.model.model.layers.31.input_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight torch.Size([4096])\n",
      "base_model.model.model.norm.weight torch.Size([4096])\n",
      "base_model.model.lm_head.weight torch.Size([32000, 4096])\n"
     ]
    }
   ],
   "source": [
    "for nm,p in md_lr.named_parameters():\n",
    "    print(nm,p.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sfl'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msfl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DRAConfig\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msfl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexp\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_dra_attacker\n\u001B[1;32m      4\u001B[0m atk_cfg \u001B[38;5;241m=\u001B[39m DRAConfig(target_model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mllama2\u001B[39m\u001B[38;5;124m'\u001B[39m, target_dataset\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msanitized\u001B[39m\u001B[38;5;124m'\u001B[39m, target_sps\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m6-6\u001B[39m\u001B[38;5;124m'\u001B[39m, train_label\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m'\u001B[39m, target_model_load_bits\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m)\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'sfl'"
     ]
    }
   ],
   "source": [
    "from sfl.config import DRAConfig\n",
    "from sfl.utils.exp import get_dra_attacker\n",
    "\n",
    "atk_cfg = DRAConfig(target_model_name='llama2', target_dataset='sanitized', target_sps='6-6', train_label='val', target_model_load_bits=4)\n",
    "atk, _ = get_dra_attacker(atk_cfg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AdamW\n",
    "from sfl.utils.model import evaluate_attacker_rouge\n",
    "from sfl.utils.exp import get_dataset_class\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "client_ids = ['0']\n",
    "\n",
    "dataset_cls = get_dataset_class('sanitized')\n",
    "dataset = dataset_cls(tokenizer=tokenizer, client_ids=client_ids)\n",
    "test_loader = dataset.get_dataloader_unsliced(2, 'train', shrink_frac=0.2)\n",
    "\n",
    "avg_rouge = 0\n",
    "avg_rouge_dlg = 0\n",
    "step = 0\n",
    "opt = AdamW(model.parameters(), lr=1e-5)\n",
    "config.noise_mode = 'dxp'\n",
    "config.noise_scale = 1000.0\n",
    "model.config_sfl(config)\n",
    "\n",
    "dlg.to(model.device)\n",
    "atk.to(model.device)\n",
    "with tqdm_notebook(total=len(test_loader)) as pbar:\n",
    "  for batch in test_loader:\n",
    "    opt.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(model.device)\n",
    "    o1 = model(input_ids, batch['input_att_mask'].to(model.device), labels=input_ids)\n",
    "    # o2 = dlg(tr2t.fx.to(model.device))\n",
    "    # print(o1)\n",
    "    o1.loss.backward()\n",
    "    b2tr, tr2t, all = model.get_all_inter()\n",
    "    opt.step()\n",
    "    pred = atk(b2tr.fx.to(model.device))\n",
    "    # print(batch['input_text'][0])\n",
    "    gt = dlg.fit(tr2t.fx.to(model.device), tr2t.grad.to(model.device), epochs=20, gt_init=pred)\n",
    "    # gt_texts = [tokenizer.decode(g.argmax(-1), skip_special_tokens=True) for g in gt]\n",
    "    avg_rouge += evaluate_attacker_rouge(tokenizer, pred, batch)['rouge-l']['f']\n",
    "    avg_rouge_dlg += evaluate_attacker_rouge(tokenizer, gt, batch)['rouge-l']['f']\n",
    "    step += 1\n",
    "    pbar.set_postfix({'dra_rouge': avg_rouge / step, 'dlg_rouge':avg_rouge_dlg/step})\n",
    "    pbar.update()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sfl.utils.exp import add_sfl_params\n",
    "import argparse\n",
    "from typing import Any\n",
    "from sfl.utils.model import Intermediate\n",
    "from sfl.simulator.strategy import BaseSFLStrategy\n",
    "from sfl.simulator.simulator import SFLSimulator\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "add_sfl_params(parser)\n",
    "args = parser.parse_args({})\n",
    "\n",
    "args.log_to_wandb = False\n",
    "args.dlg_epochs = 30\n",
    "args.dlg_init_with_dra = True\n",
    "\n",
    "# 定义Client本地学习策略\n",
    "class QAFLStrategy(BaseSFLStrategy):\n",
    "\n",
    "\n",
    "    def sample_attacker_triggered(self, global_round, client_id, local_epoch, local_step,\n",
    "                                  b2tr_inter: Intermediate, tr2t_inter: Intermediate,\n",
    "                                  all_inter: dict[Any, Intermediate],\n",
    "                                  batch, logs):\n",
    "        encoder_inter = all_inter.get('encoder', None)\n",
    "        with torch.no_grad():\n",
    "            for type, atk in zip(['b2tr', 'tr2t'], [self.dra1, self.dra2]):\n",
    "                if atk is None:\n",
    "                    continue\n",
    "                atk.to(self.simulator.device)\n",
    "                inter = b2tr_inter if type == 'b2tr' else tr2t_inter\n",
    "                if self.llm.type == 'encoder-decoder':\n",
    "                    attacked = atk(torch.concat([encoder_inter.fx.to(\n",
    "                        self.simulator.device), inter.fx.to(atk.device)], dim=1))\n",
    "                else:\n",
    "                    attacked = atk(inter.fx.to(atk.device))\n",
    "                rouge_res = calculate_rouge(self.tokenizer, attacked, batch['input_text'])\n",
    "                self.log_to_sample_result(client_id, f'attacker_{type}', rouge_res['rouge-l']['f'])\n",
    "                self.log_to_all_result(client_id, f'attacker_{type}', rouge_res['rouge-l']['f'])\n",
    "                logs[f'attacker_{type}_step'] = rouge_res['rouge-l']['f']\n",
    "        gt_init = None\n",
    "        if self.args.dlg_init_with_dra:\n",
    "            gt_init = attacked\n",
    "        self.dlg.to(self.simulator.device)\n",
    "        gt = self.dlg.fit(tr2t_inter.fx.to(self.simulator.device), tr2t_inter.grad.to(self.simulator.device),\n",
    "                          epochs=self.args.dlg_epochs,\n",
    "                          adjust=False,\n",
    "                          beta=self.args.dlg_beta,\n",
    "                          gt_init=gt_init,\n",
    "                          gt_reg=self.args.dlg_dra_reg,\n",
    "                          temp_range=self.args.dlg_temp_range,\n",
    "                          further_ft=self.args.dlg_further_ft,\n",
    "                          encoder_inter=None if encoder_inter is None else encoder_inter.fx.to(\n",
    "                              self.simulator.device)\n",
    "                          )\n",
    "        if self.llm.type == 'encoder-decoder':\n",
    "            # replace the latter half of attacked to gt\n",
    "            attacked[:, -gt.shape[1]:, :] = gt\n",
    "            rouge = calculate_rouge(self.tokenizer, attacked, batch['input_text'])\n",
    "        else:\n",
    "            rouge = calculate_rouge(self.tokenizer, gt, batch['input_text'])\n",
    "        self.log_to_sample_result(client_id, 'tag_rouge_lf', rouge['rouge-l']['f'])\n",
    "        self.log_to_all_result(client_id, 'tag_rouge_lf', rouge['rouge-l']['f'])\n",
    "        print(self.attack_all_performs)\n",
    "\n",
    "\n",
    "\n",
    "simulator = SFLSimulator(client_ids=client_ids,\n",
    "                             strategy=QAFLStrategy(args, model, tokenizer, test_loader, atk, None,dlg),\n",
    "                             llm=model,\n",
    "                             tokenizer=tokenizer,\n",
    "                             dataset=dataset, config=config, args=args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "simulator.simulate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 生成SensMarked数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "raw = pd.read_csv('/home/project/SFL-LLM/sanitized_data.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "                                              sani_label  \\\n0      <PERSON> attended the press conference to rais...   \n1      The <DATE>, who joined from <ORG> for £<MONEY>...   \n2      <ORG> slipped to 47-4 but <PERSON> led a recov...   \n3      <ORG> have  agreed a deal to loan <NORP> inter...   \n4      The <DATE> announced the ban after scoring in ...   \n...                                                  ...   \n19660  The <NORP> champions take on the <NORP> in <GP...   \n19661  (<ORG>) -- He's the king of cool but <GPE> sno...   \n19662  <PERSON>, <DATE>, is a doubt for <ORG>' Six Na...   \n19663  (CNN)Eight crew members are missing after a ca...   \n19664  By . <PERSON> . An expensive delicacy, most pe...   \n\n                                        sani_label_trans  \\\n0      <PERSON>参加了新闻发布会，他对比赛官员<PERSON>提出了关切，而不是派遣教练<P...   \n1      这位于<DATE>从<ORG>以£<MONEY>的价格加盟的球员，曾在<DATE>为<ORG...   \n2      <ORG>滑到了47-4，但<PERSON>以96分不出的成绩带领球队恢复了局面。<PERS...   \n3      <ORG>已经同意将<NORP>国际后卫<PERSON>租借给陷入困境的<ORG>，俱乐部已...   \n4      <DATE>在<PERSON>在球队3-0的世界杯预选赛胜利后得分后宣布了禁令。一份广播报道...   \n...                                                  ...   \n19660  <NORP>冠军将在<GPE>的比赛中迎战<NORP>，他们希望能够锁定一个晋级八强的名额。...   \n19661  (<ORG>) - 他是酷的王者，但<GPE>滑雪板明星<PERSON>不会在索契<DATE...   \n19662  <PERSON>，<DATE>，在对阵<GPE>的六国赛比赛中因脚踝受伤而一瘸一拐地离场。在...   \n19663  (CNN)据<ORG>报道，一艘货船在<GPE>海岸翻覆后，8名船员失踪。<ORG>表示，搜...   \n19664  由<PERSON>。一种昂贵的美味，大多数人不喜欢看到好的龙虾被浪费。但是一位厨师更进一步，...   \n\n                                                  entity  \\\n0      ['Chris Kendall', 'Widnes', 'today', 'Monday',...   \n1      ['Derby', 'a great season', '3', 'Curtis Davie...   \n2      ['Lewis Hill', 'Leicestershire', 'Ned Eckersle...   \n3      ['Spartak Moscow', 'Italian', 'Milan', 'Sparta...   \n4      ['Argentina', 'Colombia', 'Lavezzi', 'Ezequiel...   \n...                                                  ...   \n19660  ['South Korea', 'Africa', 'Costa Rica', '[Jurg...   \n19661  ['Norway', 'Team USA', 'Finnish', 'U.S.', 'Tue...   \n19662  ['Sunday', 'Wales', 'Priestland', 'Ireland', '...   \n19663  ['Sunday', 'England', 'RNLI', 'Southampton', '...   \n19664  ['Jennifer Smith', 'Huang', 'Huang Mingbo', 'F...   \n\n                                                 content  \\\n0      Beaumont attended the press conference to rais...   \n1      The 24-year-old, who joined from Liverpool for...   \n2      Derbyshire slipped to 47-4 but Alex Hughes led...   \n3      Spartak Moscow have  agreed a deal to loan Ita...   \n4      The 29-year-old announced the ban after scorin...   \n...                                                  ...   \n19660  The African champions take on the Europeans in...   \n19661  (CNN) -- He's the king of cool but U.S. snowbo...   \n19662  Biggar, 26, is a doubt for Wales' Six Nations ...   \n19663  (CNN)Eight crew members are missing after a ca...   \n19664  By . Jennifer Smith . An expensive delicacy, m...   \n\n                                               sani_gpt4  \\\n0      John Smith attended the press conference to ra...   \n1      The 30-year-old, who joined from Manchester fo...   \n2      Yorkshire slipped to 47-4 but John Smith led a...   \n3      Barcelona FC have agreed a deal to loan Spanis...   \n4      The 35-year-old announced the ban after scorin...   \n...                                                  ...   \n19660  The Asian champions take on the Americans in T...   \n19661  (BBC) -- He's the emperor of cool but Canadian...   \n19662  Hudson, 32, is a doubt for France's Six Nation...   \n19663  (CNN)Eight crew members are missing after a ca...   \n19664  By . Emily Johnson . An expensive delicacy, mo...   \n\n                                         sani_gpt4_trans  \\\n0      约翰·史密斯参加了新闻发布会，向比赛官员迈克·约翰逊表达了他对比赛失利的担忧，而不是派遣教练...   \n1      这位30岁的球员上周以700万英镑的价格从曼彻斯特加盟，他在2018-2019赛季为公牛队出...   \n2      约克郡队在比赛中一度陷入47-4的困境，但约翰·史密斯以96分不出的表现带领球队实现了反弹。...   \n3      巴塞罗那足球俱乐部已经同意将西班牙国家队后卫亚历杭德罗·费尔南德斯租借给陷入困境的皇家马德里...   \n4      这位35岁的球员在秘鲁队以3-0的比分赢得世界杯预选赛后宣布禁赛。一则广播报道声称前锋卡洛斯...   \n...                                                  ...   \n19660  亚洲冠军将在日本东京与美国队对决，他们希望能够锁定一个晋级四分之一决赛的名额。村上（Mura...   \n19661  （BBC）-- 他是酷劲十足的皇帝，但加拿大滑雪板明星约翰·多伊将不会在东京2020年的令人...   \n19662  Hudson，32岁，在对阵西班牙的比赛中因脚踝受伤而退出，成为法国对阵意大利的六国赛比赛的...   \n19663  （CNN）国际海上救援联合会周五早上表示，一艘货船在爱尔兰海岸翻覆后，有八名船员失踪。海军和...   \n19664  由艾米莉·约翰逊报道。一种昂贵的美味，大多数人不喜欢看到好的龙虾被浪费。但一位厨师更进一步，...   \n\n                                              gpt4_trans  \n0      博蒙特参加了新闻发布会，向比赛官员克里斯·肯德尔表达了他对比赛失利的担忧，而不是派遣教练尼尔...  \n1      这位24岁的球员上个月以300万英镑的价格从利物浦加盟，他在2013-2014赛季为公羊队出...  \n2      德比郡队在比赛中一度陷入47-4的困境，但亚历克斯·休斯以96分不出的表现带领球队实现了反弹...  \n3      莫斯科斯巴达克同意将意大利国家队后卫萨尔瓦托雷·博凯蒂租借给陷入困境的AC米兰，俱乐部已经确...  \n4      这位29岁的球员在哥伦比亚队以3-0的比分赢得世界杯预选赛后宣布禁赛。一则广播报道声称前锋埃...  \n...                                                  ...  \n19660  非洲冠军将在韩国济州岛与欧洲队对决，他们希望能够锁定一个晋级四分之一决赛的名额。钱贝西（Ch...  \n19661  （CNN）-- 他是酷劲十足的国际滑雪板明星肖恩·怀特，但他将不会在索契2014年的令人生畏...  \n19662  Biggar，26岁，在对阵爱尔兰的比赛中因脚踝受伤而退出，成为威尔士对阵苏格兰的六国赛比赛...  \n19663  （CNN）周六晚间，英国皇家国家救生艇机构表示，一艘货船在苏格兰海岸翻覆后，有八名船员失踪。...  \n19664  由詹妮弗·史密斯报道。一种昂贵的美味，大多数人不喜欢看到好的龙虾被浪费。但一位厨师更进一步，...  \n\n[19665 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sani_label</th>\n      <th>sani_label_trans</th>\n      <th>entity</th>\n      <th>content</th>\n      <th>sani_gpt4</th>\n      <th>sani_gpt4_trans</th>\n      <th>gpt4_trans</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;PERSON&gt; attended the press conference to rais...</td>\n      <td>&lt;PERSON&gt;参加了新闻发布会，他对比赛官员&lt;PERSON&gt;提出了关切，而不是派遣教练&lt;P...</td>\n      <td>['Chris Kendall', 'Widnes', 'today', 'Monday',...</td>\n      <td>Beaumont attended the press conference to rais...</td>\n      <td>John Smith attended the press conference to ra...</td>\n      <td>约翰·史密斯参加了新闻发布会，向比赛官员迈克·约翰逊表达了他对比赛失利的担忧，而不是派遣教练...</td>\n      <td>博蒙特参加了新闻发布会，向比赛官员克里斯·肯德尔表达了他对比赛失利的担忧，而不是派遣教练尼尔...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The &lt;DATE&gt;, who joined from &lt;ORG&gt; for £&lt;MONEY&gt;...</td>\n      <td>这位于&lt;DATE&gt;从&lt;ORG&gt;以£&lt;MONEY&gt;的价格加盟的球员，曾在&lt;DATE&gt;为&lt;ORG...</td>\n      <td>['Derby', 'a great season', '3', 'Curtis Davie...</td>\n      <td>The 24-year-old, who joined from Liverpool for...</td>\n      <td>The 30-year-old, who joined from Manchester fo...</td>\n      <td>这位30岁的球员上周以700万英镑的价格从曼彻斯特加盟，他在2018-2019赛季为公牛队出...</td>\n      <td>这位24岁的球员上个月以300万英镑的价格从利物浦加盟，他在2013-2014赛季为公羊队出...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;ORG&gt; slipped to 47-4 but &lt;PERSON&gt; led a recov...</td>\n      <td>&lt;ORG&gt;滑到了47-4，但&lt;PERSON&gt;以96分不出的成绩带领球队恢复了局面。&lt;PERS...</td>\n      <td>['Lewis Hill', 'Leicestershire', 'Ned Eckersle...</td>\n      <td>Derbyshire slipped to 47-4 but Alex Hughes led...</td>\n      <td>Yorkshire slipped to 47-4 but John Smith led a...</td>\n      <td>约克郡队在比赛中一度陷入47-4的困境，但约翰·史密斯以96分不出的表现带领球队实现了反弹。...</td>\n      <td>德比郡队在比赛中一度陷入47-4的困境，但亚历克斯·休斯以96分不出的表现带领球队实现了反弹...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;ORG&gt; have  agreed a deal to loan &lt;NORP&gt; inter...</td>\n      <td>&lt;ORG&gt;已经同意将&lt;NORP&gt;国际后卫&lt;PERSON&gt;租借给陷入困境的&lt;ORG&gt;，俱乐部已...</td>\n      <td>['Spartak Moscow', 'Italian', 'Milan', 'Sparta...</td>\n      <td>Spartak Moscow have  agreed a deal to loan Ita...</td>\n      <td>Barcelona FC have agreed a deal to loan Spanis...</td>\n      <td>巴塞罗那足球俱乐部已经同意将西班牙国家队后卫亚历杭德罗·费尔南德斯租借给陷入困境的皇家马德里...</td>\n      <td>莫斯科斯巴达克同意将意大利国家队后卫萨尔瓦托雷·博凯蒂租借给陷入困境的AC米兰，俱乐部已经确...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The &lt;DATE&gt; announced the ban after scoring in ...</td>\n      <td>&lt;DATE&gt;在&lt;PERSON&gt;在球队3-0的世界杯预选赛胜利后得分后宣布了禁令。一份广播报道...</td>\n      <td>['Argentina', 'Colombia', 'Lavezzi', 'Ezequiel...</td>\n      <td>The 29-year-old announced the ban after scorin...</td>\n      <td>The 35-year-old announced the ban after scorin...</td>\n      <td>这位35岁的球员在秘鲁队以3-0的比分赢得世界杯预选赛后宣布禁赛。一则广播报道声称前锋卡洛斯...</td>\n      <td>这位29岁的球员在哥伦比亚队以3-0的比分赢得世界杯预选赛后宣布禁赛。一则广播报道声称前锋埃...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19660</th>\n      <td>The &lt;NORP&gt; champions take on the &lt;NORP&gt; in &lt;GP...</td>\n      <td>&lt;NORP&gt;冠军将在&lt;GPE&gt;的比赛中迎战&lt;NORP&gt;，他们希望能够锁定一个晋级八强的名额。...</td>\n      <td>['South Korea', 'Africa', 'Costa Rica', '[Jurg...</td>\n      <td>The African champions take on the Europeans in...</td>\n      <td>The Asian champions take on the Americans in T...</td>\n      <td>亚洲冠军将在日本东京与美国队对决，他们希望能够锁定一个晋级四分之一决赛的名额。村上（Mura...</td>\n      <td>非洲冠军将在韩国济州岛与欧洲队对决，他们希望能够锁定一个晋级四分之一决赛的名额。钱贝西（Ch...</td>\n    </tr>\n    <tr>\n      <th>19661</th>\n      <td>(&lt;ORG&gt;) -- He's the king of cool but &lt;GPE&gt; sno...</td>\n      <td>(&lt;ORG&gt;) - 他是酷的王者，但&lt;GPE&gt;滑雪板明星&lt;PERSON&gt;不会在索契&lt;DATE...</td>\n      <td>['Norway', 'Team USA', 'Finnish', 'U.S.', 'Tue...</td>\n      <td>(CNN) -- He's the king of cool but U.S. snowbo...</td>\n      <td>(BBC) -- He's the emperor of cool but Canadian...</td>\n      <td>（BBC）-- 他是酷劲十足的皇帝，但加拿大滑雪板明星约翰·多伊将不会在东京2020年的令人...</td>\n      <td>（CNN）-- 他是酷劲十足的国际滑雪板明星肖恩·怀特，但他将不会在索契2014年的令人生畏...</td>\n    </tr>\n    <tr>\n      <th>19662</th>\n      <td>&lt;PERSON&gt;, &lt;DATE&gt;, is a doubt for &lt;ORG&gt;' Six Na...</td>\n      <td>&lt;PERSON&gt;，&lt;DATE&gt;，在对阵&lt;GPE&gt;的六国赛比赛中因脚踝受伤而一瘸一拐地离场。在...</td>\n      <td>['Sunday', 'Wales', 'Priestland', 'Ireland', '...</td>\n      <td>Biggar, 26, is a doubt for Wales' Six Nations ...</td>\n      <td>Hudson, 32, is a doubt for France's Six Nation...</td>\n      <td>Hudson，32岁，在对阵西班牙的比赛中因脚踝受伤而退出，成为法国对阵意大利的六国赛比赛的...</td>\n      <td>Biggar，26岁，在对阵爱尔兰的比赛中因脚踝受伤而退出，成为威尔士对阵苏格兰的六国赛比赛...</td>\n    </tr>\n    <tr>\n      <th>19663</th>\n      <td>(CNN)Eight crew members are missing after a ca...</td>\n      <td>(CNN)据&lt;ORG&gt;报道，一艘货船在&lt;GPE&gt;海岸翻覆后，8名船员失踪。&lt;ORG&gt;表示，搜...</td>\n      <td>['Sunday', 'England', 'RNLI', 'Southampton', '...</td>\n      <td>(CNN)Eight crew members are missing after a ca...</td>\n      <td>(CNN)Eight crew members are missing after a ca...</td>\n      <td>（CNN）国际海上救援联合会周五早上表示，一艘货船在爱尔兰海岸翻覆后，有八名船员失踪。海军和...</td>\n      <td>（CNN）周六晚间，英国皇家国家救生艇机构表示，一艘货船在苏格兰海岸翻覆后，有八名船员失踪。...</td>\n    </tr>\n    <tr>\n      <th>19664</th>\n      <td>By . &lt;PERSON&gt; . An expensive delicacy, most pe...</td>\n      <td>由&lt;PERSON&gt;。一种昂贵的美味，大多数人不喜欢看到好的龙虾被浪费。但是一位厨师更进一步，...</td>\n      <td>['Jennifer Smith', 'Huang', 'Huang Mingbo', 'F...</td>\n      <td>By . Jennifer Smith . An expensive delicacy, m...</td>\n      <td>By . Emily Johnson . An expensive delicacy, mo...</td>\n      <td>由艾米莉·约翰逊报道。一种昂贵的美味，大多数人不喜欢看到好的龙虾被浪费。但一位厨师更进一步，...</td>\n      <td>由詹妮弗·史密斯报道。一种昂贵的美味，大多数人不喜欢看到好的龙虾被浪费。但一位厨师更进一步，...</td>\n    </tr>\n  </tbody>\n</table>\n<p>19665 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_548089/1355228985.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i, row in tqdm_notebook(raw.iterrows(), total=len(raw)):\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/19665 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3badd7f1c59e49ed9757fdaa0256b611"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import pandas as pd\n",
    "raw = pd.read_csv('/home/project/SFL-LLM/sanitized_data.csv')\n",
    "# take only 'content' and 'entity'\n",
    "raw = raw[['content', 'entity', 'sani_gpt4','sani_label']]\n",
    "\n",
    "\n",
    "marked_content = []\n",
    "\n",
    "for i, row in tqdm_notebook(raw.iterrows(), total=len(raw)):\n",
    "    # get the content and entity\n",
    "    data = {}\n",
    "    content = row['content']\n",
    "    entity = row['entity']\n",
    "    data['sentence'] = content\n",
    "    # print(entity)\n",
    "    entity = ast.literal_eval(entity)\n",
    "    replaced_places = []\n",
    "    for e in entity:\n",
    "        indexes = [(m.start(), m.end()) for m in re.finditer(re.escape(e), content)]\n",
    "        for idx in indexes:\n",
    "            if any([idx[0] > r[0] and idx[1] < r[1] for r in replaced_places]):\n",
    "                continue\n",
    "            content = content[:idx[0]] + '<P>' + content[idx[0]:idx[1]] + '<\\P>' + content[idx[1]:]\n",
    "            replaced_places.append(idx)\n",
    "    marked_content.append(content)\n",
    "\n",
    "df = pd.DataFrame(marked_content, columns=['marked_content'])\n",
    "new = pd.concat([raw, df], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "type_labels = ['train'] * len(new)\n",
    "# randomly select 20% indexes\n",
    "all_indexes = list(range(len(new)))\n",
    "import random\n",
    "\n",
    "test_indexes = random.sample(all_indexes, int(len(new) * 0.25))\n",
    "for i in test_indexes:\n",
    "    type_labels[i] = 'test'\n",
    "\n",
    "all_indexes = list(set(all_indexes) - set(test_indexes))\n",
    "val_indexes = random.sample(all_indexes, int(len(new) * 0.15))\n",
    "for i in val_indexes:\n",
    "    type_labels[i] = 'validation'\n",
    "\n",
    "# make type_labels to dataframe and concat it with the original dataframe\n",
    "df = pd.DataFrame(type_labels, columns=['type'])\n",
    "# concat it with new\n",
    "new = pd.concat([new, df], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "new.to_csv('/home/project/SFL-LLM/sensi.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 content  \\\n0      Beaumont attended the press conference to rais...   \n1      The 24-year-old, who joined from Liverpool for...   \n2      Derbyshire slipped to 47-4 but Alex Hughes led...   \n3      Spartak Moscow have  agreed a deal to loan Ita...   \n4      The 29-year-old announced the ban after scorin...   \n...                                                  ...   \n19660  The African champions take on the Europeans in...   \n19661  (CNN) -- He's the king of cool but U.S. snowbo...   \n19662  Biggar, 26, is a doubt for Wales' Six Nations ...   \n19663  (CNN)Eight crew members are missing after a ca...   \n19664  By . Jennifer Smith . An expensive delicacy, m...   \n\n                                                  entity  \\\n0      ['Chris Kendall', 'Widnes', 'today', 'Monday',...   \n1      ['Derby', 'a great season', '3', 'Curtis Davie...   \n2      ['Lewis Hill', 'Leicestershire', 'Ned Eckersle...   \n3      ['Spartak Moscow', 'Italian', 'Milan', 'Sparta...   \n4      ['Argentina', 'Colombia', 'Lavezzi', 'Ezequiel...   \n...                                                  ...   \n19660  ['South Korea', 'Africa', 'Costa Rica', '[Jurg...   \n19661  ['Norway', 'Team USA', 'Finnish', 'U.S.', 'Tue...   \n19662  ['Sunday', 'Wales', 'Priestland', 'Ireland', '...   \n19663  ['Sunday', 'England', 'RNLI', 'Southampton', '...   \n19664  ['Jennifer Smith', 'Huang', 'Huang Mingbo', 'F...   \n\n                                               sani_gpt4  \\\n0      John Smith attended the press conference to ra...   \n1      The 30-year-old, who joined from Manchester fo...   \n2      Yorkshire slipped to 47-4 but John Smith led a...   \n3      Barcelona FC have agreed a deal to loan Spanis...   \n4      The 35-year-old announced the ban after scorin...   \n...                                                  ...   \n19660  The Asian champions take on the Americans in T...   \n19661  (BBC) -- He's the emperor of cool but Canadian...   \n19662  Hudson, 32, is a doubt for France's Six Nation...   \n19663  (CNN)Eight crew members are missing after a ca...   \n19664  By . Emily Johnson . An expensive delicacy, mo...   \n\n                                              sani_label  \\\n0      <PERSON> attended the press conference to rais...   \n1      The <DATE>, who joined from <ORG> for £<MONEY>...   \n2      <ORG> slipped to 47-4 but <PERSON> led a recov...   \n3      <ORG> have  agreed a deal to loan <NORP> inter...   \n4      The <DATE> announced the ban after scoring in ...   \n...                                                  ...   \n19660  The <NORP> champions take on the <NORP> in <GP...   \n19661  (<ORG>) -- He's the king of cool but <GPE> sno...   \n19662  <PERSON>, <DATE>, is a doubt for <ORG>' Six Na...   \n19663  (CNN)Eight crew members are missing after a ca...   \n19664  By . <PERSON> . An expensive delicacy, most pe...   \n\n                                          marked_content        type  \n0      <P>Beaumont<\\P> attended the press conference ...       train  \n1      The <P>24-year-old<\\P>, who joined from <P>Liv...        test  \n2      <P>Derbyshire<\\P> slipped to 47-4 but Alex <P>...       train  \n3      <P>Spartak Moscow<\\P> have  agreed a deal to l...       train  \n4      The <P>29-year-old<\\P> announced the ban after...  validation  \n...                                                  ...         ...  \n19660  The <P>Africa<\\P>n champions take on the <P>Eu...       train  \n19661  (<P>CNN<\\P>) -- He's the king of cool but <P>U...        test  \n19662  <P>Biggar<\\P>, <P>26<\\P>, is a doubt for <P>Wa...        test  \n19663  (CNN)Eight crew members are missing after a ca...       train  \n19664  By . <P>Jennifer Smith<\\P> . An expensive deli...       train  \n\n[19665 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>entity</th>\n      <th>sani_gpt4</th>\n      <th>sani_label</th>\n      <th>marked_content</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Beaumont attended the press conference to rais...</td>\n      <td>['Chris Kendall', 'Widnes', 'today', 'Monday',...</td>\n      <td>John Smith attended the press conference to ra...</td>\n      <td>&lt;PERSON&gt; attended the press conference to rais...</td>\n      <td>&lt;P&gt;Beaumont&lt;\\P&gt; attended the press conference ...</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The 24-year-old, who joined from Liverpool for...</td>\n      <td>['Derby', 'a great season', '3', 'Curtis Davie...</td>\n      <td>The 30-year-old, who joined from Manchester fo...</td>\n      <td>The &lt;DATE&gt;, who joined from &lt;ORG&gt; for £&lt;MONEY&gt;...</td>\n      <td>The &lt;P&gt;24-year-old&lt;\\P&gt;, who joined from &lt;P&gt;Liv...</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Derbyshire slipped to 47-4 but Alex Hughes led...</td>\n      <td>['Lewis Hill', 'Leicestershire', 'Ned Eckersle...</td>\n      <td>Yorkshire slipped to 47-4 but John Smith led a...</td>\n      <td>&lt;ORG&gt; slipped to 47-4 but &lt;PERSON&gt; led a recov...</td>\n      <td>&lt;P&gt;Derbyshire&lt;\\P&gt; slipped to 47-4 but Alex &lt;P&gt;...</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Spartak Moscow have  agreed a deal to loan Ita...</td>\n      <td>['Spartak Moscow', 'Italian', 'Milan', 'Sparta...</td>\n      <td>Barcelona FC have agreed a deal to loan Spanis...</td>\n      <td>&lt;ORG&gt; have  agreed a deal to loan &lt;NORP&gt; inter...</td>\n      <td>&lt;P&gt;Spartak Moscow&lt;\\P&gt; have  agreed a deal to l...</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The 29-year-old announced the ban after scorin...</td>\n      <td>['Argentina', 'Colombia', 'Lavezzi', 'Ezequiel...</td>\n      <td>The 35-year-old announced the ban after scorin...</td>\n      <td>The &lt;DATE&gt; announced the ban after scoring in ...</td>\n      <td>The &lt;P&gt;29-year-old&lt;\\P&gt; announced the ban after...</td>\n      <td>validation</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19660</th>\n      <td>The African champions take on the Europeans in...</td>\n      <td>['South Korea', 'Africa', 'Costa Rica', '[Jurg...</td>\n      <td>The Asian champions take on the Americans in T...</td>\n      <td>The &lt;NORP&gt; champions take on the &lt;NORP&gt; in &lt;GP...</td>\n      <td>The &lt;P&gt;Africa&lt;\\P&gt;n champions take on the &lt;P&gt;Eu...</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>19661</th>\n      <td>(CNN) -- He's the king of cool but U.S. snowbo...</td>\n      <td>['Norway', 'Team USA', 'Finnish', 'U.S.', 'Tue...</td>\n      <td>(BBC) -- He's the emperor of cool but Canadian...</td>\n      <td>(&lt;ORG&gt;) -- He's the king of cool but &lt;GPE&gt; sno...</td>\n      <td>(&lt;P&gt;CNN&lt;\\P&gt;) -- He's the king of cool but &lt;P&gt;U...</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>19662</th>\n      <td>Biggar, 26, is a doubt for Wales' Six Nations ...</td>\n      <td>['Sunday', 'Wales', 'Priestland', 'Ireland', '...</td>\n      <td>Hudson, 32, is a doubt for France's Six Nation...</td>\n      <td>&lt;PERSON&gt;, &lt;DATE&gt;, is a doubt for &lt;ORG&gt;' Six Na...</td>\n      <td>&lt;P&gt;Biggar&lt;\\P&gt;, &lt;P&gt;26&lt;\\P&gt;, is a doubt for &lt;P&gt;Wa...</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>19663</th>\n      <td>(CNN)Eight crew members are missing after a ca...</td>\n      <td>['Sunday', 'England', 'RNLI', 'Southampton', '...</td>\n      <td>(CNN)Eight crew members are missing after a ca...</td>\n      <td>(CNN)Eight crew members are missing after a ca...</td>\n      <td>(CNN)Eight crew members are missing after a ca...</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>19664</th>\n      <td>By . Jennifer Smith . An expensive delicacy, m...</td>\n      <td>['Jennifer Smith', 'Huang', 'Huang Mingbo', 'F...</td>\n      <td>By . Emily Johnson . An expensive delicacy, mo...</td>\n      <td>By . &lt;PERSON&gt; . An expensive delicacy, most pe...</td>\n      <td>By . &lt;P&gt;Jennifer Smith&lt;\\P&gt; . An expensive deli...</td>\n      <td>train</td>\n    </tr>\n  </tbody>\n</table>\n<p>19665 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "dataset = Dataset.from_pandas(new)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "\n",
    "# _, tokenizer = get_model_and_tokenizer('bert')\n",
    "# model, t = get_model_and_tokenizer('flan-t5-large')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from sfl.simulator.dataset import FedDataset\n",
    "\n",
    "\n",
    "class SanitizedFedDataset(FedDataset):\n",
    "\n",
    "    def _format(self, example):\n",
    "        return {'input': example['content'], 'entities': ast.literal_eval(example['entity'])}\n",
    "\n",
    "    def _col_fun(self, batch):\n",
    "        texts = [b['input'] for b in batch]\n",
    "        input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "        mask = torch.zeros_like(input['input_ids'])\n",
    "        for sp, sample in enumerate(batch):\n",
    "            seq = input['input_ids'][sp].numpy().tolist()\n",
    "            r = tokenizer(sample['entities'], add_special_tokens=False)\n",
    "            for subseq in r.input_ids:\n",
    "                for i in range(len(seq) - len(subseq) + 1):\n",
    "                    if seq[i:i + len(subseq)] == subseq:\n",
    "                        mask[sp, i:i + len(subseq)] = 1\n",
    "\n",
    "        return {'input_ids': input['input_ids'],\n",
    "                'input_att_mask': input['attention_mask'],\n",
    "                'input_text': texts, 'entities': [b['entity'] for b in batch],\n",
    "                'input_santi_mask': mask}\n",
    "\n",
    "    def __init__(self, tokenizer, client_ids: list[str], ):\n",
    "        self.df = pd.read_csv('/home/project/SFL-LLM/sanitized_data_marked.csv')\n",
    "        dataset = {\n",
    "            'train': Dataset.from_pandas(self.df[self.df['type'] == 'train']),\n",
    "            'val': Dataset.from_pandas(self.df[self.df['type'] == 'val']),\n",
    "            'test': Dataset.from_pandas(self.df[self.df['type'] == 'test'])\n",
    "        }\n",
    "        super().__init__(tokenizer, client_ids, dataset, ['train', 'val', 'test'])\n",
    "\n",
    "\n",
    "ds = SanitizedFedDataset(tokenizer, ['0'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ld = ds.get_dataloader_unsliced(6, 'val')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for batch in ld:\n",
    "    # find input_ids masked by mask\n",
    "    input_ids = batch['input_ids']\n",
    "    mask = batch['input_santi_mask']\n",
    "    masked = input_ids * mask\n",
    "\n",
    "    print(tokenizer.decode(masked[0],skip_special_tokens=True))\n",
    "    print(batch['entities'][0])\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sfl/lib/python3.11/site-packages/datasets/load.py:926: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /root/autodl-tmp/sfl/datasets/piqa/piqa.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Parameter 'function'=<function FedDataset._pre_process.<locals>.<lambda> at 0x7f14e0bf3880> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "FSHAAttacker(\n  (f_inv): GRUDRAttacker(\n    (gru): GRU(1280, 256, batch_first=True)\n    (mlp): Linear(in_features=256, out_features=50257, bias=True)\n  )\n  (f): GRUDRAttacker(\n    (gru): GRU(1280, 256, batch_first=True)\n    (mlp): Linear(in_features=256, out_features=50257, bias=True)\n  )\n  (d): GRU(1280, 256, batch_first=True)\n  (d_mlp): Sequential(\n    (0): Linear(in_features=256, out_features=1, bias=True)\n  )\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sfl/lib/python3.11/site-packages/peft/tuners/lora.py:299: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================Global Round 0=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_281437/3179479802.py:31: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  with tqdm_notebook(total=config.client_steps) as pbar:\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/700 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b280279138d34009a5d97b332d8924a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 1280, got 50257",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 109\u001B[0m\n\u001B[1;32m    102\u001B[0m attacker\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m    103\u001B[0m simulator \u001B[38;5;241m=\u001B[39m SFLSimulator(client_ids\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m0\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    104\u001B[0m                          strategy\u001B[38;5;241m=\u001B[39mFSHAStrategy(args, model, tokenizer, attacker, pub_loader),\n\u001B[1;32m    105\u001B[0m                          llm\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m    106\u001B[0m                          tokenizer\u001B[38;5;241m=\u001B[39mtokenizer,\n\u001B[1;32m    107\u001B[0m                          dataset\u001B[38;5;241m=\u001B[39mdataset, config\u001B[38;5;241m=\u001B[39mconfig, args\u001B[38;5;241m=\u001B[39margs)\n\u001B[0;32m--> 109\u001B[0m simulator\u001B[38;5;241m.\u001B[39msimulate()\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/simulator/simulator.py:166\u001B[0m, in \u001B[0;36mSFLSimulator.simulate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    164\u001B[0m iters\u001B[38;5;241m.\u001B[39msetdefault(client_id, [\u001B[38;5;28miter\u001B[39m(loaders[client_id])])\n\u001B[1;32m    165\u001B[0m itt \u001B[38;5;241m=\u001B[39m CircularDataLoaderIterator(iters[client_id], loaders[client_id], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mclient_steps)\n\u001B[0;32m--> 166\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client_step(client_id, i, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlocal_epochs[client_id], itt)\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlocal_steps[client_id] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m itt\u001B[38;5;241m.\u001B[39miterated_num\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mglobal_steps[client_id] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m itt\u001B[38;5;241m.\u001B[39miterated_num\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/simulator/simulator.py:258\u001B[0m, in \u001B[0;36mSFLSimulator._client_step\u001B[0;34m(self, client_id, global_round, local_epoch, iterator)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;66;03m# client step\u001B[39;00m\n\u001B[1;32m    257\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n\u001B[0;32m--> 258\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mclient_step(client_id, global_round, local_epoch, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm, iterator, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig)\n\u001B[1;32m    259\u001B[0m \u001B[38;5;66;03m# store updated client parameters\u001B[39;00m\n\u001B[1;32m    260\u001B[0m cm \u001B[38;5;241m=\u001B[39m ([p\u001B[38;5;241m.\u001B[39mcpu() \u001B[38;5;28;01mfor\u001B[39;00m nm, p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mget_top_params()],\n\u001B[1;32m    261\u001B[0m       [p\u001B[38;5;241m.\u001B[39mcpu() \u001B[38;5;28;01mfor\u001B[39;00m nm, p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mget_bottom_params()])\n",
      "Cell \u001B[0;32mIn[5], line 49\u001B[0m, in \u001B[0;36mFSHAStrategy.client_step\u001B[0;34m(self, client_id, global_round, client_epoch, llm, iterator, config)\u001B[0m\n\u001B[1;32m     47\u001B[0m     x_pub \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpub_loader_iter)\n\u001B[1;32m     48\u001B[0m x_pub \u001B[38;5;241m=\u001B[39m x_pub[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(llm\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m---> 49\u001B[0m z_pub \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattacker\u001B[38;5;241m.\u001B[39mf_forward(x_pub)\n\u001B[1;32m     50\u001B[0m adv_priv_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattacker\u001B[38;5;241m.\u001B[39md_forward(z_priv)\n\u001B[1;32m     51\u001B[0m adv_pub_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattacker\u001B[38;5;241m.\u001B[39md_forward(z_pub)\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/model/attacker/fsha_attacker.py:98\u001B[0m, in \u001B[0;36mFSHAAttacker.f_forward\u001B[0;34m(self, input_ids)\u001B[0m\n\u001B[1;32m     96\u001B[0m input_ids\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     97\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mone_hot(input_ids, num_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mvocab_size)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m---> 98\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf(input_ids)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/model/attacker/dra_attacker.py:179\u001B[0m, in \u001B[0;36mGRUDRAttacker.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    176\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mforward(x)\n\u001B[1;32m    177\u001B[0m \u001B[38;5;66;03m# x[batch_size, seq_len, n_embed]\u001B[39;00m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;66;03m# output [batch_size,seq_len, vocab_size]\u001B[39;00m\n\u001B[0;32m--> 179\u001B[0m hidden, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgru(x)  \u001B[38;5;66;03m# hidden [batch_size, seq_len, n_embed]\u001B[39;00m\n\u001B[1;32m    180\u001B[0m hidden \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdropout(hidden, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mdropout, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining)\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp(hidden)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/rnn.py:996\u001B[0m, in \u001B[0;36mGRU.forward\u001B[0;34m(self, input, hx)\u001B[0m\n\u001B[1;32m    991\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    992\u001B[0m     \u001B[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001B[39;00m\n\u001B[1;32m    993\u001B[0m     \u001B[38;5;66;03m# the user believes he/she is passing in.\u001B[39;00m\n\u001B[1;32m    994\u001B[0m     hx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpermute_hidden(hx, sorted_indices)\n\u001B[0;32m--> 996\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_forward_args(\u001B[38;5;28minput\u001B[39m, hx, batch_sizes)\n\u001B[1;32m    997\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    998\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mgru(\u001B[38;5;28minput\u001B[39m, hx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers,\n\u001B[1;32m    999\u001B[0m                      \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/rnn.py:253\u001B[0m, in \u001B[0;36mRNNBase.check_forward_args\u001B[0;34m(self, input, hidden, batch_sizes)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_forward_args\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]):\n\u001B[0;32m--> 253\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_input(\u001B[38;5;28minput\u001B[39m, batch_sizes)\n\u001B[1;32m    254\u001B[0m     expected_hidden_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_expected_hidden_size(\u001B[38;5;28minput\u001B[39m, batch_sizes)\n\u001B[1;32m    256\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_hidden_size(hidden, expected_hidden_size)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/rnn.py:218\u001B[0m, in \u001B[0;36mRNNBase.check_input\u001B[0;34m(self, input, batch_sizes)\u001B[0m\n\u001B[1;32m    214\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    215\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput must have \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m dimensions, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    216\u001B[0m             expected_input_dim, \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim()))\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_size \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m--> 218\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    219\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput.size(-1) must be equal to input_size. Expected \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    220\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_size, \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)))\n",
      "\u001B[0;31mRuntimeError\u001B[0m: input.size(-1) must be equal to input_size. Expected 1280, got 50257"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from sfl.utils.exp import get_model_and_tokenizer\n",
    "from sfl.config import FLConfig\n",
    "from sfl.utils.model import get_best_gpu\n",
    "\n",
    "model, processor = get_model_and_tokenizer('vit-large')\n",
    "\n",
    "device = get_best_gpu()\n",
    "model.to(device)\n",
    "config = FLConfig(\n",
    "    collect_intermediates=False,\n",
    "    global_round=10,\n",
    "    client_evaluate_freq=500,\n",
    "    client_epoch=1,  # 每轮联邦每个Client训2轮\n",
    "    split_point_1=6,\n",
    "    split_point_2=20,  # [0,1 | 2,3,.... 29| 30, 31]\n",
    "    use_lora_at_trunk=False,  # 在trunk部分使用LoRA\n",
    "    use_lora_at_top=False,\n",
    "    use_lora_at_bottom=False,\n",
    "    top_and_bottom_from_scratch='True',\n",
    "    attack_mode='b2tr',\n",
    "    client_steps=700\n",
    ")\n",
    "\n",
    "model.config_sfl(config, None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sfl/lib/python3.11/site-packages/datasets/load.py:926: FutureWarning: The repository for imagewoof contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /root/autodl-tmp/sfl/datasets/imagewoof/imagewoof.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sfl.utils.exp import get_dataset\n",
    "\n",
    "ds = get_dataset('imagewoof',processor, client_ids=['0'], shrink_frac=0.1)\n",
    "dl = ds.get_dataloader_unsliced(64,'train', 1.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from sfl.model.attacker.dra_attacker import ViTDRAttacker\n",
    "from sfl.model.attacker.dra_attacker import ViTDRAttackerConfig\n",
    "\n",
    "attacker = ViTDRAttacker(ViTDRAttackerConfig(), model.config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from sfl.utils.model import convert_to_image\n",
    "# train the attacker\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# attacker.to(device)\n",
    "\n",
    "def test(md, atk, image):\n",
    "    atk.to(md.device)\n",
    "    inter = md(image['input'].to(model.device))\n",
    "    image = convert_to_image(atk(inter))\n",
    "    image[0].show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_195557/4195046156.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  with tqdm_notebook(total=epochs*len(dl)) as pbar:\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1420 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b341fef6dc743a49e932c258c70fc16"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epc \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m----> 8\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m dl:\n\u001B[1;32m      9\u001B[0m         input_tensor \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     10\u001B[0m         inter \u001B[38;5;241m=\u001B[39m model(input_tensor)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[1;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcollate_fn(data)\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/simulator/dataset.py:52\u001B[0m, in \u001B[0;36mFedDataset.get_dataloader_unsliced.<locals>.<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m     43\u001B[0m     ds_split \u001B[38;5;241m=\u001B[39m ds\u001B[38;5;241m.\u001B[39mtrain_test_split(shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, test_size\u001B[38;5;241m=\u001B[39mfurther_test_split)\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataLoader(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pre_process(ds_split[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m], batch_size),\n\u001B[1;32m     45\u001B[0m                       collate_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_col_fun(x),\n\u001B[1;32m     46\u001B[0m                       batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     49\u001B[0m                       collate_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_col_fun(x),\n\u001B[1;32m     50\u001B[0m                       batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataLoader(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pre_process(ds, batch_size), batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m---> 52\u001B[0m                   collate_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_col_fun(x))\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/simulator/dataset.py:385\u001B[0m, in \u001B[0;36mImageWoofFedDataset._col_fun\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    383\u001B[0m images \u001B[38;5;241m=\u001B[39m [s[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m batch]\n\u001B[1;32m    384\u001B[0m labels \u001B[38;5;241m=\u001B[39m [s[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m batch]\n\u001B[0;32m--> 385\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer(images, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m'\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpixel_values\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    386\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m: labels,\n\u001B[1;32m    387\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m: images}\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/transformers/image_processing_utils.py:494\u001B[0m, in \u001B[0;36mBaseImageProcessor.__call__\u001B[0;34m(self, images, **kwargs)\u001B[0m\n\u001B[1;32m    492\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, images, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BatchFeature:\n\u001B[1;32m    493\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 494\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(images, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/transformers/models/vit/image_processing_vit.py:262\u001B[0m, in \u001B[0;36mViTImageProcessor.preprocess\u001B[0;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, **kwargs)\u001B[0m\n\u001B[1;32m    259\u001B[0m images \u001B[38;5;241m=\u001B[39m [to_numpy_array(image) \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m images]\n\u001B[1;32m    261\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m do_resize:\n\u001B[0;32m--> 262\u001B[0m     images \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresize(image\u001B[38;5;241m=\u001B[39mimage, size\u001B[38;5;241m=\u001B[39msize_dict, resample\u001B[38;5;241m=\u001B[39mresample) \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m images]\n\u001B[1;32m    264\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m do_rescale:\n\u001B[1;32m    265\u001B[0m     images \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrescale(image\u001B[38;5;241m=\u001B[39mimage, scale\u001B[38;5;241m=\u001B[39mrescale_factor) \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m images]\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/transformers/models/vit/image_processing_vit.py:262\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    259\u001B[0m images \u001B[38;5;241m=\u001B[39m [to_numpy_array(image) \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m images]\n\u001B[1;32m    261\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m do_resize:\n\u001B[0;32m--> 262\u001B[0m     images \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresize(image\u001B[38;5;241m=\u001B[39mimage, size\u001B[38;5;241m=\u001B[39msize_dict, resample\u001B[38;5;241m=\u001B[39mresample) \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m images]\n\u001B[1;32m    264\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m do_rescale:\n\u001B[1;32m    265\u001B[0m     images \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrescale(image\u001B[38;5;241m=\u001B[39mimage, scale\u001B[38;5;241m=\u001B[39mrescale_factor) \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m images]\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/transformers/models/vit/image_processing_vit.py:126\u001B[0m, in \u001B[0;36mViTImageProcessor.resize\u001B[0;34m(self, image, size, resample, data_format, **kwargs)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheight\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m size \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwidth\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m size:\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe `size` dictionary must contain the keys `height` and `width`. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msize\u001B[38;5;241m.\u001B[39mkeys()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resize(\n\u001B[1;32m    127\u001B[0m     image, size\u001B[38;5;241m=\u001B[39m(size[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheight\u001B[39m\u001B[38;5;124m\"\u001B[39m], size[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwidth\u001B[39m\u001B[38;5;124m\"\u001B[39m]), resample\u001B[38;5;241m=\u001B[39mresample, data_format\u001B[38;5;241m=\u001B[39mdata_format, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    128\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/transformers/image_transforms.py:309\u001B[0m, in \u001B[0;36mresize\u001B[0;34m(image, size, resample, reducing_gap, data_format, return_numpy)\u001B[0m\n\u001B[1;32m    307\u001B[0m height, width \u001B[38;5;241m=\u001B[39m size\n\u001B[1;32m    308\u001B[0m \u001B[38;5;66;03m# PIL images are in the format (width, height)\u001B[39;00m\n\u001B[0;32m--> 309\u001B[0m resized_image \u001B[38;5;241m=\u001B[39m image\u001B[38;5;241m.\u001B[39mresize((width, height), resample\u001B[38;5;241m=\u001B[39mresample, reducing_gap\u001B[38;5;241m=\u001B[39mreducing_gap)\n\u001B[1;32m    311\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_numpy:\n\u001B[1;32m    312\u001B[0m     resized_image \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(resized_image)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/PIL/Image.py:2174\u001B[0m, in \u001B[0;36mImage.resize\u001B[0;34m(self, size, resample, box, reducing_gap)\u001B[0m\n\u001B[1;32m   2166\u001B[0m             \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mreduce(\u001B[38;5;28mself\u001B[39m, factor, box\u001B[38;5;241m=\u001B[39mreduce_box)\n\u001B[1;32m   2167\u001B[0m         box \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2168\u001B[0m             (box[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_x,\n\u001B[1;32m   2169\u001B[0m             (box[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_y,\n\u001B[1;32m   2170\u001B[0m             (box[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_x,\n\u001B[1;32m   2171\u001B[0m             (box[\u001B[38;5;241m3\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_y,\n\u001B[1;32m   2172\u001B[0m         )\n\u001B[0;32m-> 2174\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mim\u001B[38;5;241m.\u001B[39mresize(size, resample, box))\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(attacker.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "with tqdm_notebook(total=epochs*len(dl)) as pbar:\n",
    "    step = 0\n",
    "    for epc in range(epochs):\n",
    "        for batch in dl:\n",
    "            input_tensor = batch['input'].to(device)\n",
    "            inter = model(input_tensor)\n",
    "            recovered = attacker(inter)\n",
    "            loss = torch.nn.functional.mse_loss(recovered, input_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            pbar.set_description(f'Epoch {epc} Step {step} Loss {loss.item()}')\n",
    "            pbar.update(1)\n",
    "            step += 1\n",
    "            # if step % 100 == 0:\n",
    "            #     test(model, attacker, sample)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "\n",
    "from sfl.utils.exp import get_dra_attacker, DRAConfig\n",
    "\n",
    "atk, _ = get_dra_attacker(\n",
    "    DRAConfig(target_model_name='vit-large', larger_better=False, target_sps='6-999', train_label='validation',\n",
    "              dataset='imagewoof', model='vit', tr2t_enable=False,prefix='gaussian:0.0001'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=224x224>",
      "image/png": "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",
      "image/jpeg": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=224x224>",
      "image/png": "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",
      "image/jpeg": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=224x224>",
      "image/png": "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",
      "image/jpeg": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "test_dl = ds.get_dataloader_unsliced(3, 'train')\n",
    "num = 3\n",
    "for sample in test_dl:\n",
    "    test(model, atk, sample)\n",
    "    num -= 1\n",
    "    if num == 0:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sfl/lib/python3.11/site-packages/datasets/load.py:926: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /root/autodl-tmp/sfl/datasets/piqa/piqa.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Parameter 'function'=<function FedDataset._pre_process.<locals>.<lambda> at 0x7f48f82f6700> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "\n",
    "from sfl.utils.exp import get_dataset, get_tokenizer\n",
    "\n",
    "tok = get_tokenizer('gpt2-large')\n",
    "tok.pad_token = tok.eos_token\n",
    "data = get_dataset('piqa',tok,[])\n",
    "dl = data.get_dataloader_unsliced(10,'test',1.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB (GPU 0; 23.65 GiB total capacity; 23.07 GiB already allocated; 78.06 MiB free; 23.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msfl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_best_gpu\n\u001B[1;32m      2\u001B[0m device \u001B[38;5;241m=\u001B[39m get_best_gpu()\n\u001B[0;32m----> 4\u001B[0m md\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/transformers/modeling_utils.py:1900\u001B[0m, in \u001B[0;36mPreTrainedModel.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1895\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1896\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`.to` is not supported for `4-bit` or `8-bit` models. Please use the model as it is, since the\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1897\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1898\u001B[0m     )\n\u001B[1;32m   1899\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1900\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/module.py:1145\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1141\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1142\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m   1143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[0;32m-> 1145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply(convert)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 797\u001B[0m         module\u001B[38;5;241m.\u001B[39m_apply(fn)\n\u001B[1;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 797\u001B[0m         module\u001B[38;5;241m.\u001B[39m_apply(fn)\n\u001B[1;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[0;31m[... skipping similar frames: Module._apply at line 797 (2 times)]\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    795\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    796\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 797\u001B[0m         module\u001B[38;5;241m.\u001B[39m_apply(fn)\n\u001B[1;32m    799\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    800\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    801\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    802\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    807\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    808\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/module.py:820\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    816\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    817\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    818\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    819\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 820\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m fn(param)\n\u001B[1;32m    821\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    822\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/module.py:1143\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m   1140\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m   1141\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1142\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m-> 1143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 0; 23.65 GiB total capacity; 23.07 GiB already allocated; 78.06 MiB free; 23.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from sfl.utils.model import get_best_gpu\n",
    "device = get_best_gpu()\n",
    "\n",
    "md.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sfl/lib/python3.11/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sfl.utils.model import generate\n",
    "\n",
    "generate(\"User: does the equation x^2+y^2=10 hold if x=2 and y=4? , Assistant: \",tok, md)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 2, 2, 1, 3]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "tensor = np.array([[0.1, 0.3, 0.4, 0.2, 0.6]])\n",
    "bins = [0.1, 0.3, 0.5]\n",
    "\n",
    "np.digitize(tensor, bins)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /data/stupidtree/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "306eb7270cf24bfd938428b27afcfbaf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "from sfl.utils.exp import get_model_and_tokenizer\n",
    "\n",
    "\n",
    "md, tok = get_model_and_tokenizer('vicuna',load_bits=32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### User: given the following EHR, predict the mortality and give your reason,Name|Gender|Length-of-stay|GCS|Heart Rate|Ethnicity|Age|\n",
      "Sam|Male|90000hours|43.2|102|Black|69|\n",
      "### Assistant: The probability of mortality, and the reasons are:\n",
      "\n",
      "| Name | Gender | Length-of-stay | GCS | Heart Rate | Ethnicity | Age |\n",
      "| --- | --- | --- | --- | --- | --- | --- |\n",
      "| Sam | Male | 90000hours | 43.2 | 102 | Black | 69 |\n",
      "\n",
      "The probability of mortality for this patient is 0.15, which means there is a 15% chance of death within the next year.\n",
      "\n",
      "The reasons for the high mortality probability are:\n",
      "\n",
      "1. The patient's age (69 years old) is a significant risk factor for mortality.\n",
      "2. The patient's GCS (43.2) is low, indicating a significant level of brain dysfunction, which can lead to poor outcomes and increased mortality risk.\n",
      "3. The patient's heart rate (102) is high, which can indicate various cardiovascular conditions and increase the risk of mortality.\n",
      "4. The patient's ethnicity (Black) is a risk factor for mortality, as certain ethnic groups may have a higher prevalence of certain health conditions.\n",
      "5. The patient's length of stay (90000 hours) is relatively long, which may indicate a more severe condition and a higher risk of mortality.\n",
      "\n",
      "Overall, the combination of these factors increases the patient's mortality risk, and it is recommended that the patient receives appropriate medical care and monitoring to address these issues and improve their outcomes.\n"
     ]
    }
   ],
   "source": [
    "from sfl.utils.model import generate\n",
    "\n",
    "res = generate('### User: given the following EHR, predict the mortality and give your reason,'\n",
    "               'Name|Gender|Length-of-stay|GCS|Heart Rate|Ethnicity|Age|\\n'\n",
    "               'Sam|Male|90000hours|43.2|102|Black|69|\\n'\n",
    "         '### Assistant: The probability of mortality, and the reasons are:', tok, md)\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}