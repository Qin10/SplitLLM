{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-10 18:19:02,928] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[93m [WARNING] \u001B[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001B[93m [WARNING] \u001B[0m async_io: please install the libaio-dev package with apt\n",
      "\u001B[93m [WARNING] \u001B[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001B[93m [WARNING] \u001B[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001B[93m [WARNING] \u001B[0m NVIDIA Inference is only supported on Ampere and newer architectures\n",
      "\u001B[93m [WARNING] \u001B[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001B[93m [WARNING] \u001B[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     UNEXPECTED_EOF_WHILE_READING] EOF occurred in\n",
      "[nltk_data]     violation of protocol (_ssl.c:1006)>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "import argparse\n",
    "from sfl.utils.args import FLConfig\n",
    "from sfl.utils.exp import get_model_and_tokenizer\n",
    "\n",
    "config = FLConfig(\n",
    "    collect_intermediates=False,\n",
    "    global_round=10,\n",
    "    client_evaluate_freq=500,\n",
    "    client_epoch=1,  # 每轮联邦每个Client训2轮\n",
    "    split_point_1=6,\n",
    "    split_point_2=30,  # [0,1 | 2,3,.... 29| 30, 31]\n",
    "    use_lora_at_trunk=True,  # 在trunk部分使用LoRA\n",
    "    use_lora_at_top=True,\n",
    "    use_lora_at_bottom=True,\n",
    "    use_lora_at_embed=True,\n",
    "    top_and_bottom_from_scratch='False',\n",
    "    attack_mode=None,\n",
    "    client_steps=700\n",
    ")\n",
    "\n",
    "args = {\n",
    "    'dataset_train_frac': 1.0,\n",
    "    'dataset_test_frac': 0.1,\n",
    "    'dataset': 'piqa',\n",
    "    'model_name': 'llama2',\n",
    "    'save_checkpoint': True,\n",
    "    'task_type': 'lm',\n",
    "    'attacker_freq': 10,\n",
    "    'attacker_samples':2,\n",
    "    'log_to_wandb': False\n",
    "}\n",
    "# convert to namespace\n",
    "args = argparse.Namespace(**args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "from torch import nn\n",
    "import  sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "import argparse\n",
    "from sfl.utils.args import FLConfig\n",
    "\n",
    "# md,tok = get_model_and_tokenizer('gpt2')\n",
    "#\n",
    "# md.config_sfl(config)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.ln = Linear(3, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ln(x)\n",
    "\n",
    "\n",
    "md = MLP()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "content = {'model':md.state_dict()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def transform_to_list(x):\n",
    "        if isinstance(x, list) or isinstance(x, tuple):\n",
    "            return [transform_to_list(each_x) for each_x in x]\n",
    "        elif isinstance(x, dict):\n",
    "            for key in x.keys():\n",
    "                x[key] = transform_to_list(x[key])\n",
    "            # x['order'] = '::'.join(list(x.keys()))\n",
    "            return x\n",
    "        else:\n",
    "            if hasattr(x, 'tolist'):\n",
    "               return x.tolist()\n",
    "            else:\n",
    "                return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "content1 = transform_to_list(content)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from experiments.notebook import gRPC_comm_manager_pb2\n",
    "\n",
    "splited_msg = gRPC_comm_manager_pb2.MessageRequest()  # map/dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def create_by_type(value, nested=False):\n",
    "    if isinstance(value, dict):\n",
    "        if isinstance(list(value.keys())[0], str):\n",
    "            m_dict = gRPC_comm_manager_pb2.mDict_keyIsString()\n",
    "            key_type = 'string'\n",
    "        else:\n",
    "            m_dict = gRPC_comm_manager_pb2.mDict_keyIsInt()\n",
    "            key_type = 'int'\n",
    "        for key in value.keys():\n",
    "            m_dict.dict_value[key].MergeFrom(\n",
    "                create_by_type(value[key], nested=True))\n",
    "        if nested:\n",
    "            msg_value = gRPC_comm_manager_pb2.MsgValue()\n",
    "            if key_type == 'string':\n",
    "                msg_value.dict_msg_string_key.MergeFrom(m_dict)\n",
    "            else:\n",
    "                msg_value.dict_msg_int_key.MergeFrom(m_dict)\n",
    "            return msg_value\n",
    "        else:\n",
    "            return m_dict\n",
    "    elif isinstance(value, list) or isinstance(value, tuple):\n",
    "        m_list = gRPC_comm_manager_pb2.mList()\n",
    "        for each in value:\n",
    "            m_list.list_value.append(create_by_type(each,\n",
    "                                                         nested=True))\n",
    "        if nested:\n",
    "            msg_value = gRPC_comm_manager_pb2.MsgValue()\n",
    "            msg_value.list_msg.MergeFrom(m_list)\n",
    "            return msg_value\n",
    "        else:\n",
    "            return m_list\n",
    "    else:\n",
    "        m_single = gRPC_comm_manager_pb2.mSingle()\n",
    "        if type(value) in [int, np.int32]:\n",
    "            m_single.int_value = value\n",
    "        elif type(value) in [str, bytes]:\n",
    "            m_single.str_value = value\n",
    "        elif type(value) in [float, np.float32]:\n",
    "            m_single.float_value = value\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'The data type {type(value)} has not been supported.')\n",
    "\n",
    "        if nested:\n",
    "            msg_value = gRPC_comm_manager_pb2.MsgValue()\n",
    "            msg_value.single_msg.MergeFrom(m_single)\n",
    "            return msg_value\n",
    "        else:\n",
    "            return m_single"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def build_msg_value(value):\n",
    "    msg_value = gRPC_comm_manager_pb2.MsgValue()\n",
    "\n",
    "    if isinstance(value, list) or isinstance(value, tuple):\n",
    "        msg_value.list_msg.MergeFrom(create_by_type(value))\n",
    "    elif isinstance(value, dict):\n",
    "        if isinstance(list(value.keys())[0], str):\n",
    "            msg_value.dict_msg_string_key.MergeFrom(\n",
    "                create_by_type(value))\n",
    "        else:\n",
    "            msg_value.dict_msg_int_key.MergeFrom(create_by_type(value))\n",
    "    else:\n",
    "        msg_value.single_msg.MergeFrom(create_by_type(value))\n",
    "\n",
    "    return msg_value\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "dict_msg_string_key",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m x \u001B[38;5;241m=\u001B[39m build_msg_value(\n\u001B[1;32m      2\u001B[0m             content)\n",
      "Cell \u001B[0;32mIn[14], line 8\u001B[0m, in \u001B[0;36mbuild_msg_value\u001B[0;34m(value)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mlist\u001B[39m(value\u001B[38;5;241m.\u001B[39mkeys())[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m----> 8\u001B[0m         msg_value\u001B[38;5;241m.\u001B[39mdict_msg_string_key\u001B[38;5;241m.\u001B[39mMergeFrom(\n\u001B[1;32m      9\u001B[0m             create_by_type(value))\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     11\u001B[0m         msg_value\u001B[38;5;241m.\u001B[39mdict_msg_int_key\u001B[38;5;241m.\u001B[39mMergeFrom(create_by_type(value))\n",
      "\u001B[0;31mAttributeError\u001B[0m: dict_msg_string_key"
     ]
    }
   ],
   "source": [
    "x = build_msg_value(\n",
    "            content)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "dict_msg_string_key",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m splited_msg\u001B[38;5;241m.\u001B[39mmsg[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mMergeFrom(build_msg_value(\n\u001B[1;32m      2\u001B[0m             content))\n",
      "Cell \u001B[0;32mIn[14], line 8\u001B[0m, in \u001B[0;36mbuild_msg_value\u001B[0;34m(value)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mlist\u001B[39m(value\u001B[38;5;241m.\u001B[39mkeys())[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m----> 8\u001B[0m         msg_value\u001B[38;5;241m.\u001B[39mdict_msg_string_key\u001B[38;5;241m.\u001B[39mMergeFrom(\n\u001B[1;32m      9\u001B[0m             create_by_type(value))\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     11\u001B[0m         msg_value\u001B[38;5;241m.\u001B[39mdict_msg_int_key\u001B[38;5;241m.\u001B[39mMergeFrom(create_by_type(value))\n",
      "\u001B[0;31mAttributeError\u001B[0m: dict_msg_string_key"
     ]
    }
   ],
   "source": [
    "splited_msg.msg['content'].MergeFrom()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "md.print_split_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sfl.utils.model import generate, get_output\n",
    "\n",
    "get_output(\"### User:Hi, what\", tok, md)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# md.print_split_model()\n",
    "md_lr = md.convert_to_lora_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "md_lr.print_split_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for nm,p in md_lr.named_parameters():\n",
    "    print(nm,p.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sfl.config import DRAConfig\n",
    "from sfl.utils.exp import get_dra_attacker\n",
    "\n",
    "atk_cfg = DRAConfig(target_model_name='llama2', target_dataset='sanitized', target_sps='6-6', train_label='val', target_model_load_bits=4)\n",
    "atk, _ = get_dra_attacker(atk_cfg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AdamW\n",
    "from sfl.utils.model import evaluate_attacker_rouge\n",
    "from sfl.utils.exp import get_dataset_class\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "client_ids = ['0']\n",
    "\n",
    "dataset_cls = get_dataset_class('sanitized')\n",
    "dataset = dataset_cls(tokenizer=tokenizer, client_ids=client_ids)\n",
    "test_loader = dataset.get_dataloader_unsliced(2, 'train', shrink_frac=0.2)\n",
    "\n",
    "avg_rouge = 0\n",
    "avg_rouge_dlg = 0\n",
    "step = 0\n",
    "opt = AdamW(model.parameters(), lr=1e-5)\n",
    "config.noise_mode = 'dxp'\n",
    "config.noise_scale = 1000.0\n",
    "model.config_sfl(config)\n",
    "\n",
    "dlg.to(model.device)\n",
    "atk.to(model.device)\n",
    "with tqdm_notebook(total=len(test_loader)) as pbar:\n",
    "  for batch in test_loader:\n",
    "    opt.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(model.device)\n",
    "    o1 = model(input_ids, batch['attention_mask'].to(model.device), labels=input_ids)\n",
    "    # o2 = dlg(tr2t.fx.to(model.device))\n",
    "    # print(o1)\n",
    "    o1.loss.backward()\n",
    "    b2tr, tr2t, all = model.get_all_inter()\n",
    "    opt.step()\n",
    "    pred = atk(b2tr.fx.to(model.device))\n",
    "    # print(batch['input_text'][0])\n",
    "    gt = dlg.fit(tr2t.fx.to(model.device), tr2t.grad.to(model.device), epochs=20, gt_init=pred)\n",
    "    # gt_texts = [tokenizer.decode(g.argmax(-1), skip_special_tokens=True) for g in gt]\n",
    "    avg_rouge += evaluate_attacker_rouge(tokenizer, pred, batch)['rouge-l']['f']\n",
    "    avg_rouge_dlg += evaluate_attacker_rouge(tokenizer, gt, batch)['rouge-l']['f']\n",
    "    step += 1\n",
    "    pbar.set_postfix({'dra_rouge': avg_rouge / step, 'dlg_rouge':avg_rouge_dlg/step})\n",
    "    pbar.update()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sfl.utils.exp import add_sfl_params\n",
    "import argparse\n",
    "from typing import Any\n",
    "from sfl.utils.model import Intermediate\n",
    "from sfl.simulator.strategy import BaseSFLStrategy\n",
    "from sfl.simulator.simulator import SFLSimulator\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "add_sfl_params(parser)\n",
    "args = parser.parse_args({})\n",
    "\n",
    "args.log_to_wandb = False\n",
    "args.dlg_epochs = 30\n",
    "args.dlg_init_with_dra = True\n",
    "\n",
    "# 定义Client本地学习策略\n",
    "class QAFLStrategy(BaseSFLStrategy):\n",
    "\n",
    "\n",
    "    def sample_attacker_triggered(self, global_round, client_id, local_epoch, local_step,\n",
    "                                  b2tr_inter: Intermediate, tr2t_inter: Intermediate,\n",
    "                                  all_inter: dict[Any, Intermediate],\n",
    "                                  batch, logs):\n",
    "        encoder_inter = all_inter.get('encoder', None)\n",
    "        with torch.no_grad():\n",
    "            for type, atk in zip(['b2tr', 'tr2t'], [self.dra1, self.dra2]):\n",
    "                if atk is None:\n",
    "                    continue\n",
    "                atk.to(self.simulator.device)\n",
    "                inter = b2tr_inter if type == 'b2tr' else tr2t_inter\n",
    "                if self.llm.type == 'encoder-decoder':\n",
    "                    attacked = atk(torch.concat([encoder_inter.fx.to(\n",
    "                        self.simulator.device), inter.fx.to(atk.device)], dim=1))\n",
    "                else:\n",
    "                    attacked = atk(inter.fx.to(atk.device))\n",
    "                rouge_res = calculate_rouge(self.tokenizer, attacked, batch['input_text'])\n",
    "                self.log_to_sample_result(client_id, f'attacker_{type}', rouge_res['rouge-l']['f'])\n",
    "                self.log_to_all_result(client_id, f'attacker_{type}', rouge_res['rouge-l']['f'])\n",
    "                logs[f'attacker_{type}_step'] = rouge_res['rouge-l']['f']\n",
    "        gt_init = None\n",
    "        if self.args.dlg_init_with_dra:\n",
    "            gt_init = attacked\n",
    "        self.dlg.to(self.simulator.device)\n",
    "        gt = self.dlg.fit(tr2t_inter.fx.to(self.simulator.device), tr2t_inter.grad.to(self.simulator.device),\n",
    "                          epochs=self.args.dlg_epochs,\n",
    "                          adjust=False,\n",
    "                          beta=self.args.dlg_beta,\n",
    "                          gt_init=gt_init,\n",
    "                          gt_reg=self.args.dlg_dra_reg,\n",
    "                          temp_range=self.args.dlg_temp_range,\n",
    "                          further_ft=self.args.dlg_further_ft,\n",
    "                          encoder_inter=None if encoder_inter is None else encoder_inter.fx.to(\n",
    "                              self.simulator.device)\n",
    "                          )\n",
    "        if self.llm.type == 'encoder-decoder':\n",
    "            # replace the latter half of attacked to gt\n",
    "            attacked[:, -gt.shape[1]:, :] = gt\n",
    "            rouge = calculate_rouge(self.tokenizer, attacked, batch['input_text'])\n",
    "        else:\n",
    "            rouge = calculate_rouge(self.tokenizer, gt, batch['input_text'])\n",
    "        self.log_to_sample_result(client_id, 'tag_rouge_lf', rouge['rouge-l']['f'])\n",
    "        self.log_to_all_result(client_id, 'tag_rouge_lf', rouge['rouge-l']['f'])\n",
    "        print(self.attack_all_performs)\n",
    "\n",
    "\n",
    "\n",
    "simulator = SFLSimulator(client_ids=client_ids,\n",
    "                             strategy=QAFLStrategy(args, model, tokenizer, test_loader, atk, None,dlg),\n",
    "                             llm=model,\n",
    "                             tokenizer=tokenizer,\n",
    "                             dataset=dataset, config=config, args=args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "simulator.simulate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 生成SensMarked数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "raw = pd.read_csv('/home/project/SFL-LLM/sanitized_data.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import pandas as pd\n",
    "raw = pd.read_csv('/home/project/SFL-LLM/sanitized_data.csv')\n",
    "# take only 'content' and 'entity'\n",
    "raw = raw[['content', 'entity', 'sani_gpt4','sani_label']]\n",
    "\n",
    "\n",
    "marked_content = []\n",
    "\n",
    "for i, row in tqdm_notebook(raw.iterrows(), total=len(raw)):\n",
    "    # get the content and entity\n",
    "    data = {}\n",
    "    content = row['content']\n",
    "    entity = row['entity']\n",
    "    data['sentence'] = content\n",
    "    # print(entity)\n",
    "    entity = ast.literal_eval(entity)\n",
    "    replaced_places = []\n",
    "    for e in entity:\n",
    "        indexes = [(m.start(), m.end()) for m in re.finditer(re.escape(e), content)]\n",
    "        for idx in indexes:\n",
    "            if any([idx[0] > r[0] and idx[1] < r[1] for r in replaced_places]):\n",
    "                continue\n",
    "            content = content[:idx[0]] + '<P>' + content[idx[0]:idx[1]] + '<\\P>' + content[idx[1]:]\n",
    "            replaced_places.append(idx)\n",
    "    marked_content.append(content)\n",
    "\n",
    "df = pd.DataFrame(marked_content, columns=['marked_content'])\n",
    "new = pd.concat([raw, df], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "type_labels = ['train'] * len(new)\n",
    "# randomly select 20% indexes\n",
    "all_indexes = list(range(len(new)))\n",
    "import random\n",
    "\n",
    "test_indexes = random.sample(all_indexes, int(len(new) * 0.25))\n",
    "for i in test_indexes:\n",
    "    type_labels[i] = 'test'\n",
    "\n",
    "all_indexes = list(set(all_indexes) - set(test_indexes))\n",
    "val_indexes = random.sample(all_indexes, int(len(new) * 0.15))\n",
    "for i in val_indexes:\n",
    "    type_labels[i] = 'validation'\n",
    "\n",
    "# make type_labels to dataframe and concat it with the original dataframe\n",
    "df = pd.DataFrame(type_labels, columns=['type'])\n",
    "# concat it with new\n",
    "new = pd.concat([new, df], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new.to_csv('/home/project/SFL-LLM/sensi.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "dataset = Dataset.from_pandas(new)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "\n",
    "# _, tokenizer = get_model_and_tokenizer('bert')\n",
    "# model, t = get_model_and_tokenizer('flan-t5-large')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from sfl.simulator.dataset import FedDataset\n",
    "\n",
    "\n",
    "class SanitizedFedDataset(FedDataset):\n",
    "\n",
    "    def _format(self, example):\n",
    "        return {'input': example['content'], 'entities': ast.literal_eval(example['entity'])}\n",
    "\n",
    "    def _col_fun(self, batch):\n",
    "        texts = [b['input'] for b in batch]\n",
    "        input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "        mask = torch.zeros_like(input['input_ids'])\n",
    "        for sp, sample in enumerate(batch):\n",
    "            seq = input['input_ids'][sp].numpy().tolist()\n",
    "            r = tokenizer(sample['entities'], add_special_tokens=False)\n",
    "            for subseq in r.input_ids:\n",
    "                for i in range(len(seq) - len(subseq) + 1):\n",
    "                    if seq[i:i + len(subseq)] == subseq:\n",
    "                        mask[sp, i:i + len(subseq)] = 1\n",
    "\n",
    "        return {'input_ids': input['input_ids'],\n",
    "                'attention_mask': input['attention_mask'],\n",
    "                'input_text': texts, 'entities': [b['entity'] for b in batch],\n",
    "                'input_santi_mask': mask}\n",
    "\n",
    "    def __init__(self, tokenizer, client_ids: list[str], ):\n",
    "        self.df = pd.read_csv('/home/project/SFL-LLM/sanitized_data_marked.csv')\n",
    "        dataset = {\n",
    "            'train': Dataset.from_pandas(self.df[self.df['type'] == 'train']),\n",
    "            'val': Dataset.from_pandas(self.df[self.df['type'] == 'val']),\n",
    "            'test': Dataset.from_pandas(self.df[self.df['type'] == 'test'])\n",
    "        }\n",
    "        super().__init__(tokenizer, client_ids, dataset, ['train', 'val', 'test'])\n",
    "\n",
    "\n",
    "ds = SanitizedFedDataset(tokenizer, ['0'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ld = ds.get_dataloader_unsliced(6, 'val')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for batch in ld:\n",
    "    # find input_ids masked by mask\n",
    "    input_ids = batch['input_ids']\n",
    "    mask = batch['input_santi_mask']\n",
    "    masked = input_ids * mask\n",
    "\n",
    "    print(tokenizer.decode(masked[0],skip_special_tokens=True))\n",
    "    print(batch['entities'][0])\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from sfl.utils.exp import get_model_and_tokenizer\n",
    "from sfl.utils.args import FLConfig\n",
    "from sfl.utils.model import get_best_gpu\n",
    "\n",
    "model, processor = get_model_and_tokenizer('vit-large')\n",
    "\n",
    "device = get_best_gpu()\n",
    "model.to(device)\n",
    "config = FLConfig(\n",
    "    collect_intermediates=False,\n",
    "    global_round=10,\n",
    "    client_evaluate_freq=500,\n",
    "    client_epoch=1,  # 每轮联邦每个Client训2轮\n",
    "    split_point_1=6,\n",
    "    split_point_2=20,  # [0,1 | 2,3,.... 29| 30, 31]\n",
    "    use_lora_at_trunk=False,  # 在trunk部分使用LoRA\n",
    "    use_lora_at_top=False,\n",
    "    use_lora_at_bottom=False,\n",
    "    top_and_bottom_from_scratch='True',\n",
    "    attack_mode='b2tr',\n",
    "    client_steps=700\n",
    ")\n",
    "\n",
    "model.config_sfl(config, None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sfl.utils.exp import get_dataset\n",
    "\n",
    "ds = get_dataset('imagewoof',processor, client_ids=['0'], shrink_frac=0.1)\n",
    "dl = ds.get_dataloader_unsliced(64,'train', 1.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sfl.model.attacker.dra_attacker import ViTDRAttacker\n",
    "from sfl.model.attacker.dra_attacker import ViTDRAttackerConfig\n",
    "\n",
    "attacker = ViTDRAttacker(ViTDRAttackerConfig(), model.config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sfl.utils.model import convert_to_image\n",
    "# train the attacker\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# attacker.to(device)\n",
    "\n",
    "def test(md, atk, image):\n",
    "    atk.to(md.device)\n",
    "    inter = md(image['input'].to(model.device))\n",
    "    image = convert_to_image(atk(inter))\n",
    "    image[0].show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(attacker.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "with tqdm_notebook(total=epochs*len(dl)) as pbar:\n",
    "    step = 0\n",
    "    for epc in range(epochs):\n",
    "        for batch in dl:\n",
    "            input_tensor = batch['input'].to(device)\n",
    "            inter = model(input_tensor)\n",
    "            recovered = attacker(inter)\n",
    "            loss = torch.nn.functional.mse_loss(recovered, input_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            pbar.set_description(f'Epoch {epc} Step {step} Loss {loss.item()}')\n",
    "            pbar.update(1)\n",
    "            step += 1\n",
    "            # if step % 100 == 0:\n",
    "            #     test(model, attacker, sample)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from sfl.utils.exp import get_dra_attacker, DRAConfig\n",
    "\n",
    "atk, _ = get_dra_attacker(\n",
    "    DRAConfig(target_model_name='vit-large', larger_better=False, target_sps='6-999', train_label='validation',\n",
    "              dataset='imagewoof', model='vit', tr2t_enable=False,prefix='gaussian:0.0001'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "test_dl = ds.get_dataloader_unsliced(3, 'train')\n",
    "num = 3\n",
    "for sample in test_dl:\n",
    "    test(model, atk, sample)\n",
    "    num -= 1\n",
    "    if num == 0:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "\n",
    "from sfl.utils.exp import get_dataset, get_tokenizer\n",
    "\n",
    "tok = get_tokenizer('gpt2-large')\n",
    "tok.pad_token = tok.eos_token\n",
    "data = get_dataset('piqa',tok,[])\n",
    "dl = data.get_dataloader_unsliced(10,'test',1.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sfl.utils.model import get_best_gpu\n",
    "device = get_best_gpu()\n",
    "\n",
    "md.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sfl.utils.model import generate\n",
    "\n",
    "generate(\"User: does the equation x^2+y^2=10 hold if x=2 and y=4? , Assistant: \",tok, md)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "tensor = np.array([[0.1, 0.3, 0.4, 0.2, 0.6]])\n",
    "bins = [0.1, 0.3, 0.5]\n",
    "\n",
    "np.digitize(tensor, bins)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "from sfl.utils.exp import get_model_and_tokenizer\n",
    "\n",
    "\n",
    "md, tok = get_model_and_tokenizer('vicuna',load_bits=32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sfl.utils.model import generate\n",
    "\n",
    "res = generate('### User: given the following EHR, predict the mortality and give your reason,'\n",
    "               'Name|Gender|Length-of-stay|GCS|Heart Rate|Ethnicity|Age|\\n'\n",
    "               'Sam|Male|90000hours|43.2|102|Black|69|\\n'\n",
    "         '### Assistant: The probability of mortality, and the reasons are:', tok, md)\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 2, 2, 1, 3]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "tensor = np.array([[0.1, 0.3, 0.4, 0.2, 0.6]])\n",
    "bins = [0.1, 0.3, 0.5]\n",
    "\n",
    "np.digitize(tensor, bins)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /data/stupidtree/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "306eb7270cf24bfd938428b27afcfbaf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "from sfl.utils.exp import get_model_and_tokenizer\n",
    "\n",
    "\n",
    "md, tok = get_model_and_tokenizer('vicuna',load_bits=32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### User: given the following EHR, predict the mortality and give your reason,Name|Gender|Length-of-stay|GCS|Heart Rate|Ethnicity|Age|\n",
      "Sam|Male|90000hours|43.2|102|Black|69|\n",
      "### Assistant: The probability of mortality, and the reasons are:\n",
      "\n",
      "| Name | Gender | Length-of-stay | GCS | Heart Rate | Ethnicity | Age |\n",
      "| --- | --- | --- | --- | --- | --- | --- |\n",
      "| Sam | Male | 90000hours | 43.2 | 102 | Black | 69 |\n",
      "\n",
      "The probability of mortality for this patient is 0.15, which means there is a 15% chance of death within the next year.\n",
      "\n",
      "The reasons for the high mortality probability are:\n",
      "\n",
      "1. The patient's age (69 years old) is a significant risk factor for mortality.\n",
      "2. The patient's GCS (43.2) is low, indicating a significant level of brain dysfunction, which can lead to poor outcomes and increased mortality risk.\n",
      "3. The patient's heart rate (102) is high, which can indicate various cardiovascular conditions and increase the risk of mortality.\n",
      "4. The patient's ethnicity (Black) is a risk factor for mortality, as certain ethnic groups may have a higher prevalence of certain health conditions.\n",
      "5. The patient's length of stay (90000 hours) is relatively long, which may indicate a more severe condition and a higher risk of mortality.\n",
      "\n",
      "Overall, the combination of these factors increases the patient's mortality risk, and it is recommended that the patient receives appropriate medical care and monitoring to address these issues and improve their outcomes.\n"
     ]
    }
   ],
   "source": [
    "from sfl.utils.model import generate\n",
    "\n",
    "res = generate('### User: given the following EHR, predict the mortality and give your reason,'\n",
    "               'Name|Gender|Length-of-stay|GCS|Heart Rate|Ethnicity|Age|\\n'\n",
    "               'Sam|Male|90000hours|43.2|102|Black|69|\\n'\n",
    "         '### Assistant: The probability of mortality, and the reasons are:', tok, md)\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "# Project is specified by <entity/project-name>\n",
    "runs = api.runs(\"stupidtree/[CR]BiSR(b+f)\")\n",
    "\n",
    "summary_list, config_list, name_list = [], [], []\n",
    "for run in runs:\n",
    "    # .summary contains the output keys/values for metrics like accuracy.\n",
    "    #  We call ._json_dict to omit large files\n",
    "    summary_list.append(run.summary._json_dict)\n",
    "\n",
    "    # .config contains the hyperparameters.\n",
    "    #  We remove special values that start with _.\n",
    "    config_list.append(\n",
    "        {k: v for k,v in run.config.items()\n",
    "          if not k.startswith('_')})\n",
    "\n",
    "    # .name is the human-readable name of the run.\n",
    "    name_list.append(run.name)\n",
    "\n",
    "runs_df = pd.DataFrame({\n",
    "    \"summary\": summary_list,\n",
    "    \"config\": config_list,\n",
    "    \"name\": name_list\n",
    "    })\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "model = 'llama2'\n",
    "metric = 'METEOR'\n",
    "methods = ['SIP_b2tr','BiSR(b)','BiSR(f)','BiSR(b+f)']\n",
    "all_datasets = runs_df['config'].apply(lambda x: x['dataset']).unique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIP_b2tr\t96.37$\\pm$1.18\t\t66.67$\\pm$0.89\t\t88.05$\\pm$0.25\t\t96.57$\\pm$0.26\t\t89.06$\\pm$0.54\t\t\n",
      "\n",
      "BiSR(b)\t97.42$\\pm$0.76\t\t85.65$\\pm$0.83\t\t93.80$\\pm$1.03\t\t97.10$\\pm$0.32\t\t94.65$\\pm$0.32\t\t\n",
      "\n",
      "BiSR(f)\t98.94$\\pm$0.34\t\t94.39$\\pm$1.22\t\t95.96$\\pm$1.05\t\t99.01$\\pm$0.29\t\t97.70$\\pm$0.19\t\t\n",
      "\n",
      "BiSR(b+f)\t99.47$\\pm$0.42\t\t96.58$\\pm$0.14\t\t97.71$\\pm$0.06\t\t99.46$\\pm$0.14\t\t98.03$\\pm$0.17\t\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select where config['model_name'] == model\n",
    "df_model = runs_df[runs_df['config'].apply(lambda x: x['model_name'] == model)]\n",
    "\n",
    "for method in methods:\n",
    "    print(method, end='\\t')\n",
    "    for dataset in all_datasets:\n",
    "        df = df_model[df_model['config'].apply(lambda x: x['dataset'] == dataset)]\n",
    "        seeds = df['config'].apply(lambda x: x['seed']).unique()\n",
    "        ress = []\n",
    "        for seed in seeds:\n",
    "            df_seed = df[df['config'].apply(lambda x: x['seed'] == seed)]\n",
    "            key = f'client0_{method}_{metric}_avg'\n",
    "            performance = df_seed['summary'].apply(lambda x: x[key]).mean()\n",
    "            ress.append(performance*100)\n",
    "\n",
    "        print(f'{np.mean(ress):.2f}$\\pm${np.std(ress):.2f}\\t',end='\\t')\n",
    "    print('\\n')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}