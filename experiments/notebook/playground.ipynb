{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = {\n",
    "  \"client0_attacker_14_normal_step\": 0.5175706596299099,\n",
    "  \"client0_attacker_19_normal_step\": 0.5429864203393666,\n",
    "  \"client0_attacker_32_normal_step\": 0.3857142807290817,\n",
    "  \"_step\": 4249,\n",
    "  \"client0_loss\": 2.031713839054108,\n",
    "  \"client0_attacker_29_tr2t_avg\": 0.5275647235774503,\n",
    "  \"client0_attacker_8_normal_avg\": 0.6366766778182428,\n",
    "  \"client0_attacker_16_normal_step\": 0.5285769033264401,\n",
    "  \"client0_attacker_24_normal_step\": 0.544871789879511,\n",
    "  \"client0_attacker_28_normal_step\": 0.4467532417557092,\n",
    "  \"client0_test-ppl\": 14.366519927978516,\n",
    "  \"client0_attacker_0_normal_avg\": 0.5362440276624566,\n",
    "  \"client0_attacker_32_normal_avg\": 0.4366247128481753,\n",
    "  \"client0_attacker_9_normal_step\": 0.5223380441441088,\n",
    "  \"client0_attacker_34_normal_avg\": 0.31704178424232093,\n",
    "  \"client0_attacker_35_normal_avg\": 0.27924916250140014,\n",
    "  \"client0_attacker_30_normal_step\": 0.4590469049036024,\n",
    "  \"global_step\": 4249,\n",
    "  \"client0_global_round\": 0,\n",
    "  \"client0_attacker_5_b2tr_avg\": 0.6304907277186129,\n",
    "  \"client0_attacker_3_normal_avg\": 0.6183885509769569,\n",
    "  \"client0_attacker_23_normal_step\": 0.5818181768206444,\n",
    "  \"client0_attacker_31_normal_step\": 0.41103848447329144,\n",
    "  \"client0_self_pt\": 0,\n",
    "  \"client0_attacker_11_normal_avg\": 0.6377722236693276,\n",
    "  \"client0_attacker_33_normal_avg\": 0.39310278683275807,\n",
    "  \"client0_attacker_6_normal_step\": 0.5439360879591302,\n",
    "  \"client0_attacker_2_normal_avg\": 0.6089481650353057,\n",
    "  \"client0_attacker_4_normal_avg\": 0.6238578184133798,\n",
    "  \"client0_attacker_22_normal_step\": 0.5893812020286877,\n",
    "  \"client0_attacker_25_normal_step\": 0.5504761854822313,\n",
    "  \"client0_attacker_13_normal_step\": 0.5263951684539971,\n",
    "  \"client0_attacker_9_normal_avg\": 0.6384095213150243,\n",
    "  \"client0_attacker_16_normal_avg\": 0.6330719245495596,\n",
    "  \"client0_attacker_24_normal_avg\": 0.5943958147410345,\n",
    "  \"client0_attacker_26_normal_avg\": 0.5723204183075637,\n",
    "  \"client0_attacker_15_normal_step\": 0.5175706596299099,\n",
    "  \"client0_attacker_5_b2tr_step\": 0.5145243232532479,\n",
    "  \"client0_attacker_7_normal_avg\": 0.6351210251036679,\n",
    "  \"client0_attacker_12_normal_avg\": 0.6366890118070682,\n",
    "  \"client0_attacker_13_normal_avg\": 0.6347554874791649,\n",
    "  \"client0_attacker_21_normal_avg\": 0.620009580633759,\n",
    "  \"client0_attacker_25_normal_avg\": 0.5867858649221694,\n",
    "  \"client0_attacker_27_normal_avg\": 0.5616850112415854,\n",
    "  \"client0_attacker_3_normal_step\": 0.5351071642617553,\n",
    "  \"_wandb.runtime\": 1649,\n",
    "  \"client0_local_epoch\": 0,\n",
    "  \"client0_attacker_10_normal_avg\": 0.6380667878549796,\n",
    "  \"client0_attacker_20_normal_avg\": 0.624557655192967,\n",
    "  \"client0_attacker_20_normal_step\": 0.5729323258308399,\n",
    "  \"client0_attacker_21_normal_step\": 0.5688311638336314,\n",
    "  \"client0_attacker_33_normal_step\": 0.33369407870222145,\n",
    "  \"client0_attacker_34_normal_step\": 0.30359476626650117,\n",
    "  \"client0_attacker_31_normal_avg\": 0.4747830768028701,\n",
    "  \"client0_attacker_10_normal_step\": 0.5096798162960074,\n",
    "  \"client0_attacker_12_normal_step\": 0.5175706596299099,\n",
    "  \"client0_attacker_26_normal_step\": 0.5203007468834715,\n",
    "  \"_runtime\": 1651.2562370300293,\n",
    "  \"_timestamp\": 1707696679.616939,\n",
    "  \"client0_attacker_1_normal_step\": 0.5177280500885659,\n",
    "  \"client0_attacker_28_normal_avg\": 0.5459204030441271,\n",
    "  \"client0_attacker_6_normal_avg\": 0.6324742085562801,\n",
    "  \"client0_attacker_17_normal_avg\": 0.6309008512450668,\n",
    "  \"client0_attacker_18_normal_avg\": 0.6301045366130664,\n",
    "  \"client0_attacker_4_normal_step\": 0.5473022862129748,\n",
    "  \"client0_attacker_17_normal_step\": 0.5469824243357921,\n",
    "  \"client0_attacker_15_normal_avg\": 0.6324732529184328,\n",
    "  \"client0_attacker_22_normal_avg\": 0.611303527540108,\n",
    "  \"client0_attacker_23_normal_avg\": 0.6043506538926331,\n",
    "  \"client0_attacker_7_normal_step\": 0.5562817669714759,\n",
    "  \"client0_self\": 0.0005098648707545071,\n",
    "  \"client0_local_step\": 4249,\n",
    "  \"client0_attacker_8_normal_step\": 0.5549242374281007,\n",
    "  \"client0_attacker_29_tr2t_step\": 0.454184699192842,\n",
    "  \"client0_attacker_2_normal_step\": 0.5012778336386043,\n",
    "  \"client0_attacker_11_normal_step\": 0.5135746556334841,\n",
    "  \"client0_attacker_0_normal_step\": 0.5027100221139359,\n",
    "  \"client0_attacker_14_normal_avg\": 0.6337987627417009,\n",
    "  \"client0_attacker_35_normal_step\": 0.2107142807178891,\n",
    "  \"client0_attacker_27_normal_step\": 0.5097402547483976,\n",
    "  \"client0_attacker_1_normal_avg\": 0.5966522677713505,\n",
    "  \"client0_attacker_19_normal_avg\": 0.627233252992626,\n",
    "  \"client0_attacker_30_normal_avg\": 0.5104403879249436,\n",
    "  \"client0_attacker_18_normal_step\": 0.5510835863330007\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FLConfig.__init__() got an unexpected keyword argument 'noise_scale'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msfl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FLConfig\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msfl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexp\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_model_and_tokenizer\n\u001B[0;32m----> 7\u001B[0m config \u001B[38;5;241m=\u001B[39m FLConfig(collect_intermediates\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m      8\u001B[0m                   global_round\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m,\n\u001B[1;32m      9\u001B[0m                   client_evaluate_freq\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m,\n\u001B[1;32m     10\u001B[0m                   client_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,  \u001B[38;5;66;03m# 每轮联邦每个Client训2轮\u001B[39;00m\n\u001B[1;32m     11\u001B[0m                   split_point_1\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m6\u001B[39m,\n\u001B[1;32m     12\u001B[0m                   split_point_2\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m,  \u001B[38;5;66;03m# [0,1 | 2,3,.... 29| 30, 31]\u001B[39;00m\n\u001B[1;32m     13\u001B[0m                   use_lora_at_trunk\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,  \u001B[38;5;66;03m# 在trunk部分使用LoRA\u001B[39;00m\n\u001B[1;32m     14\u001B[0m                   use_lora_at_top\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     15\u001B[0m                   use_lora_at_bottom\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     16\u001B[0m                   top_and_bottom_from_scratch\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFalse\u001B[39m\u001B[38;5;124m'\u001B[39m,  \u001B[38;5;66;03m# top和bottom都不采用预训练参数.\u001B[39;00m\n\u001B[1;32m     17\u001B[0m                   noise_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnone\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     18\u001B[0m                   noise_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m,  \u001B[38;5;66;03m# 噪声大小,\u001B[39;00m\n\u001B[1;32m     19\u001B[0m                   batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m     20\u001B[0m                   dataset_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     21\u001B[0m                   )\n",
      "\u001B[0;31mTypeError\u001B[0m: FLConfig.__init__() got an unexpected keyword argument 'noise_scale'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "from sfl.config import FLConfig\n",
    "from sfl.utils.exp import get_model_and_tokenizer\n",
    "config = FLConfig(collect_intermediates=True,\n",
    "                  global_round=4,\n",
    "                  client_evaluate_freq=500,\n",
    "                  client_epoch=1,  # 每轮联邦每个Client训2轮\n",
    "                  split_point_1=6,\n",
    "                  split_point_2=30,  # [0,1 | 2,3,.... 29| 30, 31]\n",
    "                  use_lora_at_trunk=True,  # 在trunk部分使用LoRA\n",
    "                  use_lora_at_top=True,\n",
    "                  use_lora_at_bottom=True,\n",
    "                  top_and_bottom_from_scratch='False',  # top和bottom都不采用预训练参数.\n",
    "                  noise_mode=\"none\",\n",
    "                  noise_scale=1000,  # 噪声大小,\n",
    "                  batch_size=2,\n",
    "                  dataset_type='train'\n",
    "                  )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from sfl.utils.exp import get_dlg_attacker\n",
    "tmp_model, _ = get_model_and_tokenizer('llama2', load_bits=32)\n",
    "tmp_model.config_sfl(config)\n",
    "dlg = get_dlg_attacker(tmp_model)\n",
    "del tmp_model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from sfl.utils.exp import get_model_and_tokenizer\n",
    "\n",
    "model, tokenizer = get_model_and_tokenizer('llama2', load_bits=4)\n",
    "\n",
    "model.config_sfl(config)\n",
    "model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fl_config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#\n",
    "# ts = get_output(\"test\", tokenizer, model)\n",
    "# # print(ts)\n",
    "# dlg = get_dlg_attacker(model)\n",
    "# model = model.convert_to_lora_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sfl.config import DRAConfig\n",
    "from sfl.utils.exp import get_dra_attacker\n",
    "\n",
    "atk_cfg = DRAConfig(target_model_name='llama2', target_dataset='sanitized', target_sps='6-6', train_label='val', target_model_load_bits=4)\n",
    "atk, _ = get_dra_attacker(atk_cfg)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AdamW\n",
    "from sfl.utils.model import evaluate_attacker_rouge\n",
    "from sfl.utils.exp import get_dataset_class\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "client_ids = ['0']\n",
    "\n",
    "dataset_cls = get_dataset_class('sanitized')\n",
    "dataset = dataset_cls(tokenizer=tokenizer, client_ids=client_ids)\n",
    "test_loader = dataset.get_dataloader_unsliced(2, 'train', shrink_frac=0.2)\n",
    "\n",
    "avg_rouge = 0\n",
    "avg_rouge_dlg = 0\n",
    "step = 0\n",
    "opt = AdamW(model.parameters(), lr=1e-5)\n",
    "config.noise_mode = 'dxp'\n",
    "config.noise_scale = 1000.0\n",
    "model.config_sfl(config)\n",
    "\n",
    "dlg.to(model.device)\n",
    "atk.to(model.device)\n",
    "with tqdm_notebook(total=len(test_loader)) as pbar:\n",
    "  for batch in test_loader:\n",
    "    opt.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(model.device)\n",
    "    o1 = model(input_ids, batch['input_att_mask'].to(model.device), labels=input_ids)\n",
    "    # o2 = dlg(tr2t.fx.to(model.device))\n",
    "    # print(o1)\n",
    "    o1.loss.backward()\n",
    "    b2tr, tr2t, all = model.get_all_inter()\n",
    "    opt.step()\n",
    "    pred = atk(b2tr.fx.to(model.device))\n",
    "    # print(batch['input_text'][0])\n",
    "    gt = dlg.fit(tr2t.fx.to(model.device), tr2t.grad.to(model.device), epochs=20, gt_init=pred)\n",
    "    # gt_texts = [tokenizer.decode(g.argmax(-1), skip_special_tokens=True) for g in gt]\n",
    "    avg_rouge += evaluate_attacker_rouge(tokenizer, pred, batch)['rouge-l']['f']\n",
    "    avg_rouge_dlg += evaluate_attacker_rouge(tokenizer, gt, batch)['rouge-l']['f']\n",
    "    step += 1\n",
    "    pbar.set_postfix({'dra_rouge': avg_rouge / step, 'dlg_rouge':avg_rouge_dlg/step})\n",
    "    pbar.update()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sfl.utils.exp import add_sfl_params\n",
    "import argparse\n",
    "from typing import Any\n",
    "from sfl.utils.model import Intermediate\n",
    "from sfl.simulator.strategy import BaseSFLStrategy\n",
    "from sfl.simulator.simulator import SFLSimulator\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "add_sfl_params(parser)\n",
    "args = parser.parse_args({})\n",
    "\n",
    "args.log_to_wandb = False\n",
    "args.dlg_epochs = 30\n",
    "args.dlg_init_with_dra = True\n",
    "\n",
    "# 定义Client本地学习策略\n",
    "class QAFLStrategy(BaseSFLStrategy):\n",
    "\n",
    "\n",
    "    def sample_attacker_triggered(self, global_round, client_id, local_epoch, local_step,\n",
    "                                  b2tr_inter: Intermediate, tr2t_inter: Intermediate,\n",
    "                                  all_inter: dict[Any, Intermediate],\n",
    "                                  batch, logs):\n",
    "        encoder_inter = all_inter.get('encoder', None)\n",
    "        with torch.no_grad():\n",
    "            for type, atk in zip(['b2tr', 'tr2t'], [self.dra1, self.dra2]):\n",
    "                if atk is None:\n",
    "                    continue\n",
    "                atk.to(self.simulator.device)\n",
    "                inter = b2tr_inter if type == 'b2tr' else tr2t_inter\n",
    "                if self.llm.type == 'encoder-decoder':\n",
    "                    attacked = atk(torch.concat([encoder_inter.fx.to(\n",
    "                        self.simulator.device), inter.fx.to(atk.device)], dim=1))\n",
    "                else:\n",
    "                    attacked = atk(inter.fx.to(atk.device))\n",
    "                rouge_res = calculate_rouge(self.tokenizer, attacked, batch['input_text'])\n",
    "                self.log_to_sample_result(client_id, f'attacker_{type}', rouge_res['rouge-l']['f'])\n",
    "                self.log_to_all_result(client_id, f'attacker_{type}', rouge_res['rouge-l']['f'])\n",
    "                logs[f'attacker_{type}_step'] = rouge_res['rouge-l']['f']\n",
    "        gt_init = None\n",
    "        if self.args.dlg_init_with_dra:\n",
    "            gt_init = attacked\n",
    "        self.dlg.to(self.simulator.device)\n",
    "        gt = self.dlg.fit(tr2t_inter.fx.to(self.simulator.device), tr2t_inter.grad.to(self.simulator.device),\n",
    "                          epochs=self.args.dlg_epochs,\n",
    "                          adjust=False,\n",
    "                          beta=self.args.dlg_beta,\n",
    "                          gt_init=gt_init,\n",
    "                          gt_reg=self.args.dlg_dra_reg,\n",
    "                          temp_range=self.args.dlg_temp_range,\n",
    "                          further_ft=self.args.dlg_further_ft,\n",
    "                          encoder_inter=None if encoder_inter is None else encoder_inter.fx.to(\n",
    "                              self.simulator.device)\n",
    "                          )\n",
    "        if self.llm.type == 'encoder-decoder':\n",
    "            # replace the latter half of attacked to gt\n",
    "            attacked[:, -gt.shape[1]:, :] = gt\n",
    "            rouge = calculate_rouge(self.tokenizer, attacked, batch['input_text'])\n",
    "        else:\n",
    "            rouge = calculate_rouge(self.tokenizer, gt, batch['input_text'])\n",
    "        self.log_to_sample_result(client_id, 'tag_rouge_lf', rouge['rouge-l']['f'])\n",
    "        self.log_to_all_result(client_id, 'tag_rouge_lf', rouge['rouge-l']['f'])\n",
    "        print(self.attack_all_performs)\n",
    "\n",
    "\n",
    "\n",
    "simulator = SFLSimulator(client_ids=client_ids,\n",
    "                             strategy=QAFLStrategy(args, model, tokenizer, test_loader, atk, None,dlg),\n",
    "                             llm=model,\n",
    "                             tokenizer=tokenizer,\n",
    "                             dataset=dataset, config=config, args=args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "simulator.simulate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import pandas as pd\n",
    "raw = pd.read_csv('/home/project/SFL-LLM/sanitized_data.csv')\n",
    "# take only 'content' and 'entity'\n",
    "raw = raw[['content', 'entity']]\n",
    "\n",
    "\n",
    "marked_content = []\n",
    "\n",
    "for i, row in tqdm_notebook(raw.iterrows(), total=len(raw)):\n",
    "    # get the content and entity\n",
    "    data = {}\n",
    "    content = row['content']\n",
    "    entity = row['entity']\n",
    "    data['sentence'] = content\n",
    "    # print(entity)\n",
    "    entity = ast.literal_eval(entity)\n",
    "    replaced_places = []\n",
    "    for e in entity:\n",
    "        indexes = [(m.start(), m.end()) for m in re.finditer(re.escape(e), content)]\n",
    "        for idx in indexes:\n",
    "            if any([idx[0] > r[0] and idx[1] < r[1] for r in replaced_places]):\n",
    "                continue\n",
    "            content = content[:idx[0]] + '<P>' + content[idx[0]:idx[1]] + '<\\P>' + content[idx[1]:]\n",
    "            replaced_places.append(idx)\n",
    "    marked_content.append(content)\n",
    "\n",
    "df = pd.DataFrame(marked_content, columns=['marked_content'])\n",
    "new = pd.concat([raw, df], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "type_labels = ['train'] * len(new)\n",
    "# randomly select 20% indexes\n",
    "all_indexes = list(range(len(new)))\n",
    "import random\n",
    "\n",
    "test_indexes = random.sample(all_indexes, int(len(new) * 0.25))\n",
    "for i in test_indexes:\n",
    "    type_labels[i] = 'test'\n",
    "\n",
    "all_indexes = list(set(all_indexes) - set(test_indexes))\n",
    "val_indexes = random.sample(all_indexes, int(len(new) * 0.15))\n",
    "for i in val_indexes:\n",
    "    type_labels[i] = 'val'\n",
    "\n",
    "# make type_labels to dataframe and concat it with the original dataframe\n",
    "df = pd.DataFrame(type_labels, columns=['type'])\n",
    "# concat it with new\n",
    "new = pd.concat([new, df], axis=1)\n",
    "new"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new.to_csv('/home/project/SFL-LLM/sanitized_data_marked.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "dataset = Dataset.from_pandas(new)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "from sfl.utils.exp import get_model_and_tokenizer\n",
    "\n",
    "_, tokenizer = get_model_and_tokenizer('bert')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from sfl.simulator.dataset import FedDataset\n",
    "\n",
    "\n",
    "class SanitizedFedDataset(FedDataset):\n",
    "\n",
    "    def _format(self, example):\n",
    "        return {'input': example['content'], 'entities': ast.literal_eval(example['entity'])}\n",
    "\n",
    "    def _col_fun(self, batch):\n",
    "        texts = [b['input'] for b in batch]\n",
    "        input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "        mask = torch.zeros_like(input['input_ids'])\n",
    "        for sp, sample in enumerate(batch):\n",
    "            seq = input['input_ids'][sp].numpy().tolist()\n",
    "            r = tokenizer(sample['entities'], add_special_tokens=False)\n",
    "            for subseq in r.input_ids:\n",
    "                for i in range(len(seq) - len(subseq) + 1):\n",
    "                    if seq[i:i + len(subseq)] == subseq:\n",
    "                        mask[sp, i:i + len(subseq)] = 1\n",
    "\n",
    "        return {'input_ids': input['input_ids'],\n",
    "                'input_att_mask': input['attention_mask'],\n",
    "                'input_text': texts, 'entities': [b['entity'] for b in batch],\n",
    "                'input_santi_mask': mask}\n",
    "\n",
    "    def __init__(self, tokenizer, client_ids: list[str], ):\n",
    "        self.df = pd.read_csv('/home/project/SFL-LLM/sanitized_data_marked.csv')\n",
    "        dataset = {\n",
    "            'train': Dataset.from_pandas(self.df[self.df['type'] == 'train']),\n",
    "            'val': Dataset.from_pandas(self.df[self.df['type'] == 'val']),\n",
    "            'test': Dataset.from_pandas(self.df[self.df['type'] == 'test'])\n",
    "        }\n",
    "        super().__init__(tokenizer, client_ids, dataset, ['train', 'val', 'test'])\n",
    "\n",
    "\n",
    "ds = SanitizedFedDataset(tokenizer, ['0'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ld = ds.get_dataloader_unsliced(6, 'val')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for batch in ld:\n",
    "    # find input_ids masked by mask\n",
    "    input_ids = batch['input_ids']\n",
    "    mask = batch['input_santi_mask']\n",
    "    masked = input_ids * mask\n",
    "\n",
    "    print(tokenizer.decode(masked[0],skip_special_tokens=True))\n",
    "    print(batch['entities'][0])\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "from sfl.config import FLConfig\n",
    "from sfl.utils.exp import get_model_and_tokenizer\n",
    "import argparse\n",
    "\n",
    "config = FLConfig(\n",
    "    collect_intermediates=False,\n",
    "    global_round=10,\n",
    "    client_evaluate_freq=500,\n",
    "    client_epoch=1,  # 每轮联邦每个Client训2轮\n",
    "    split_point_1=6,\n",
    "    split_point_2=30,  # [0,1 | 2,3,.... 29| 30, 31]\n",
    "    use_lora_at_trunk=True,  # 在trunk部分使用LoRA\n",
    "    use_lora_at_top=True,\n",
    "    use_lora_at_bottom=False,\n",
    "    top_and_bottom_from_scratch='True',\n",
    "    attack_mode='b2tr',\n",
    "    client_steps=700\n",
    ")\n",
    "\n",
    "args = {\n",
    "    'dataset_train_frac': 1.0,\n",
    "    'dataset_test_frac': 0.1,\n",
    "    'dataset': 'piqa',\n",
    "    'model_name': 'gpt2-large',\n",
    "    'save_checkpoint': True,\n",
    "    'task_type': 'lm',\n",
    "    'attacker_freq': 10000,\n",
    "    'log_to_wandb': False\n",
    "}\n",
    "# convert to namespace\n",
    "args = argparse.Namespace(**args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model, tokenizer = get_model_and_tokenizer(args.model_name)\n",
    "model.config_sfl(config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from sfl.utils.exp import get_dataset\n",
    "from sfl.config import DRA_train_label, DRA_test_label\n",
    "\n",
    "dataset = get_dataset(args.dataset, tokenizer,client_ids=['0'],shrink_frac=0.08)\n",
    "pub_loader = dataset.get_dataloader_unsliced(16, DRA_train_label[args.dataset], args.dataset_train_frac)\n",
    "test_loader = dataset.get_dataloader_unsliced(16, DRA_test_label[args.dataset], args.dataset_test_frac)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "FSHAAttacker(\n  (f_inv): GRUDRAttacker(\n    (gru): GRU(1280, 256, batch_first=True)\n    (mlp): Linear(in_features=256, out_features=50257, bias=True)\n  )\n  (f): GRUDRAttacker(\n    (gru): GRU(50257, 256, batch_first=True)\n    (mlp): Linear(in_features=256, out_features=1280, bias=True)\n  )\n  (d): GRU(1280, 256, batch_first=True)\n  (d_mlp): Sequential(\n    (0): Linear(in_features=256, out_features=1, bias=True)\n  )\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sfl.utils.model import get_best_gpu\n",
    "from sfl.model.attacker.fsha_attacker import FSHAAttacker, AutoEncoderConfig\n",
    "\n",
    "device = get_best_gpu()\n",
    "model.to(device)\n",
    "attacker = FSHAAttacker(AutoEncoderConfig(), target_config=model.config)\n",
    "attacker.to(model.device)\n",
    "# attacker.fit_auto_encoder(model, tokenizer,pub_loader,test_loader, 50, args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from sfl.utils.exp import get_fsha_attacker, DRAConfig\n",
    "#\n",
    "# attacker = get_fsha_attacker(DRAConfig(b2tr_sp=6,target_model_name='gpt2-large',target_sps='6-6', train_label='validation',dataset='piqa'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sfl/lib/python3.11/site-packages/peft/tuners/lora.py:299: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================Global Round 0=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_543624/3998992960.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  with tqdm_notebook(total=config.client_steps) as pbar:\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/700 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23fe44388f9c49c582af0457669ae8b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:0.00 Bytes, downlink:0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "Global Round 0 communication overhead: uplink=0.00 Bytes, downlink=0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "==================================Global Round 1=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/700 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33ccd8734cf94a6d8ef239cfbea6dece"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:0.00 Bytes, downlink:0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "Global Round 1 communication overhead: uplink=0.00 Bytes, downlink=0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "==================================Global Round 2=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/700 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f5aa53e3ef81456b86d02f4c9a4f36d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:0.00 Bytes, downlink:0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "Global Round 2 communication overhead: uplink=0.00 Bytes, downlink=0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "==================================Global Round 3=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/700 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3afe56af7e2f4fc48844cf308300453e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:0.00 Bytes, downlink:0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "Global Round 3 communication overhead: uplink=0.00 Bytes, downlink=0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "==================================Global Round 4=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/700 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7150356da7ad46489f183a0e731edbd8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:0.00 Bytes, downlink:0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "Global Round 4 communication overhead: uplink=0.00 Bytes, downlink=0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "==================================Global Round 5=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/700 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "608eefaa2ee145c4bff9cf1ed1b5ea33"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:0.00 Bytes, downlink:0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "Global Round 5 communication overhead: uplink=0.00 Bytes, downlink=0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "==================================Global Round 6=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/700 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63024cc92c4e40f396aa941d67f37179"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:0.00 Bytes, downlink:0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "Global Round 6 communication overhead: uplink=0.00 Bytes, downlink=0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "==================================Global Round 7=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/700 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ccf6e22b2df540ec94da07775d7df11c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:0.00 Bytes, downlink:0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "Global Round 7 communication overhead: uplink=0.00 Bytes, downlink=0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "==================================Global Round 8=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/700 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50f31a4c3f184363975d847fd71f0165"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:0.00 Bytes, downlink:0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "Global Round 8 communication overhead: uplink=0.00 Bytes, downlink=0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "==================================Global Round 9=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/700 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92e8ddc0b7214c83b32657b2d23ba99b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:0.00 Bytes, downlink:0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "Global Round 9 communication overhead: uplink=0.00 Bytes, downlink=0.00 Bytes\n",
      "SERVER: AGGREGATION\n",
      "FL communication overhead: uplink=0.00 Bytes, downlink=0.00 Bytes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sfl.simulator.simulator import SFLSimulator\n",
    "from sfl.utils.model import get_t5_input, calc_unshift_loss\n",
    "from sfl.model.attacker.fsha_attacker import FSHAAttacker\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Iterator\n",
    "from sfl.model.llm.split_model import SplitWrapperModel\n",
    "from sfl.simulator.strategy import BaseSFLStrategy\n",
    "from torch.optim import AdamW, Adam\n",
    "from tqdm import tqdm_notebook\n",
    "from sfl.utils.model import calculate_rouge\n",
    "import torch\n",
    "\n",
    "\n",
    "class FSHAStrategy(BaseSFLStrategy):\n",
    "\n",
    "    def __init__(self, args, llm, tokenizer, attacker: FSHAAttacker, pub_loader: DataLoader):\n",
    "        super().__init__(args, llm, tokenizer)\n",
    "        self.attacker = attacker\n",
    "        self.pub_loader = pub_loader\n",
    "        self.pub_loader_iter = iter(pub_loader)\n",
    "        self.optim_d = Adam(list(self.attacker.d_mlp.parameters())+list(self.attacker.d.parameters()),lr=1e-5, weight_decay=1e-6)\n",
    "        self.optim_f = Adam(list(self.attacker.f.parameters())+list(self.attacker.f_inv.parameters()),lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "    def client_step(self, client_id: str, global_round, client_epoch, llm: SplitWrapperModel, iterator: Iterator,\n",
    "                    config: FLConfig):\n",
    "        optimizer = Adam([p for _, p in llm.get_top_params()], lr=5e-7, weight_decay=1e-7)\n",
    "        # optimizer = AdamW([p for _, p in llm.get_top_params()], lr=3e-7, weight_decay=1e-4)\n",
    "        avg_d_loss = 0\n",
    "        avg_f_loss = 0\n",
    "        avg_rouge_lf = 0\n",
    "        batch_num = 0\n",
    "        with tqdm_notebook(total=config.client_steps) as pbar:\n",
    "            for step, batch in enumerate(iterator):\n",
    "                if llm.type == 'encoder-decoder':\n",
    "                    outputs = llm(**get_t5_input(batch, self.tokenizer, llm.device))\n",
    "                else:\n",
    "                    input_ids = batch['input_ids'].to(llm.device)\n",
    "                    attention_mask = batch['input_att_mask'].to(llm.device)\n",
    "                    labels = input_ids\n",
    "                    if 'labels' in batch and self.task_type == 'clsf':\n",
    "                        labels = batch['labels'].to(llm.device)\n",
    "                    outputs = llm(input_ids=input_ids, labels=labels, attention_mask=attention_mask)\n",
    "                z_priv = outputs\n",
    "                try:\n",
    "                    x_pub = next(self.pub_loader_iter)\n",
    "                except StopIteration:\n",
    "                    self.pub_loader_iter = iter(self.pub_loader)\n",
    "                    x_pub = next(self.pub_loader_iter)\n",
    "                x_pub = x_pub['input_ids'].to(llm.device)\n",
    "                z_pub = self.attacker.f_forward(x_pub)\n",
    "                adv_priv_logits = self.attacker.d_forward(z_priv)\n",
    "                adv_pub_logits = self.attacker.d_forward(z_pub)\n",
    "                # print('pub', adv_pub_logits, 'priv', adv_priv_logits)\n",
    "\n",
    "                # f_loss = torch.mean(adv_priv_logits)\n",
    "                f_loss = torch.mean(\n",
    "                    torch.binary_cross_entropy_with_logits(adv_priv_logits, torch.ones_like(adv_priv_logits)))\n",
    "\n",
    "                d_loss_true = torch.mean(\n",
    "                    torch.binary_cross_entropy_with_logits(adv_pub_logits, torch.ones_like(adv_pub_logits)\n",
    "                                                           ))\n",
    "                d_loss_fake = torch.mean(\n",
    "                    torch.binary_cross_entropy_with_logits(adv_priv_logits, torch.zeros_like(adv_priv_logits)))\n",
    "                d_loss = (d_loss_true + d_loss_fake) / 2\n",
    "                # d_loss_true = torch.mean(adv_pub_logits)\n",
    "                # d_loss_fake = -torch.mean(adv_priv_logits)\n",
    "                # # print(d_loss_true, d_loss_fake)\n",
    "                # d_loss = d_loss_true + d_loss_fake\n",
    "                rec_x_pub = self.attacker.f_inv_forward(z_pub)\n",
    "                inv_loss = calc_unshift_loss(rec_x_pub, x_pub)\n",
    "\n",
    "                rec_x_priv = self.attacker.f_inv_forward(z_priv)\n",
    "                recover_rouge = calculate_rouge(self.tokenizer, rec_x_priv, batch['input_text'])\n",
    "                avg_rouge_lf += recover_rouge['rouge-l']['f']\n",
    "\n",
    "                # (d_loss+f_loss).backward()\n",
    "                self.optim_d.zero_grad()\n",
    "                self.optim_f.zero_grad()\n",
    "                (inv_loss+d_loss).backward(retain_graph=True)\n",
    "                self.optim_d.step()\n",
    "                self.optim_f.step()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                f_grad = torch.autograd.grad(f_loss, z_priv)[0]\n",
    "                z_priv.backward(f_grad)\n",
    "                optimizer.step()\n",
    "                # optimizer.step()\n",
    "\n",
    "                batch_num += 1\n",
    "                avg_d_loss += d_loss.detach().cpu().item()\n",
    "                avg_f_loss += f_loss.detach().cpu().item()\n",
    "                pbar.set_description(\n",
    "                    f'Client {client_id} HIJACK Epoch {client_epoch} Step {self.simulator.get_current_step(client_id, step)} D_Loss {d_loss.item():.3f}, F_Loss {f_loss.item():.3f}, Rouge_L_F {recover_rouge[\"rouge-l\"][\"f\"]:.3f}, Avg_Rouge_L_F {avg_rouge_lf / (step + 1):.3f}')\n",
    "                self.step_done(client_id, step, batch,\n",
    "                               {\"d_loss\": float(avg_d_loss / batch_num),\n",
    "                                \"f_loss\": float(avg_f_loss / batch_num),\n",
    "                                \"rouge_l_f\": float(avg_rouge_lf / batch_num),\n",
    "                                })\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "attacker.to(model.device)\n",
    "attacker.train()\n",
    "simulator = SFLSimulator(client_ids=['0'],\n",
    "                         strategy=FSHAStrategy(args, model, tokenizer, attacker, pub_loader),\n",
    "                         llm=model,\n",
    "                         tokenizer=tokenizer,\n",
    "                         dataset=dataset, config=config, args=args)\n",
    "\n",
    "simulator.simulate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}