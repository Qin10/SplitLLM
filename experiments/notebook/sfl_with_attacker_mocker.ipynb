{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1. 加载模型与Tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "GPT2SplitLMHeadModel(\n  (transformer): GPT2SplitModel(\n    (wte): Embedding(50257, 1280)\n    (wpe): Embedding(1024, 1280)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-35): 36 x GPT2Block(\n        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "from sfl.utils.training import get_best_gpu\n",
    "from transformers import AutoTokenizer\n",
    "from sfl.model.gpt2.gpt2_split import GPT2SplitLMHeadModel\n",
    "from sfl import config\n",
    "\n",
    "device = get_best_gpu()\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(config.model_download_dir, \"gpt2-large/\"))\n",
    "model = GPT2SplitLMHeadModel.from_pretrained(os.path.join(config.model_download_dir, \"gpt2-large/\"))\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = 50256\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To mix food coloring with sugar, you can use the following:\n",
      "\n",
      "1 1/2 cups powdered sugar (or 1 cup granulated sugar plus 1 teaspoon of cornstarch mixed with 3/4 cup water)\n",
      "\n",
      "\n",
      "2 tablespoons corn starch (also known as corn syrup) or other sweetener (such as xylitol or stevia, or a combination of the two, such as erythritol and sorbitol) (see below for more information on sweeteners\n"
     ]
    }
   ],
   "source": [
    "from sfl.utils.model import generate\n",
    "\n",
    "\n",
    "# 测试模型输出\n",
    "def get_output(text, md=model):\n",
    "    t = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    res = model(t['input_ids'].to(md.device), attention_mask=t['attention_mask'].to(md.device))\n",
    "    r = tokenizer.decode(res.logits.argmax(dim=-1)[-1], skip_special_tokens=True)\n",
    "    return r\n",
    "\n",
    "\n",
    "print(generate(\"To mix food coloring with sugar, you can\", tokenizer, model))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from sfl.config import attacker_path\n",
    "from sfl.utils.training import get_attacker_class, extract_attacker_path\n",
    "\n",
    "# 加载攻击模型\n",
    "# attacker_cls = get_attacker_class('gru')\n",
    "# attacker_path_1, attacker_path_2 = extract_attacker_path(\n",
    "#     {'split_point_1': 2, 'split_point_2': 30, 'attacker_path': attacker_path, 'model_name': 'gpt2-large','attacker_dataset':'piqa','attacker_train_label':'test','attacker_train_frac':1.0,'attack_model':'gru','attacker_prefix':'normal'})\n",
    "# attacker2 = attacker_cls.from_pretrained(attacker_path_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2. 初始化攻击模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3. 设置联邦训练流程"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 97])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "torch.Size([2, 22])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "torch.Size([2, 94])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================Global Round 0=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/96 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a6994b2d3aed43c0a7b8ef488947d25d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: ,,ianiast\n",
      "\n",
      ".b????xyjster\n",
      "REAL: penny, Solution: can replace a stove \n",
      "MOCK: ,,ianiast\n",
      "\n",
      ".bianky\n",
      "_\n"
     ]
    },
    {
     "data": {
      "text/plain": "Mocking:   0%|          | 0/1000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fab20c891a02487ba8c6c155f9f222c8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: ,..ge\n",
      "on\n",
      "\n",
      "\n",
      "unction:\n",
      "Solution\n",
      "- Solution Solution.\n",
      " kvenum:- kraseeh\n",
      "ata hite Sicis- kbingspelss Sc Sc Scal Scal\n",
      "REAL: How to best cut the meat to place on a grill?, Solution: Place a knife on the grill for around 15 minutes for the blade to be red hot. Gently push the knife through the meat to get a perfect cut.\n",
      "MOCK: ...ston\n",
      "on,\n",
      "\n",
      "acet:\n",
      ":\n",
      "p Solution Solutionos\n",
      " kfut:- kraseeh\n",
      "ata:ite:ings- kbingsarss Sc Scal Scal Scal\n"
     ]
    },
    {
     "data": {
      "text/plain": "Mocking:   0%|          | 0/1000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e106401b64284a49b05d2961809a0261"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 134\u001B[0m\n\u001B[1;32m    132\u001B[0m mocker \u001B[38;5;241m=\u001B[39m GPT2TopMocker(config, model)\n\u001B[1;32m    133\u001B[0m mocker\u001B[38;5;241m.\u001B[39mto(simulator\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m--> 134\u001B[0m simulator\u001B[38;5;241m.\u001B[39msimulate()\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/simulator/simulator.py:64\u001B[0m, in \u001B[0;36mSFLSimulator.simulate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;66;03m# sequentially train each client\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m client_id \u001B[38;5;129;01min\u001B[39;00m sampled_clients:\n\u001B[0;32m---> 64\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client_step(i, client_id)\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__summarize_communication(i, client_id)\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__summarize_communication(i, client_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/simulator/simulator.py:81\u001B[0m, in \u001B[0;36mSFLSimulator._client_step\u001B[0;34m(self, global_round, client_id)\u001B[0m\n\u001B[1;32m     79\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m---> 81\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mclient_step(global_round, client_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm,\n\u001B[1;32m     82\u001B[0m                           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mget_dataloader(client_id, \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalidation\u001B[39m\u001B[38;5;124m'\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig)\n\u001B[1;32m     83\u001B[0m \u001B[38;5;66;03m# store updated client parameters\u001B[39;00m\n\u001B[1;32m     84\u001B[0m cm \u001B[38;5;241m=\u001B[39m ([p\u001B[38;5;241m.\u001B[39mcpu() \u001B[38;5;28;01mfor\u001B[39;00m nm, p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mget_top_params()],\n\u001B[1;32m     85\u001B[0m       [p\u001B[38;5;241m.\u001B[39mcpu() \u001B[38;5;28;01mfor\u001B[39;00m nm, p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mget_bottom_params()])\n",
      "Cell \u001B[0;32mIn[7], line 53\u001B[0m, in \u001B[0;36mQAFLStrategy.client_step\u001B[0;34m(self, global_round, client_id, llm, dataloader, cfg)\u001B[0m\n\u001B[1;32m     51\u001B[0m pbar\u001B[38;5;241m.\u001B[39mset_description(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mClient \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclient_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     52\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 53\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbp_done(client_id, epoch, step, batch)  \u001B[38;5;66;03m# Collect gradients\u001B[39;00m\n\u001B[1;32m     54\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     55\u001B[0m pbar\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/simulator/strategy.py:51\u001B[0m, in \u001B[0;36mFLStrategy.bp_done\u001B[0;34m(self, client_id, local_epoch, local_step, batch)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbp_done\u001B[39m(\u001B[38;5;28mself\u001B[39m, client_id, local_epoch, local_step, batch):\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msimulator\u001B[38;5;241m.\u001B[39m_collect_bp_result(client_id, local_epoch, local_step, batch)\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/simulator/simulator.py:148\u001B[0m, in \u001B[0;36mSFLSimulator._collect_bp_result\u001B[0;34m(self, client_id, local_epoch, local_step, batch)\u001B[0m\n\u001B[1;32m    145\u001B[0m b2tr_fx, tr2b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mget_trunk_to_bottom_grad()  \u001B[38;5;66;03m# trunk-to-bottom\u001B[39;00m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;66;03m# b2tr_fx = self.llm.get_bottom_to_trunk_fx()  # bottom-to-trunk\u001B[39;00m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;66;03m# tr2t_fx = self.llm.get_trunk_to_top_fx()  # trunk-to-top\u001B[39;00m\n\u001B[0;32m--> 148\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mcallback_bp_param(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_global_round, client_id, local_epoch, local_step,\n\u001B[1;32m    149\u001B[0m                                 b2tr_fx, tr2b,\n\u001B[1;32m    150\u001B[0m                                 tr2t_fx, t2tr,\n\u001B[1;32m    151\u001B[0m                                 batch)\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommunication_overhead_uplink\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_global_round, {})\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommunication_overhead_uplink[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_global_round]\u001B[38;5;241m.\u001B[39msetdefault(client_id, {})\n",
      "Cell \u001B[0;32mIn[7], line 106\u001B[0m, in \u001B[0;36mQAFLStrategy.callback_bp_param\u001B[0;34m(self, global_round, client_id, local_epoch, local_step, b2tr_fx, tr2b_grad, tr2t_fx, t2tr_grad, batch)\u001B[0m\n\u001B[1;32m    104\u001B[0m x \u001B[38;5;241m=\u001B[39m mocker(inter)\n\u001B[1;32m    105\u001B[0m loss \u001B[38;5;241m=\u001B[39m calc_shifted_loss_logits(x, torch\u001B[38;5;241m.\u001B[39msoftmax(gt,dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m--> 106\u001B[0m grad \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mgrad(loss, inter, create_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    107\u001B[0m grad_diff \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m gx, gy \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(grad, t2tr_grad\u001B[38;5;241m.\u001B[39mto(loss\u001B[38;5;241m.\u001B[39mdevice)):\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/autograd/__init__.py:303\u001B[0m, in \u001B[0;36mgrad\u001B[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001B[0m\n\u001B[1;32m    301\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _vmap_internals\u001B[38;5;241m.\u001B[39m_vmap(vjp, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, allow_none_pass_through\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)(grad_outputs_)\n\u001B[1;32m    302\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 303\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    304\u001B[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001B[1;32m    305\u001B[0m         allow_unused, accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sfl.utils.training import calc_shifted_loss_logits\n",
    "from sfl.utils.model import calculate_rouge\n",
    "from sfl.model.mocker import GPT2TopMocker\n",
    "from sfl.simulator.simulator import SFLSimulator\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sfl.model.split_model import SplitModel\n",
    "from sfl.simulator.strategy import FLStrategy\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from sfl.config import FLConfig\n",
    "\n",
    "client_ids = [str(i) for i in range(3)]\n",
    "config = FLConfig(global_round=50,\n",
    "                  client_epoch=2,  # 每轮联邦每个Client训2轮\n",
    "                  split_point_1=2,\n",
    "                  split_point_2=30,  # [0,1 | 2,3,.... 29| 30, 31]\n",
    "                  use_lora_at_trunk=True,  # 在trunk部分使用LoRA\n",
    "                  top_and_bottom_from_scratch='False',  # top和bottom都不采用预训练参数.\n",
    "                  noise_mode=\"dxp\",\n",
    "                  noise_scale=3.0,  # 噪声大小\n",
    "                  )\n",
    "\n",
    "from sfl.simulator.dataset import PIQAFedDataset\n",
    "\n",
    "fed_dataset = PIQAFedDataset(tokenizer=tokenizer, client_ids=client_ids, shrink_frac=0.15)\n",
    "\n",
    "\n",
    "# 定义Client本地学习策略\n",
    "class QAFLStrategy(FLStrategy):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attacker_rouge_b2tr = []\n",
    "        self.attacker_rouge_tr2t = []\n",
    "        self.mocker_rouge_tr2t = []\n",
    "        self.client_logs = {}\n",
    "\n",
    "    def client_step(self, global_round, client_id: str, llm: SplitModel, dataloader: DataLoader, cfg: FLConfig):\n",
    "        optimizer = AdamW(llm.parameters(), lr=1e-5)\n",
    "        with tqdm_notebook(total=cfg.client_epoch * len(dataloader)) as pbar:\n",
    "            for epoch in range(cfg.client_epoch):\n",
    "                for step, batch in enumerate(dataloader):\n",
    "                    optimizer.zero_grad()\n",
    "                    input_ids = batch['input_ids'].to(llm.device)\n",
    "                    attention_mask = batch['input_att_mask'].to(llm.device)\n",
    "                    outputs = llm(input_ids=input_ids, labels=input_ids, attention_mask=attention_mask)\n",
    "                    self.fp_done(client_id, epoch, step, batch)  # Collect intermediate results\n",
    "                    loss = outputs.loss\n",
    "                    pbar.set_description(f'Client {client_id} Epoch {epoch} Loss {loss.item():.3f}')\n",
    "                    loss.backward()\n",
    "                    self.bp_done(client_id, epoch, step, batch)  # Collect gradients\n",
    "                    optimizer.step()\n",
    "                    pbar.update(1)\n",
    "                if len(self.mocker_rouge_tr2t) > 0:\n",
    "                    avg_rouge2 = sum([r[\"rouge-l\"][\"f\"] for r in self.mocker_rouge_tr2t]) / len(self.mocker_rouge_tr2t)\n",
    "                    print(f'MOCK! Bottom-trunk, Client {client_id} Epoch {epoch} RougeL {avg_rouge2:.3f}')\n",
    "                    self.attacker_rouge_b2tr.clear()\n",
    "                    self.attacker_rouge_tr2t.clear()\n",
    "                    self.mocker_rouge_tr2t.clear()\n",
    "                self.client_logs.setdefault(client_id, {})\n",
    "                # self.client_logs[client_id][epoch] = {\"bottom-trunk\": avg_rouge1, \"trunk-top\": avg_rouge2}\n",
    "\n",
    "    def aggregation_step(self, global_round, params):\n",
    "        report = {}\n",
    "        report['global_round'] = global_round\n",
    "        for cid, epochs in self.client_logs.items():\n",
    "            for epc, rep in epochs.items():\n",
    "                for k, v in rep.items():\n",
    "                    report[f'client{cid}-epoch{epc}-{k}'] = v\n",
    "        # wandb.log(report)\n",
    "        self.client_logs = {}\n",
    "        return super(QAFLStrategy, self).aggregation_step(global_round, params)\n",
    "\n",
    "    def callback_fp_param(self, global_round, client_id, local_epoch, local_step, b2tr_params, tr2t_params, batch):\n",
    "        pass\n",
    "\n",
    "    def callback_bp_param(self, global_round, client_id, local_epoch, local_step,\n",
    "                          b2tr_fx, tr2b_grad,\n",
    "                          tr2t_fx, t2tr_grad,\n",
    "                          batch):\n",
    "        #if global_round % 10 == 0 and client_id == '0' and local_epoch == 1:\n",
    "            # self.collect_fp_bp.append((tr2t_fx, t2tr_grad, batch['input_text']))\n",
    "            real = batch['input_text'][0]\n",
    "            print(\"REAL:\", real)\n",
    "            out = mocker(tr2t_fx.to(self.simulator.device))\n",
    "            out_sent = tokenizer.decode(out[0].argmax(dim=-1).tolist(), skip_special_tokens=True,\n",
    "                                        clean_up_tokenization_spaces=True)\n",
    "            print(\"MOCK:\", out_sent)\n",
    "            batch_size, seq_len = tr2t_fx.shape[:2]\n",
    "            inter = tr2t_fx.to(self.simulator.device)\n",
    "            vocab_size = model.config.vocab_size\n",
    "            gt = torch.softmax(torch.randn((batch_size, seq_len, vocab_size)).to(inter.device), dim=-1)\n",
    "            gt.requires_grad = True\n",
    "            inter.requires_grad = True\n",
    "            optimizer = torch.optim.AdamW([gt], lr=0.09, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.01)\n",
    "            epochs = 1000\n",
    "            beta = 0.9\n",
    "            with tqdm_notebook(total=epochs, desc='Mocking') as pbar:\n",
    "                for i in range(epochs):\n",
    "                    optimizer.zero_grad()\n",
    "                    x = mocker(inter)\n",
    "                    loss = calc_shifted_loss_logits(x, torch.softmax(gt,dim=-1))\n",
    "                    grad = torch.autograd.grad(loss, inter, create_graph=True)\n",
    "                    grad_diff = 0\n",
    "                    for gx, gy in zip(grad, t2tr_grad.to(loss.device)):\n",
    "                        grad_diff += beta * ((gx - gy) ** 2).sum() + (1 - beta) * torch.abs((gx - gy)).sum()\n",
    "                    grad_diff.backward()\n",
    "                    optimizer.step()\n",
    "                    sent = tokenizer.decode(gt.argmax(-1)[0], skip_special_tokens=True)\n",
    "                    pbar.set_postfix(sent=sent, grad_diff=grad_diff.item(), loss=loss.item())\n",
    "                    pbar.update(1)\n",
    "            rouge = calculate_rouge(tokenizer, gt, batch['input_text'])\n",
    "            self.mocker_rouge_tr2t.append(rouge)\n",
    "            # guess = attacker2(tr2t_fx)\n",
    "            # rouge = calculate_rouge(tokenizer, guess, batch['input_text'])\n",
    "            # self.attacker_rouge_tr2t.append(rouge)\n",
    "\n",
    "\n",
    "simulator = SFLSimulator(client_ids=client_ids, strategy=QAFLStrategy(), llm=model, tokenizer=tokenizer,\n",
    "                         dataset=fed_dataset, config=config)\n",
    "# model.print_split_model()\n",
    "mocker = GPT2TopMocker(config, model)\n",
    "mocker.to(simulator.device)\n",
    "simulator.simulate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3. 开始联邦模拟"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"sfl-with-attacker\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"dataset\": 'piqa',\n",
    "        \"attacker\": \"piqa-validation\",\n",
    "        \"noise\": \"2.0\"\n",
    "    }\n",
    ")\n",
    "\n",
    "simulator.simulate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sfl.utils.model import sentence_score\n",
    "\n",
    "sentence_score(\"I'm fine, thank you!\", model, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}