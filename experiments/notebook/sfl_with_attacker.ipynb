{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1. 加载模型与Tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../../..'))
\n",
    "from sfl.utils.model import get_best_gpu\n",
    "from sfl.utils.exp import get_model_and_tokenizer\n",
    "device = get_best_gpu()\n",
    "model, tokenizer = get_model_and_tokenizer('gpt2-large')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'To mix food coloring with sugar, you can use the following:\\n\\n1 1/2 cups powdered sugar (or 1 cup granulated sugar plus 1 teaspoon of cornstarch mixed with 3/4 cup water)\\n\\n\\n2 tablespoons corn starch (also known as corn syrup) or other sweetener (such as xylitol or stevia, or a combination of the two, such as erythritol and sorbitol) (Note: If you are using a food processor, be sure to use a fine-mesh strainer to remove all the solids from the mixture before you add it to the processor.)\\n...\\nThe mixture will be very thick, so you may need to add a little more water to thin it out a bit. If it is too thick for your liking, just add more sugar and mix it again. You may also want to try mixing it with a small amount of water or milk to make it easier to work with. The mixture should be thick enough to coat the back of a spoon, but not so thick that it would be difficult to pour into a piping bag. It should also not be too runny that you would have to strain it through a sieve or cheesecloth to get the desired consistency.\\nIn a large bowl, mix together the powdered and sweeteners until well-combined. Add the milk and stir to combine. Pour the batter into the greased and floured 9-inch springform pan. Bake at 350°F for 30-35 minutes or until a toothpick inserted in the center comes out clean. Allow to cool completely before frosting or decorating.'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sfl.utils.model import generate\n",
    "\n",
    "model.to(device)\n",
    "# 测试模型输出\n",
    "text = \"To mix food coloring with sugar, you can\"\n",
    "generate(text, tokenizer, model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step2 配置SFL参数，切分模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from sfl.config import FLConfig\n",
    "config = FLConfig(collect_intermediates=True,\n",
    "                  global_round=4,\n",
    "                  client_evaluate_freq=25,\n",
    "                  client_epoch=1,  # 每轮联邦每个Client训2轮\n",
    "                  split_point_1=6,\n",
    "                  split_point_2=30,  # [0,1 | 2,3,.... 29| 30, 31]\n",
    "                  use_lora_at_trunk=True,  # 在trunk部分使用LoRA\n",
    "                  top_and_bottom_from_scratch='False',  # top和bottom都不采用预训练参数.\n",
    "                  noise_mode=\"none\",\n",
    "                  noise_scale=0.0,  # 噪声大小,\n",
    "                  batch_size=2,\n",
    "                  dataset_type='train'\n",
    "                  )\n",
    "\n",
    "model.config_sfl(config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3. 加载数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sfl.utils.exp import get_dataset_class\n",
    "\n",
    "client_ids = [str(i) for i in range(3)]\n",
    "dataset_cls = get_dataset_class('piqa')\n",
    "fed_dataset = dataset_cls(tokenizer=tokenizer, client_ids=client_ids, shrink_frac=0.04)\n",
    "test_dataset = dataset_cls(tokenizer=tokenizer, client_ids=[])\n",
    "test_loader = test_dataset.get_dataloader_unsliced(1, 'test', shrink_frac=1.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4. 加载攻击模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sfl.utils.exp import get_dra_attacker, get_dlg_attacker\n",
    "from sfl.config import DRAConfig\n",
    "\n",
    "# DRA模型\n",
    "dra_config = DRAConfig(target_model_name='gpt2-large', target_sps='6-30', b2tr_sp=6, tr2t_sp=6, target_dataset='piqa')\n",
    "attacker, attacker2 = get_dra_attacker(dra_config)\n",
    "\n",
    "# TAG模型\n",
    "tag_attacker = get_dlg_attacker(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5. 设置联邦切分训练流程"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 97])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "torch.Size([2, 22])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "torch.Size([2, 94])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n",
    "from typing import Any\n",
    "from sfl.utils.model import Intermediate\n",
    "from sfl.simulator.simulator import SFLSimulator\n",
    "from sfl.simulator.strategy import BaseSFLStrategy\n",
    "import argparse\n",
    "from sfl.utils.exp import add_sfl_params\n",
    "parser = argparse.ArgumentParser()\n",
    "add_sfl_params(parser)\n",
    "args = parser.parse_args({})\n",
    "args.attacker_samples = 10\n",
    "args.log_to_wandb = False\n",
    "\n",
    "\n",
    "# 定义Client本地学习策略\n",
    "class QAFLStrategy(BaseSFLStrategy):\n",
    "    def sample_attacker_triggered(self, global_round, client_id, local_epoch, local_step,\n",
    "                                  b2tr_inter: Intermediate, tr2t_inter: Intermediate,\n",
    "                                  all_inter: dict[Any, Intermediate],\n",
    "                                  batch, logs):\n",
    "        super().sample_attacker_triggered(global_round, client_id, local_epoch, local_step, b2tr_inter, tr2t_inter,\n",
    "                                          all_inter, batch, logs)\n",
    "        print(self.attack_all_performs)\n",
    "\n",
    "\n",
    "simulator = SFLSimulator(client_ids=client_ids,\n",
    "                         strategy=QAFLStrategy(args, model, tokenizer, test_loader, dra1=attacker, dra2=attacker2, dlg=tag_attacker),\n",
    "                         llm=model, tokenizer=tokenizer,\n",
    "                         dataset=fed_dataset, config=config, args=args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3. 开始联邦模拟"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================Global Round 0=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sfl/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Client 1 Epoch 0 Step (24, 24) Loss 2.014:  48%|████████████████████████████████████████████▏                                               | 24/50 [00:04<00:04,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'attacker_tr2t': [0.6285714235755102], 'attacker_b2tr': [0.999999995], 'tag_rouge_lf': [0.6007130074800142]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Client 1 Epoch 0 Step (24, 24) Loss 2.014:  48%|████████████████████████████████████████████▏                                               | 24/50 [00:56<01:01,  2.37s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 13\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwandb\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# wandb.init(\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#     project=\"sfl-eval\",\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m#     name=\"gpt2-large-wikitext-ppl\",\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m#     }\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# )\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m simulator\u001B[38;5;241m.\u001B[39msimulate()\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/simulator/simulator.py:147\u001B[0m, in \u001B[0;36mSFLSimulator.simulate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    145\u001B[0m iters\u001B[38;5;241m.\u001B[39msetdefault(client_id, [\u001B[38;5;28miter\u001B[39m(loaders[client_id])])\n\u001B[1;32m    146\u001B[0m itt \u001B[38;5;241m=\u001B[39m CircularDataLoaderIterator(iters[client_id], loaders[client_id], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mclient_steps)\n\u001B[0;32m--> 147\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client_step(client_id, i, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlocal_epochs[client_id], itt)\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlocal_steps[client_id] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m itt\u001B[38;5;241m.\u001B[39miterated_num\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mglobal_steps[client_id] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m itt\u001B[38;5;241m.\u001B[39miterated_num\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/simulator/simulator.py:229\u001B[0m, in \u001B[0;36mSFLSimulator._client_step\u001B[0;34m(self, client_id, global_round, local_epoch, iterator)\u001B[0m\n\u001B[1;32m    227\u001B[0m \u001B[38;5;66;03m# client step\u001B[39;00m\n\u001B[1;32m    228\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n\u001B[0;32m--> 229\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mclient_step(client_id, global_round, local_epoch, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm, iterator, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig)\n\u001B[1;32m    230\u001B[0m \u001B[38;5;66;03m# store updated client parameters\u001B[39;00m\n\u001B[1;32m    231\u001B[0m cm \u001B[38;5;241m=\u001B[39m ([p\u001B[38;5;241m.\u001B[39mcpu() \u001B[38;5;28;01mfor\u001B[39;00m nm, p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mget_top_params()],\n\u001B[1;32m    232\u001B[0m       [p\u001B[38;5;241m.\u001B[39mcpu() \u001B[38;5;28;01mfor\u001B[39;00m nm, p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mget_bottom_params()])\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/simulator/strategy.py:114\u001B[0m, in \u001B[0;36mBaseSFLStrategy.client_step\u001B[0;34m(self, client_id, global_round, client_epoch, llm, iterator, config)\u001B[0m\n\u001B[1;32m    105\u001B[0m avg_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m    106\u001B[0m \u001B[38;5;66;03m# avg_self_rouge += \\\u001B[39;00m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;66;03m#     calculate_rouge(self.tokenizer, outputs.logits.argmax(dim=-1), batch['input_text'])['rouge-l'][\u001B[39;00m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;66;03m#         'f']\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;66;03m#     avg_self_rouge_pt += \\\u001B[39;00m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;66;03m#         calculate_rouge(self.tokenizer, outputs_pt.logits, batch['input_text'])['rouge-l']['f']\u001B[39;00m\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_done(client_id, step, batch,\n\u001B[1;32m    115\u001B[0m                {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mfloat\u001B[39m(avg_loss \u001B[38;5;241m/\u001B[39m batch_num)})\n\u001B[1;32m    116\u001B[0m \u001B[38;5;66;03m# , \"self\": float(avg_self_rouge / batch_num),\u001B[39;00m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;66;03m#  \"self_pt\": float(avg_self_rouge_pt / batch_num)})  # Collect gradients\u001B[39;00m\n\u001B[1;32m    118\u001B[0m pbar\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/simulator/strategy.py:55\u001B[0m, in \u001B[0;36mFLStrategy.step_done\u001B[0;34m(self, client_id, mini_step, batch, logs)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep_done\u001B[39m(\u001B[38;5;28mself\u001B[39m, client_id, mini_step, batch, logs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m---> 55\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msimulator\u001B[38;5;241m.\u001B[39m_client_one_step_done(client_id, mini_step, batch, logs)\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/simulator/simulator.py:279\u001B[0m, in \u001B[0;36mSFLSimulator._client_one_step_done\u001B[0;34m(self, client_id, mini_step, batch, logs)\u001B[0m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mcallback_intermediate_result(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_global_round,\n\u001B[1;32m    273\u001B[0m                                            client_id,\n\u001B[1;32m    274\u001B[0m                                            local_epoch,\n\u001B[1;32m    275\u001B[0m                                            local_step,\n\u001B[1;32m    276\u001B[0m                                            b2tr_inter, tr2t_inter, all_inters,\n\u001B[1;32m    277\u001B[0m                                            batch, logs)\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (local_step \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mclient_evaluate_freq \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 279\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mclient_evaluate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_global_round, client_id, logs)\n\u001B[1;32m    280\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m logs\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    281\u001B[0m     report[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclient\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclient_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m v\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/simulator/strategy.py:77\u001B[0m, in \u001B[0;36mBaseSFLStrategy.client_evaluate\u001B[0;34m(self, global_round, client_id, log)\u001B[0m\n\u001B[1;32m     75\u001B[0m     log[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest-acc\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m evaluate_accuracy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msimulator\u001B[38;5;241m.\u001B[39mllm, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_loader)\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtask_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlm\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m---> 77\u001B[0m     ppl \u001B[38;5;241m=\u001B[39m evaluate_perplexity(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msimulator\u001B[38;5;241m.\u001B[39mllm, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_loader)\n\u001B[1;32m     78\u001B[0m     log[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest-ppl\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m ppl\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/utils/model.py:280\u001B[0m, in \u001B[0;36mevaluate_perplexity\u001B[0;34m(model, loader, stride)\u001B[0m\n\u001B[1;32m    278\u001B[0m target_ids[:, :\u001B[38;5;241m-\u001B[39mtrg_len] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m100\u001B[39m\n\u001B[1;32m    279\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 280\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m model(input_ids, labels\u001B[38;5;241m=\u001B[39mtarget_ids)\n\u001B[1;32m    281\u001B[0m     neg_log_likelihood \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mloss\n\u001B[1;32m    282\u001B[0m nlls\u001B[38;5;241m.\u001B[39mappend(neg_log_likelihood)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/model/llm/gpt2/gpt2_wrapper.py:79\u001B[0m, in \u001B[0;36mGPT2SplitLMHeadModel.forward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m     61\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     62\u001B[0m         input_ids: Optional[torch\u001B[38;5;241m.\u001B[39mLongTensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     75\u001B[0m         return_dict: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     76\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[Tuple, CausalLMOutputWithCrossAttentions]:\n\u001B[1;32m     77\u001B[0m     return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m---> 79\u001B[0m     transformer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer(\n\u001B[1;32m     80\u001B[0m         input_ids,\n\u001B[1;32m     81\u001B[0m         past_key_values\u001B[38;5;241m=\u001B[39mpast_key_values,\n\u001B[1;32m     82\u001B[0m         attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[1;32m     83\u001B[0m         token_type_ids\u001B[38;5;241m=\u001B[39mtoken_type_ids,\n\u001B[1;32m     84\u001B[0m         position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m     85\u001B[0m         head_mask\u001B[38;5;241m=\u001B[39mhead_mask,\n\u001B[1;32m     86\u001B[0m         inputs_embeds\u001B[38;5;241m=\u001B[39minputs_embeds,\n\u001B[1;32m     87\u001B[0m         encoder_hidden_states\u001B[38;5;241m=\u001B[39mencoder_hidden_states,\n\u001B[1;32m     88\u001B[0m         encoder_attention_mask\u001B[38;5;241m=\u001B[39mencoder_attention_mask,\n\u001B[1;32m     89\u001B[0m         use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n\u001B[1;32m     90\u001B[0m         output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m     91\u001B[0m         output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[1;32m     92\u001B[0m         return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[1;32m     93\u001B[0m     )\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfl_config \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfl_config\u001B[38;5;241m.\u001B[39mattack_mode:\n\u001B[1;32m     95\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m transformer_outputs\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/model/llm/gpt2/gpt2_split.py:178\u001B[0m, in \u001B[0;36mGPT2SplitModel.forward\u001B[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    168\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    169\u001B[0m         create_custom_forward(block),\n\u001B[1;32m    170\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    175\u001B[0m         encoder_attention_mask,\n\u001B[1;32m    176\u001B[0m     )\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 178\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m block(\n\u001B[1;32m    179\u001B[0m         hidden_states,\n\u001B[1;32m    180\u001B[0m         layer_past\u001B[38;5;241m=\u001B[39mlayer_past,\n\u001B[1;32m    181\u001B[0m         attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[1;32m    182\u001B[0m         head_mask\u001B[38;5;241m=\u001B[39mhead_mask[i],\n\u001B[1;32m    183\u001B[0m         encoder_hidden_states\u001B[38;5;241m=\u001B[39mencoder_hidden_states,\n\u001B[1;32m    184\u001B[0m         encoder_attention_mask\u001B[38;5;241m=\u001B[39mencoder_attention_mask,\n\u001B[1;32m    185\u001B[0m         use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n\u001B[1;32m    186\u001B[0m         output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m    187\u001B[0m     )\n\u001B[1;32m    189\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    190\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:391\u001B[0m, in \u001B[0;36mGPT2Block.forward\u001B[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001B[0m\n\u001B[1;32m    389\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[1;32m    390\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln_1(hidden_states)\n\u001B[0;32m--> 391\u001B[0m attn_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn(\n\u001B[1;32m    392\u001B[0m     hidden_states,\n\u001B[1;32m    393\u001B[0m     layer_past\u001B[38;5;241m=\u001B[39mlayer_past,\n\u001B[1;32m    394\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[1;32m    395\u001B[0m     head_mask\u001B[38;5;241m=\u001B[39mhead_mask,\n\u001B[1;32m    396\u001B[0m     use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n\u001B[1;32m    397\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[1;32m    398\u001B[0m )\n\u001B[1;32m    399\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_outputs[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# output_attn: a, present, (attentions)\u001B[39;00m\n\u001B[1;32m    400\u001B[0m outputs \u001B[38;5;241m=\u001B[39m attn_outputs[\u001B[38;5;241m1\u001B[39m:]\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:313\u001B[0m, in \u001B[0;36mGPT2Attention.forward\u001B[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001B[0m\n\u001B[1;32m    311\u001B[0m     attention_mask \u001B[38;5;241m=\u001B[39m encoder_attention_mask\n\u001B[1;32m    312\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 313\u001B[0m     query, key, value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mc_attn(hidden_states)\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit_size, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    315\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_split_heads(query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_dim)\n\u001B[1;32m    316\u001B[0m key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_split_heads(key, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead_dim)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/peft/tuners/lora.py:823\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    817\u001B[0m     result \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mlinear(x, transpose(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfan_in_fan_out), bias\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n\u001B[1;32m    819\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlora_A[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactive_adapter]\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[1;32m    821\u001B[0m     result \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    822\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlora_B[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactive_adapter](\n\u001B[0;32m--> 823\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlora_A[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactive_adapter](\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlora_dropout[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactive_adapter](x))\n\u001B[1;32m    824\u001B[0m         )\n\u001B[1;32m    825\u001B[0m         \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscaling[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactive_adapter]\n\u001B[1;32m    826\u001B[0m     )\n\u001B[1;32m    827\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    828\u001B[0m     result \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mlinear(x, transpose(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfan_in_fan_out), bias\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mlinear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "if args.log_to_wandb:\n",
    "    wandb.init(\n",
    "    project=\"sfl-eval\",\n",
    "    name=\"gpt2-large-wikitext-ppl\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"dataset\": 'code',\n",
    "        \"noise\": \"0.0\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "simulator.simulate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}