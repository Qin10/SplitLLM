{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1. 加载模型与Tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "from transformers import AutoTokenizer\n",
    "from sfl.model.gpt2.gpt2_split import GPT2SplitLMHeadModel\n",
    "\n",
    "cache_dir = '/root/autodl-tmp/sfl/models'  # 模型的缓存位置，需要修改\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\", cache_dir=cache_dir)\n",
    "model = GPT2SplitLMHeadModel.from_pretrained(\"gpt2-large\", cache_dir=cache_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = 50256"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 测试模型的生成文本\n",
    "def generate(text, md=model):\n",
    "    model.train(False)\n",
    "    t = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    res = md.generate(t['input_ids'].to(md.device), attention_mask=t['attention_mask'].to(md.device),\n",
    "                      max_length=100, num_beams=6, no_repeat_ngram_size=2, early_stopping=True,\n",
    "                      num_return_sequences=1, pad_token_id=tokenizer.pad_token_id)\n",
    "    return tokenizer.decode(res[0], skip_special_tokens=True)\n",
    "\n",
    "# 测试模型输出\n",
    "def get_output(text, md=model):\n",
    "    t = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    res = model(t['input_ids'].to(md.device), attention_mask=t['attention_mask'].to(md.device))\n",
    "    r = tokenizer.decode(res.logits.argmax(dim=-1)[-1], skip_special_tokens=True)\n",
    "    return r\n",
    "\n",
    "\n",
    "# print(generate(\"To mix food coloring with sugar, you can\", model))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2. 加载攻击模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GPT2AttackModel:\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([512, 1280]) from checkpoint, the shape in current model is torch.Size([1024, 1280]).\n\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for mlp.weight: copying a param with shape torch.Size([50257, 128]) from checkpoint, the shape in current model is torch.Size([50257, 256]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# 攻击bottom-trunk数据\u001B[39;00m\n\u001B[1;32m      5\u001B[0m attacker \u001B[38;5;241m=\u001B[39m GPT2AttackModel(model\u001B[38;5;241m.\u001B[39mconfig)\n\u001B[0;32m----> 6\u001B[0m attacker\u001B[38;5;241m.\u001B[39mload_state_dict(\n\u001B[1;32m      7\u001B[0m     torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/root/autodl-tmp/sfl/models/checkpoints/attacker/gpt2-large/piqa-validation/b2tr-2/epoch_7_rouge_0.8874827658861361.pt\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# 攻击trunk-top 数据\u001B[39;00m\n\u001B[1;32m     10\u001B[0m attacker2 \u001B[38;5;241m=\u001B[39m GPT2AttackModel(model\u001B[38;5;241m.\u001B[39mconfig)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/module.py:2041\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict)\u001B[0m\n\u001B[1;32m   2036\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   2037\u001B[0m             \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2038\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(k) \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[1;32m   2040\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2041\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2042\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)))\n\u001B[1;32m   2043\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for GPT2AttackModel:\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([512, 1280]) from checkpoint, the shape in current model is torch.Size([1024, 1280]).\n\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for mlp.weight: copying a param with shape torch.Size([50257, 128]) from checkpoint, the shape in current model is torch.Size([50257, 256])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sfl.model.attack_model import GPT2AttackModel\n",
    "\n",
    "# 攻击bottom-trunk数据\n",
    "attacker = GPT2AttackModel(model.config)\n",
    "attacker.load_state_dict(\n",
    "    torch.load('/root/autodl-tmp/sfl/models/checkpoints/attacker/gpt2-large/piqa-validation/b2tr-2/epoch_7_rouge_0.8874827658861361.pt'))\n",
    "\n",
    "# 攻击trunk-top 数据\n",
    "attacker2 = GPT2AttackModel(model.config)\n",
    "attacker2.load_state_dict(\n",
    "    torch.load('/root/autodl-tmp/sfl/models/checkpoints/attacker/gpt2-large/piqa-validation/tr2t-30/epoch_14_rouge_0.7890438096900106.pt'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3. 设置联邦训练流程"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sfl.simulator.simulator import SFLSimulator\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sfl.model.split_model import SplitModel\n",
    "from sfl.simulator.strategy import FLStrategy\n",
    "from sfl.simulator.dataset import PIQAFedDataset, FedDataset, DialogSumFedDataset\n",
    "from sfl.utils import FLConfig, calculate_rouge\n",
    "from torch.optim import AdamW\n",
    "import wandb\n",
    "\n",
    "\n",
    "# 定义Client本地学习策略\n",
    "class QAFLStrategy(FLStrategy):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attacker_rouge_b2tr = []\n",
    "        self.attacker_rouge_tr2t = []\n",
    "        self.client_logs = {}\n",
    "\n",
    "    def client_step(self, global_round, client_id: str, llm: SplitModel, dataloader: DataLoader, cfg: FLConfig):\n",
    "        optimizer = AdamW(llm.parameters(), lr=1e-5)\n",
    "        with tqdm_notebook(total=cfg.client_epoch * len(dataloader)) as pbar:\n",
    "            for epoch in range(cfg.client_epoch):\n",
    "                for step, batch in enumerate(dataloader):\n",
    "                    optimizer.zero_grad()\n",
    "                    input_ids = batch['input_ids'].to(llm.device)\n",
    "                    attention_mask = batch['input_att_mask'].to(llm.device)\n",
    "                    outputs = llm(input_ids=input_ids, labels=input_ids, attention_mask=attention_mask)\n",
    "                    self.fp_done(client_id, epoch, step, batch)  # Collect intermediate results\n",
    "                    loss = outputs.loss\n",
    "                    pbar.set_description(f'Client {client_id} Epoch {epoch} Loss {loss.item():.3f}')\n",
    "                    loss.backward()\n",
    "                    self.bp_done(client_id, epoch, step, batch)  # Collect gradients\n",
    "                    optimizer.step()\n",
    "                    pbar.update(1)\n",
    "                avg_rouge1 = sum([r[\"rouge-l\"][\"f\"] for r in self.attacker_rouge_b2tr]) / len(self.attacker_rouge_b2tr)\n",
    "                print(f'ATTACK! Bottom-trunk, Client {client_id} Epoch {epoch} RougeL {avg_rouge1:.3f}')\n",
    "                avg_rouge2 = sum([r['rouge-l']['f'] for r in self.attacker_rouge_tr2t]) / len(self.attacker_rouge_tr2t)\n",
    "                print(f'ATTACK! Trunk-Top, Client {client_id} Epoch {epoch} RougeL {avg_rouge2:.3f}')\n",
    "                self.client_logs.setdefault(client_id, {})\n",
    "                self.client_logs[client_id][epoch] = {\"bottom-trunk\": avg_rouge1, \"trunk-top\": avg_rouge2}\n",
    "                self.attacker_rouge_b2tr.clear()\n",
    "                self.attacker_rouge_tr2t.clear()\n",
    "\n",
    "    def aggregation_step(self, global_round, params):\n",
    "        report = {}\n",
    "        report['global_round'] = global_round\n",
    "        for cid, epochs in self.client_logs.items():\n",
    "            for epc, rep in epochs.items():\n",
    "                for k, v in rep.items():\n",
    "                    report[f'client{cid}-epoch{epc}-{k}'] = v\n",
    "        wandb.log(report)\n",
    "        print(report)\n",
    "        self.client_logs = {}\n",
    "        return super(QAFLStrategy, self).aggregation_step(global_round, params)\n",
    "\n",
    "    def callback_fp_param(self, client_id, local_epoch, local_step, b2tr_params, tr2t_params, batch):\n",
    "        #  这里获取某epoch、step中，前传过程的两次传输参数，b2tr(bottom-trunk), tr2t(trunk-top)\n",
    "        with torch.no_grad():\n",
    "            rouge_res_b2tr = calculate_rouge(tokenizer, attacker(b2tr_params), batch['input_text'])\n",
    "            rouge_res_tr2t = calculate_rouge(tokenizer, attacker2(tr2t_params), batch['input_text'])\n",
    "            self.attacker_rouge_b2tr.append(rouge_res_b2tr)\n",
    "            self.attacker_rouge_tr2t.append(rouge_res_tr2t)\n",
    "\n",
    "    def callback_bp_param(self, client_id, local_epoch, local_step, t2tr_params, tr2b_params, batch):\n",
    "        #  这里获取某epoch、step中，反传过程的两次传输参数\n",
    "        pass\n",
    "\n",
    "\n",
    "client_ids = [str(i) for i in range(3)]\n",
    "config = FLConfig(global_round=50,\n",
    "                  client_epoch=2,  # 每轮联邦每个Client训2轮\n",
    "                  split_point_1=2,\n",
    "                  split_point_2=30,  # [0,1 | 2,3,.... 29| 30, 31]\n",
    "                  use_lora_at_trunk=True,  # 在trunk部分使用LoRA\n",
    "                  top_and_bottom_from_scratch=False,  # top和bottom都不采用预训练参数.\n",
    "                  noise_scale=2.0,  # 噪声大小\n",
    "                  )\n",
    "fed_dataset = PIQAFedDataset(tokenizer=tokenizer, client_ids=client_ids, shrink_frac=0.15)\n",
    "simulator = SFLSimulator(client_ids=client_ids, strategy=QAFLStrategy(), llm=model, tokenizer=tokenizer,\n",
    "                         dataset=fed_dataset, config=config)\n",
    "model.print_split_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3. 开始联邦模拟"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"sfl\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"dataset\": 'piqa',\n",
    "        \"attacker\": \"piqa-validation\",\n",
    "        \"noise\": \"2.0\"\n",
    "    }\n",
    ")\n",
    "\n",
    "simulator.simulate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}