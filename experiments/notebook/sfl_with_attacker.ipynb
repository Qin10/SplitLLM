{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1. 加载模型与Tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "GPT2SplitLMHeadModel(\n  (transformer): GPT2SplitModel(\n    (wte): Embedding(50257, 1280)\n    (wpe): Embedding(1024, 1280)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-35): 36 x GPT2Block(\n        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "from sfl.utils.training import get_best_gpu\n",
    "from transformers import AutoTokenizer\n",
    "from sfl.model.gpt2.gpt2_split import GPT2SplitLMHeadModel\n",
    "from sfl import config\n",
    "\n",
    "device = get_best_gpu()\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(config.model_download_dir, \"gpt2/\"))\n",
    "model = GPT2SplitLMHeadModel.from_pretrained(os.path.join(config.model_download_dir, \"gpt2-large/\"))\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = 50256\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To mix food coloring with sugar, you can use the following:\n",
      "\n",
      "1 1/2 cups powdered sugar (or 1 cup granulated sugar plus 1 teaspoon of cornstarch mixed with 3/4 cup water)\n",
      "\n",
      "\n",
      "2 tablespoons corn starch (also known as corn syrup) or other sweetener (such as xylitol or stevia, or a combination of the two, such as erythritol and sorbitol) (see below for more information on sweeteners\n"
     ]
    }
   ],
   "source": [
    "from sfl.utils.model import generate\n",
    "\n",
    "\n",
    "# 测试模型输出\n",
    "def get_output(text, md=model):\n",
    "    t = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    res = model(t['input_ids'].to(md.device), attention_mask=t['attention_mask'].to(md.device))\n",
    "    r = tokenizer.decode(res.logits.argmax(dim=-1)[-1], skip_special_tokens=True)\n",
    "    return r\n",
    "\n",
    "\n",
    "print(generate(\"To mix food coloring with sugar, you can\", tokenizer, model))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2. 加载攻击模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from sfl.config import attacker_path\n",
    "from sfl.utils.training import get_attacker_class, extract_attacker_path\n",
    "from sfl.model.attack_model import GRUAttackModel\n",
    "\n",
    "attack_model = 'gru'\n",
    "# 加载攻击模型\n",
    "attacker_cls = get_attacker_class(attack_model)\n",
    "attacker_path_1, attacker_path_2 = extract_attacker_path(\n",
    "    {'split_point_1': 2, 'split_point_2': 10, 'attacker_path': attacker_path})\n",
    "attacker = attacker_cls.from_pretrained(attacker_path_1)\n",
    "attacker2 = attacker_cls.from_pretrained(attacker_path_2)\n",
    "#\n",
    "# attacker = GRUAttackModel.from_pretrained(\n",
    "#     '/root/autodl-tmp/sfl/models/attacker/gpt2/piqa/train*1.000-test*1.000/gru/b2tr-2/epoch_19_rouge_0.9849')\n",
    "# attacker2 = GRUAttackModel.from_pretrained(\n",
    "#     '/root/autodl-tmp/sfl/models/attacker/gpt2/piqa/train*1.000-test*1.000/gru/tr2t-10/epoch_19_rouge_0.9261')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3. 设置联邦训练流程"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sfl/lib/python3.11/site-packages/peft/tuners/lora.py:299: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================Split-/root/autodl-tmp/sfl/models/gpt2-large/=================\n",
      "==================Top Layers==================\n",
      "\n",
      "transformer.h.30:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.31:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.32:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.33:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.34:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.35:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.ln_f.weight:[: (1280,)]\n",
      "\n",
      "transformer.ln_f.bias:[: (1280,)]\n",
      "==================Trunk Layers==================\n",
      "\n",
      "transformer.h.2:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.3:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.4:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.5:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.6:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.7:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.8:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.9:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.10:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.11:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.12:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.13:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.14:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.15:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.16:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.17:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.18:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.19:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.20:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.21:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.22:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.23:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.24:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.25:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.26:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.27:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.28:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.29:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "==================Bottom Layers==================\n",
      "\n",
      "transformer.wte.weight:[: (50257, 1280)]\n",
      "\n",
      "transformer.wpe.weight:[: (1024, 1280)]\n",
      "\n",
      "transformer.h.0:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.1:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "=============================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "GRUAttackModel(\n  (gru): GRU(1280, 256, batch_first=True)\n  (mlp): Linear(in_features=256, out_features=50257, bias=True)\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from sfl.utils.model import calculate_rouge, calculate_rouge_text\n",
    "from sfl.config import FLConfig\n",
    "from sfl.simulator.simulator import SFLSimulator\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sfl.model.split_model import SplitModel\n",
    "from sfl.simulator.strategy import FLStrategy\n",
    "from sfl.simulator.dataset import PIQAFedDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "# 定义Client本地学习策略\n",
    "class QAFLStrategy(FLStrategy):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attacker_rouge_b2tr = []\n",
    "        self.attacker_rouge_tr2t = []\n",
    "        self.client_logs = {}\n",
    "\n",
    "    def client_step(self, global_round, client_id: str, llm: SplitModel, dataloader: DataLoader, cfg: FLConfig):\n",
    "        optimizer = AdamW(llm.parameters(), lr=1e-5)\n",
    "        with tqdm_notebook(total=cfg.client_epoch * len(dataloader)) as pbar:\n",
    "            for epoch in range(cfg.client_epoch):\n",
    "                for step, batch in enumerate(dataloader):\n",
    "                    optimizer.zero_grad()\n",
    "                    input_ids = batch['input_ids'].to(llm.device)\n",
    "                    attention_mask = batch['input_att_mask'].to(llm.device)\n",
    "                    outputs = llm(input_ids=input_ids, labels=input_ids, attention_mask=attention_mask)\n",
    "                    self.fp_done(client_id, epoch, step, batch)  # Collect intermediate results\n",
    "                    loss = outputs.loss\n",
    "                    pbar.set_description(f'Client {client_id} Epoch {epoch} Loss {loss.item():.3f}')\n",
    "                    loss.backward()\n",
    "                    self.bp_done(client_id, epoch, step, batch)  # Collect gradients\n",
    "                    optimizer.step()\n",
    "                    pbar.update(1)\n",
    "                avg_rouge1 = sum([r[\"rouge-l\"][\"f\"] for r in self.attacker_rouge_b2tr]) / len(self.attacker_rouge_b2tr)\n",
    "                print(f'ATTACK! Bottom-trunk, Client {client_id} Epoch {epoch} RougeL {avg_rouge1:.3f}')\n",
    "                avg_rouge2 = sum([r['rouge-l']['f'] for r in self.attacker_rouge_tr2t]) / len(self.attacker_rouge_tr2t)\n",
    "                print(f'ATTACK! Trunk-Top, Client {client_id} Epoch {epoch} RougeL {avg_rouge2:.3f}')\n",
    "                self.client_logs.setdefault(client_id, {})\n",
    "                self.client_logs[client_id][epoch] = {\"bottom-trunk\": avg_rouge1, \"trunk-top\": avg_rouge2}\n",
    "                self.attacker_rouge_b2tr.clear()\n",
    "                self.attacker_rouge_tr2t.clear()\n",
    "\n",
    "    def aggregation_step(self, global_round, params):\n",
    "        report = {}\n",
    "        report['global_round'] = global_round\n",
    "        for cid, epochs in self.client_logs.items():\n",
    "            for epc, rep in epochs.items():\n",
    "                for k, v in rep.items():\n",
    "                    report[f'client{cid}-epoch{epc}-{k}'] = v\n",
    "        wandb.log(report)\n",
    "        print(report)\n",
    "        self.client_logs = {}\n",
    "        return super(QAFLStrategy, self).aggregation_step(global_round, params)\n",
    "\n",
    "    def callback_fp_param(self, client_id, local_epoch, local_step, b2tr_params, tr2t_params, batch):\n",
    "        #  这里获取某epoch、step中，前传过程的两次传输参数，b2tr(bottom-trunk), tr2t(trunk-top)\n",
    "        with torch.no_grad():\n",
    "            rouge_res_b2tr = calculate_rouge(tokenizer, attacker.search(b2tr_params, model), batch['input_text'],\n",
    "                                             is_tokens=True)\n",
    "            rouge_res_tr2t = calculate_rouge(tokenizer, attacker2.search(tr2t_params, model), batch['input_text'],\n",
    "                                             is_tokens=True)\n",
    "            self.attacker_rouge_b2tr.append(rouge_res_b2tr)\n",
    "            self.attacker_rouge_tr2t.append(rouge_res_tr2t)\n",
    "            print(\n",
    "                f'ATTACK! Bottom-trunk, Client {client_id} Epoch {local_epoch} Step {local_step} RougeL {rouge_res_b2tr[\"rouge-l\"][\"f\"]:.3f}')\n",
    "\n",
    "    def callback_bp_param(self, client_id, local_epoch, local_step, t2tr_params, tr2b_params, batch):\n",
    "        #  这里获取某epoch、step中，反传过程的两次传输参数\n",
    "        pass\n",
    "\n",
    "\n",
    "client_ids = [str(i) for i in range(3)]\n",
    "config = FLConfig(global_round=50,\n",
    "                  client_epoch=2,  # 每轮联邦每个Client训2轮\n",
    "                  split_point_1=2,\n",
    "                  split_point_2=10,  # [0,1 | 2,3,.... 29| 30, 31]\n",
    "                  use_lora_at_trunk=True,  # 在trunk部分使用LoRA\n",
    "                  top_and_bottom_from_scratch=False,  # top和bottom都不采用预训练参数.\n",
    "                  noise_mode=\"dxp\",\n",
    "                  noise_scale=5.0,  # 噪声大小\n",
    "                  )\n",
    "fed_dataset = PIQAFedDataset(tokenizer=tokenizer, client_ids=client_ids, shrink_frac=0.15)\n",
    "simulator = SFLSimulator(client_ids=client_ids, strategy=QAFLStrategy(), llm=model, tokenizer=tokenizer,\n",
    "                         dataset=fed_dataset, config=config)\n",
    "model.print_split_model()\n",
    "attacker.to(model.device)\n",
    "attacker2.to(model.device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3. 开始联邦模拟"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mstupidtree\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/project/SFL-LLM/experiments/notebook/wandb/run-20240111_160218-k0xsrxyq</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/stupidtree/sfl-with-attacker/runs/k0xsrxyq' target=\"_blank\">dainty-galaxy-128</a></strong> to <a href='https://wandb.ai/stupidtree/sfl-with-attacker' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/stupidtree/sfl-with-attacker' target=\"_blank\">https://wandb.ai/stupidtree/sfl-with-attacker</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/stupidtree/sfl-with-attacker/runs/k0xsrxyq' target=\"_blank\">https://wandb.ai/stupidtree/sfl-with-attacker/runs/k0xsrxyq</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================Global Round 0=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/94 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c9e0afd2e1b498f8b1789289085da9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 0 RougeL 0.955\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 1 RougeL 1.000\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 2 RougeL 1.000\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 3 RougeL 1.000\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 4 RougeL 1.000\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 5 RougeL 0.967\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 6 RougeL 0.986\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 7 RougeL 1.000\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 8 RougeL 1.000\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 9 RougeL 1.000\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 10 RougeL 1.000\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 11 RougeL 1.000\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 12 RougeL 0.974\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 13 RougeL 0.942\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 14 RougeL 0.939\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 15 RougeL 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"sfl-with-attacker\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"dataset\": 'piqa',\n",
    "        \"attacker\": \"piqa-validation\",\n",
    "        \"noise\": \"2.0\"\n",
    "    }\n",
    ")\n",
    "\n",
    "simulator.simulate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sfl.utils.model import sentence_score\n",
    "\n",
    "sentence_score(\"I'm fine, thank you!\", model, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}