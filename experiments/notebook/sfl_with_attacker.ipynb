{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1. 加载模型与Tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "from transformers import AutoTokenizer\n",
    "from sfl.model.gpt2.gpt2_split import GPT2SplitLMHeadModel\n",
    "\n",
    "cache_dir = '/root/autodl-tmp/sfl/models'  # 模型的缓存位置，需要修改\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", cache_dir=cache_dir)\n",
    "model = GPT2SplitLMHeadModel.from_pretrained(\"gpt2\", cache_dir=cache_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = 50256"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To mix food coloring with sugar, you can also use it as a sweetener.\n",
      "\n",
      "If you want to add more sugar to the mix, add a little more water and mix well. If you add too much water, the mixture will be too thick, and you will end up with a mess. You can use a spoon to scoop out the excess water from the mixing bowl, but it's best to leave it at room temperature for at least 30 minutes before adding the rest of the water.\n"
     ]
    }
   ],
   "source": [
    "# 测试模型的生成文本\n",
    "def generate(text, md=model):\n",
    "    model.train(False)\n",
    "    t = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    res = md.generate(t['input_ids'].to(md.device), attention_mask=t['attention_mask'].to(md.device),\n",
    "                      max_length=300, num_beams=6, no_repeat_ngram_size=2, early_stopping=True,\n",
    "                      num_return_sequences=1, pad_token_id=tokenizer.pad_token_id)\n",
    "    return tokenizer.decode(res[0], skip_special_tokens=True)\n",
    "\n",
    "# 测试模型输出\n",
    "def get_output(text, md=model):\n",
    "    t = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    res = model(t['input_ids'].to(md.device), attention_mask=t['attention_mask'].to(md.device))\n",
    "    r = tokenizer.decode(res.logits.argmax(dim=-1)[-1], skip_special_tokens=True)\n",
    "    return r\n",
    "\n",
    "\n",
    "print(generate(\"To mix food coloring with sugar, you can\", model))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2. 加载攻击模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from sfl.model.attack_model import GPT2AttackModel\n",
    "\n",
    "# 攻击bottom-trunk数据\n",
    "attacker = GPT2AttackModel(model.config)\n",
    "attacker.load_state_dict(\n",
    "    torch.load('/root/autodl-tmp/sfl/models/checkpoints/attacker/epoch_4_rouge_0.9752261900524914.pt'))\n",
    "\n",
    "# 攻击trunk-top 数据\n",
    "attacker2 = GPT2AttackModel(model.config)\n",
    "attacker2.load_state_dict(\n",
    "    torch.load('/root/autodl-tmp/sfl/models/checkpoints/attacker/b2tr-9/epoch_4_rouge_0.9400152550325381.pt'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3. 设置联邦训练流程"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sfl/lib/python3.11/site-packages/peft/tuners/lora.py:299: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================Split-gpt2=================\n",
      "==================Top Layers==================\n",
      "\n",
      "transformer.h.9:[ln_1.weight: (768,), ln_1.bias: (768,), attn.c_attn.weight: (768, 2304), attn.c_attn.bias: (2304,), attn.c_proj.weight: (768, 768), attn.c_proj.bias: (768,), ln_2.weight: (768,), ln_2.bias: (768,), mlp.c_fc.weight: (768, 3072), mlp.c_fc.bias: (3072,), mlp.c_proj.weight: (3072, 768), mlp.c_proj.bias: (768,)]\n",
      "\n",
      "transformer.h.10:[ln_1.weight: (768,), ln_1.bias: (768,), attn.c_attn.weight: (768, 2304), attn.c_attn.bias: (2304,), attn.c_proj.weight: (768, 768), attn.c_proj.bias: (768,), ln_2.weight: (768,), ln_2.bias: (768,), mlp.c_fc.weight: (768, 3072), mlp.c_fc.bias: (3072,), mlp.c_proj.weight: (3072, 768), mlp.c_proj.bias: (768,)]\n",
      "\n",
      "transformer.h.11:[ln_1.weight: (768,), ln_1.bias: (768,), attn.c_attn.weight: (768, 2304), attn.c_attn.bias: (2304,), attn.c_proj.weight: (768, 768), attn.c_proj.bias: (768,), ln_2.weight: (768,), ln_2.bias: (768,), mlp.c_fc.weight: (768, 3072), mlp.c_fc.bias: (3072,), mlp.c_proj.weight: (3072, 768), mlp.c_proj.bias: (768,)]\n",
      "\n",
      "transformer.ln_f.weight:[: (768,)]\n",
      "\n",
      "transformer.ln_f.bias:[: (768,)]\n",
      "==================Trunk Layers==================\n",
      "\n",
      "transformer.h.2:[attn.c_attn.lora_A.default.weight: (8, 768), attn.c_attn.lora_B.default.weight: (2304, 8), attn.c_proj.lora_A.default.weight: (8, 768), attn.c_proj.lora_B.default.weight: (768, 8), mlp.c_fc.lora_A.default.weight: (8, 768), mlp.c_fc.lora_B.default.weight: (3072, 8), mlp.c_proj.lora_A.default.weight: (8, 3072), mlp.c_proj.lora_B.default.weight: (768, 8)]\n",
      "\n",
      "transformer.h.3:[attn.c_attn.lora_A.default.weight: (8, 768), attn.c_attn.lora_B.default.weight: (2304, 8), attn.c_proj.lora_A.default.weight: (8, 768), attn.c_proj.lora_B.default.weight: (768, 8), mlp.c_fc.lora_A.default.weight: (8, 768), mlp.c_fc.lora_B.default.weight: (3072, 8), mlp.c_proj.lora_A.default.weight: (8, 3072), mlp.c_proj.lora_B.default.weight: (768, 8)]\n",
      "\n",
      "transformer.h.4:[attn.c_attn.lora_A.default.weight: (8, 768), attn.c_attn.lora_B.default.weight: (2304, 8), attn.c_proj.lora_A.default.weight: (8, 768), attn.c_proj.lora_B.default.weight: (768, 8), mlp.c_fc.lora_A.default.weight: (8, 768), mlp.c_fc.lora_B.default.weight: (3072, 8), mlp.c_proj.lora_A.default.weight: (8, 3072), mlp.c_proj.lora_B.default.weight: (768, 8)]\n",
      "\n",
      "transformer.h.5:[attn.c_attn.lora_A.default.weight: (8, 768), attn.c_attn.lora_B.default.weight: (2304, 8), attn.c_proj.lora_A.default.weight: (8, 768), attn.c_proj.lora_B.default.weight: (768, 8), mlp.c_fc.lora_A.default.weight: (8, 768), mlp.c_fc.lora_B.default.weight: (3072, 8), mlp.c_proj.lora_A.default.weight: (8, 3072), mlp.c_proj.lora_B.default.weight: (768, 8)]\n",
      "\n",
      "transformer.h.6:[attn.c_attn.lora_A.default.weight: (8, 768), attn.c_attn.lora_B.default.weight: (2304, 8), attn.c_proj.lora_A.default.weight: (8, 768), attn.c_proj.lora_B.default.weight: (768, 8), mlp.c_fc.lora_A.default.weight: (8, 768), mlp.c_fc.lora_B.default.weight: (3072, 8), mlp.c_proj.lora_A.default.weight: (8, 3072), mlp.c_proj.lora_B.default.weight: (768, 8)]\n",
      "\n",
      "transformer.h.7:[attn.c_attn.lora_A.default.weight: (8, 768), attn.c_attn.lora_B.default.weight: (2304, 8), attn.c_proj.lora_A.default.weight: (8, 768), attn.c_proj.lora_B.default.weight: (768, 8), mlp.c_fc.lora_A.default.weight: (8, 768), mlp.c_fc.lora_B.default.weight: (3072, 8), mlp.c_proj.lora_A.default.weight: (8, 3072), mlp.c_proj.lora_B.default.weight: (768, 8)]\n",
      "\n",
      "transformer.h.8:[attn.c_attn.lora_A.default.weight: (8, 768), attn.c_attn.lora_B.default.weight: (2304, 8), attn.c_proj.lora_A.default.weight: (8, 768), attn.c_proj.lora_B.default.weight: (768, 8), mlp.c_fc.lora_A.default.weight: (8, 768), mlp.c_fc.lora_B.default.weight: (3072, 8), mlp.c_proj.lora_A.default.weight: (8, 3072), mlp.c_proj.lora_B.default.weight: (768, 8)]\n",
      "==================Bottom Layers==================\n",
      "\n",
      "transformer.wte.weight:[: (50257, 768)]\n",
      "\n",
      "transformer.wpe.weight:[: (1024, 768)]\n",
      "\n",
      "transformer.h.0:[ln_1.weight: (768,), ln_1.bias: (768,), attn.c_attn.weight: (768, 2304), attn.c_attn.bias: (2304,), attn.c_proj.weight: (768, 768), attn.c_proj.bias: (768,), ln_2.weight: (768,), ln_2.bias: (768,), mlp.c_fc.weight: (768, 3072), mlp.c_fc.bias: (3072,), mlp.c_proj.weight: (3072, 768), mlp.c_proj.bias: (768,)]\n",
      "\n",
      "transformer.h.1:[ln_1.weight: (768,), ln_1.bias: (768,), attn.c_attn.weight: (768, 2304), attn.c_attn.bias: (2304,), attn.c_proj.weight: (768, 768), attn.c_proj.bias: (768,), ln_2.weight: (768,), ln_2.bias: (768,), mlp.c_fc.weight: (768, 3072), mlp.c_fc.bias: (3072,), mlp.c_proj.weight: (3072, 768), mlp.c_proj.bias: (768,)]\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "from sfl.simulator.simulator import SFLSimulator\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sfl.model.split_model import SplitModel\n",
    "from sfl.simulator.strategy import FLStrategy\n",
    "from sfl.simulator.dataset import PIQAFedDataset, FedDataset\n",
    "from sfl.utils import FLConfig, calculate_rouge\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "# 定义Client本地学习策略\n",
    "class QAFLStrategy(FLStrategy):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attacker_rouge_b2tr = []\n",
    "        self.attacker_rouge_tr2t = []\n",
    "\n",
    "    def client_step(self, client_id: str, llm: SplitModel, dataloader: DataLoader, cfg: FLConfig):\n",
    "        optimizer = AdamW(llm.parameters(), lr=1e-5)\n",
    "        with tqdm_notebook(total=cfg.client_epoch * len(dataloader)) as pbar:\n",
    "            for epoch in range(cfg.client_epoch):\n",
    "                for step, batch in enumerate(dataloader):\n",
    "                    optimizer.zero_grad()\n",
    "                    input_ids = batch['input_ids'].to(llm.device)\n",
    "                    attention_mask = batch['input_att_mask'].to(llm.device)\n",
    "                    outputs = llm(input_ids=input_ids, labels=input_ids, attention_mask=attention_mask)\n",
    "                    self.fp_done(client_id, epoch, step, batch)  # Collect intermediate results\n",
    "                    loss = outputs.loss\n",
    "                    pbar.set_description(f'Client {client_id} Epoch {epoch} Loss {loss.item():.3f}')\n",
    "                    loss.backward()\n",
    "                    self.bp_done(client_id, epoch, step, batch)  # Collect gradients\n",
    "                    optimizer.step()\n",
    "                    pbar.update(1)\n",
    "                avg_rouge = sum([r[\"rouge-l\"][\"f\"] for r in self.attacker_rouge_b2tr]) / len(self.attacker_rouge_b2tr)\n",
    "                print(f'ATTACK! Bottom-trunk, Client {client_id} Epoch {epoch} RougeL {avg_rouge:.3f}')\n",
    "                avg_rouge = sum([r['rouge-l']['f'] for r in self.attacker_rouge_tr2t]) / len(self.attacker_rouge_tr2t)\n",
    "                print(f'ATTACK! Trunk-Top, Client {client_id} Epoch {epoch} RougeL {avg_rouge:.3f}')\n",
    "                self.attacker_rouge_b2tr.clear()\n",
    "                self.attacker_rouge_tr2t.clear()\n",
    "\n",
    "    def callback_fp_param(self, client_id, local_epoch, local_step, b2tr_params, tr2t_params, batch):\n",
    "        #  这里获取某epoch、step中，前传过程的两次传输参数，b2tr(bottom-trunk), tr2t(trunk-top)\n",
    "        with torch.no_grad():\n",
    "            rouge_res_b2tr = calculate_rouge(tokenizer, attacker(b2tr_params), batch['input_text'])\n",
    "            rouge_res_tr2t = calculate_rouge(tokenizer, attacker2(tr2t_params), batch['input_text'])\n",
    "            self.attacker_rouge_b2tr.append(rouge_res_b2tr)\n",
    "            self.attacker_rouge_tr2t.append(rouge_res_tr2t)\n",
    "\n",
    "    def callback_bp_param(self, client_id, local_epoch, local_step, t2tr_params, tr2b_params, batch):\n",
    "        #  这里获取某epoch、step中，反传过程的两次传输参数\n",
    "        pass\n",
    "\n",
    "\n",
    "client_ids = [str(i) for i in range(3)]\n",
    "config = FLConfig(global_round=10, client_epoch=2, split_point_1=2, split_point_2=9, use_lora_at_trunk=True)\n",
    "fed_dataset = PIQAFedDataset(tokenizer=tokenizer, client_ids=client_ids,shrink_frac=0.15)\n",
    "simulator = SFLSimulator(client_ids=client_ids, strategy=QAFLStrategy(), llm=model, tokenizer=tokenizer,\n",
    "                         dataset=fed_dataset, config=config)\n",
    "model.print_split_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3. 开始联邦模拟"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================Global Round 0=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/72 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39bdd0afd76d4af0b093b56653d73a17"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 RougeL 0.909\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 0 RougeL 0.832\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 1 RougeL 0.912\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 1 RougeL 0.803\n",
      "Client 2 communication overhead: uplink:852.00 MB, downlink:852.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/112 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff290889db2145119ca19db28fede3d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 1 Epoch 0 RougeL 0.954\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 0 RougeL 0.876\n",
      "ATTACK! Bottom-trunk, Client 1 Epoch 1 RougeL 0.955\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 1 RougeL 0.838\n",
      "Client 1 communication overhead: uplink:1.30 GB, downlink:1.30 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/94 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cfb2ce9c7d7946358a6262e0f87ed79b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 0 Epoch 0 RougeL 0.938\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 0 RougeL 0.853\n",
      "ATTACK! Bottom-trunk, Client 0 Epoch 1 RougeL 0.941\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 1 RougeL 0.823\n",
      "Client 0 communication overhead: uplink:1.09 GB, downlink:1.09 GB\n",
      "Global Round 0 communication overhead: uplink=3.22 GB, downlink=3.22 GB\n",
      "==================================Global Round 1=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/94 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b02e9ef4a21d4eaa90f708f2555ffc67"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 0 Epoch 0 RougeL 0.944\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 0 RougeL 0.804\n",
      "ATTACK! Bottom-trunk, Client 0 Epoch 1 RougeL 0.934\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 1 RougeL 0.820\n",
      "Client 0 communication overhead: uplink:1.09 GB, downlink:1.09 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/72 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89ca326516b443828005da5213a44134"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 RougeL 0.910\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 0 RougeL 0.777\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 1 RougeL 0.900\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 1 RougeL 0.784\n",
      "Client 2 communication overhead: uplink:852.00 MB, downlink:852.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/112 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e46de88da7184081b7f7bd0df42c4d1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 1 Epoch 0 RougeL 0.951\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 0 RougeL 0.848\n",
      "ATTACK! Bottom-trunk, Client 1 Epoch 1 RougeL 0.949\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 1 RougeL 0.868\n",
      "Client 1 communication overhead: uplink:1.30 GB, downlink:1.30 GB\n",
      "Global Round 1 communication overhead: uplink=3.22 GB, downlink=3.22 GB\n",
      "==================================Global Round 2=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/112 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "623fc5726e2948489a989a0af087d1e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 1 Epoch 0 RougeL 0.952\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 0 RougeL 0.871\n",
      "ATTACK! Bottom-trunk, Client 1 Epoch 1 RougeL 0.946\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 1 RougeL 0.871\n",
      "Client 1 communication overhead: uplink:1.30 GB, downlink:1.30 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/94 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "798aab3b63e944ef9ab3f37d8375e48d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 0 Epoch 0 RougeL 0.931\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 0 RougeL 0.841\n",
      "ATTACK! Bottom-trunk, Client 0 Epoch 1 RougeL 0.931\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 1 RougeL 0.837\n",
      "Client 0 communication overhead: uplink:1.09 GB, downlink:1.09 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/72 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "96bf8073ec9a4f908234302edaba2497"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 RougeL 0.906\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 0 RougeL 0.824\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 1 RougeL 0.907\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 1 RougeL 0.827\n",
      "Client 2 communication overhead: uplink:852.00 MB, downlink:852.00 MB\n",
      "Global Round 2 communication overhead: uplink=3.22 GB, downlink=3.22 GB\n",
      "==================================Global Round 3=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/72 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "087865bcb5a04bfb9582b65c0f4e0079"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 RougeL 0.910\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 0 RougeL 0.827\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 1 RougeL 0.906\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 1 RougeL 0.827\n",
      "Client 2 communication overhead: uplink:852.00 MB, downlink:852.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/94 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d30737a85b845eb94634e9f65fe104a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 0 Epoch 0 RougeL 0.935\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 0 RougeL 0.847\n",
      "ATTACK! Bottom-trunk, Client 0 Epoch 1 RougeL 0.932\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 1 RougeL 0.852\n",
      "Client 0 communication overhead: uplink:1.09 GB, downlink:1.09 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/112 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c954ad03e805440a811b43ebd95dd524"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 1 Epoch 0 RougeL 0.948\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 0 RougeL 0.879\n",
      "ATTACK! Bottom-trunk, Client 1 Epoch 1 RougeL 0.950\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 1 RougeL 0.881\n",
      "Client 1 communication overhead: uplink:1.30 GB, downlink:1.30 GB\n",
      "Global Round 3 communication overhead: uplink=3.22 GB, downlink=3.22 GB\n",
      "==================================Global Round 4=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/72 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad749d6c3c1542c88dfacf35d69cd883"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 RougeL 0.899\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 0 RougeL 0.830\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 1 RougeL 0.903\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 1 RougeL 0.828\n",
      "Client 2 communication overhead: uplink:852.00 MB, downlink:852.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/112 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e37ee2cb034b412bbafc5d86cc44ff79"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 1 Epoch 0 RougeL 0.949\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 0 RougeL 0.878\n",
      "ATTACK! Bottom-trunk, Client 1 Epoch 1 RougeL 0.947\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 1 RougeL 0.871\n",
      "Client 1 communication overhead: uplink:1.30 GB, downlink:1.30 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/94 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e7aab1a867d4b0db573198d318b7838"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 0 Epoch 0 RougeL 0.925\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 0 RougeL 0.847\n",
      "ATTACK! Bottom-trunk, Client 0 Epoch 1 RougeL 0.931\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 1 RougeL 0.853\n",
      "Client 0 communication overhead: uplink:1.09 GB, downlink:1.09 GB\n",
      "Global Round 4 communication overhead: uplink=3.22 GB, downlink=3.22 GB\n",
      "==================================Global Round 5=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/94 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55e1c3a1fb0544398eb0cf1e159e667d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 0 Epoch 0 RougeL 0.931\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 0 RougeL 0.848\n",
      "ATTACK! Bottom-trunk, Client 0 Epoch 1 RougeL 0.927\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 1 RougeL 0.853\n",
      "Client 0 communication overhead: uplink:1.09 GB, downlink:1.09 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/112 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "245ed5cbf22a4b79b3034ebe4b348670"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 1 Epoch 0 RougeL 0.950\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 0 RougeL 0.879\n",
      "ATTACK! Bottom-trunk, Client 1 Epoch 1 RougeL 0.950\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 1 RougeL 0.880\n",
      "Client 1 communication overhead: uplink:1.30 GB, downlink:1.30 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/72 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb38afb209454d829013fbc3548aeb5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 RougeL 0.907\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 0 RougeL 0.831\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 1 RougeL 0.897\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 1 RougeL 0.836\n",
      "Client 2 communication overhead: uplink:852.00 MB, downlink:852.00 MB\n",
      "Global Round 5 communication overhead: uplink=3.22 GB, downlink=3.22 GB\n",
      "==================================Global Round 6=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/72 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e52638bd1df9480086d3adc7c41930b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 RougeL 0.895\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 0 RougeL 0.831\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 1 RougeL 0.893\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 1 RougeL 0.825\n",
      "Client 2 communication overhead: uplink:852.00 MB, downlink:852.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/94 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53e816a6bb594643b5f4ec82ac552d8c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 0 Epoch 0 RougeL 0.930\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 0 RougeL 0.849\n",
      "ATTACK! Bottom-trunk, Client 0 Epoch 1 RougeL 0.922\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 1 RougeL 0.846\n",
      "Client 0 communication overhead: uplink:1.09 GB, downlink:1.09 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/112 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "821ddd1a39ae486ea9cf0241cf342b5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 1 Epoch 0 RougeL 0.947\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 0 RougeL 0.868\n",
      "ATTACK! Bottom-trunk, Client 1 Epoch 1 RougeL 0.946\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 1 RougeL 0.869\n",
      "Client 1 communication overhead: uplink:1.30 GB, downlink:1.30 GB\n",
      "Global Round 6 communication overhead: uplink=3.22 GB, downlink=3.22 GB\n",
      "==================================Global Round 7=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/72 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d82e7d0b4f9f40469efa0cf4c96f5fcd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 RougeL 0.906\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 0 RougeL 0.826\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 1 RougeL 0.903\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 1 RougeL 0.818\n",
      "Client 2 communication overhead: uplink:852.00 MB, downlink:852.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/112 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9bd7d9cf65b4a0dbc4b9129cf8d1352"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 1 Epoch 0 RougeL 0.951\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 0 RougeL 0.872\n",
      "ATTACK! Bottom-trunk, Client 1 Epoch 1 RougeL 0.950\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 1 RougeL 0.863\n",
      "Client 1 communication overhead: uplink:1.30 GB, downlink:1.30 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/94 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8fe3903f20964172b0a8b828bab52aa0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 0 Epoch 0 RougeL 0.922\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 0 RougeL 0.845\n",
      "ATTACK! Bottom-trunk, Client 0 Epoch 1 RougeL 0.924\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 1 RougeL 0.842\n",
      "Client 0 communication overhead: uplink:1.09 GB, downlink:1.09 GB\n",
      "Global Round 7 communication overhead: uplink=3.22 GB, downlink=3.22 GB\n",
      "==================================Global Round 8=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/94 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a47c1a0bc19a411fae827b7dca6e29ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 0 Epoch 0 RougeL 0.922\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 0 RougeL 0.843\n",
      "ATTACK! Bottom-trunk, Client 0 Epoch 1 RougeL 0.916\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 1 RougeL 0.841\n",
      "Client 0 communication overhead: uplink:1.09 GB, downlink:1.09 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/72 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2523b2b803a4779af4539ca980aab76"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 RougeL 0.902\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 0 RougeL 0.827\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 1 RougeL 0.902\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 1 RougeL 0.827\n",
      "Client 2 communication overhead: uplink:852.00 MB, downlink:852.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/112 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de0770c62b1049e3a3682ff5a57d53c8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 1 Epoch 0 RougeL 0.945\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 0 RougeL 0.869\n",
      "ATTACK! Bottom-trunk, Client 1 Epoch 1 RougeL 0.943\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 1 RougeL 0.874\n",
      "Client 1 communication overhead: uplink:1.30 GB, downlink:1.30 GB\n",
      "Global Round 8 communication overhead: uplink=3.22 GB, downlink=3.22 GB\n",
      "==================================Global Round 9=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/112 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f0c99a798d1d4383b1341d6e00ab29fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 1 Epoch 0 RougeL 0.944\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 0 RougeL 0.867\n",
      "ATTACK! Bottom-trunk, Client 1 Epoch 1 RougeL 0.950\n",
      "ATTACK! Trunk-Top, Client 1 Epoch 1 RougeL 0.869\n",
      "Client 1 communication overhead: uplink:1.30 GB, downlink:1.30 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/72 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13f71e1f791e4396a5f657573cdc8314"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 RougeL 0.897\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 0 RougeL 0.819\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 1 RougeL 0.899\n",
      "ATTACK! Trunk-Top, Client 2 Epoch 1 RougeL 0.825\n",
      "Client 2 communication overhead: uplink:852.00 MB, downlink:852.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/94 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f96e70b63fc145c7831b902667929006"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 0 Epoch 0 RougeL 0.926\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 0 RougeL 0.845\n",
      "ATTACK! Bottom-trunk, Client 0 Epoch 1 RougeL 0.921\n",
      "ATTACK! Trunk-Top, Client 0 Epoch 1 RougeL 0.843\n",
      "Client 0 communication overhead: uplink:1.09 GB, downlink:1.09 GB\n",
      "Global Round 9 communication overhead: uplink=3.22 GB, downlink=3.22 GB\n",
      "FL communication overhead: uplink=32.23 GB, downlink=32.23 GB\n"
     ]
    }
   ],
   "source": [
    "simulator.simulate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to finish my thesis. Solution:Gather all the materials needed for the project.\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"How to finish my thesis.\", model))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}