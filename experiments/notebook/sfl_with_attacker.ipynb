{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1. 加载模型与Tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"client0_attacker_14_normal_step\": 0.5175706596299099,\n",
    "  \"client0_attacker_19_normal_step\": 0.5429864203393666,\n",
    "  \"client0_attacker_32_normal_step\": 0.3857142807290817,\n",
    "  \"_step\": 4249,\n",
    "  \"client0_loss\": 2.031713839054108,\n",
    "  \"client0_attacker_29_tr2t_avg\": 0.5275647235774503,\n",
    "  \"client0_attacker_8_normal_avg\": 0.6366766778182428,\n",
    "  \"client0_attacker_16_normal_step\": 0.5285769033264401,\n",
    "  \"client0_attacker_24_normal_step\": 0.544871789879511,\n",
    "  \"client0_attacker_28_normal_step\": 0.4467532417557092,\n",
    "  \"client0_test-ppl\": 14.366519927978516,\n",
    "  \"client0_attacker_0_normal_avg\": 0.5362440276624566,\n",
    "  \"client0_attacker_32_normal_avg\": 0.4366247128481753,\n",
    "  \"client0_attacker_9_normal_step\": 0.5223380441441088,\n",
    "  \"client0_attacker_34_normal_avg\": 0.31704178424232093,\n",
    "  \"client0_attacker_35_normal_avg\": 0.27924916250140014,\n",
    "  \"client0_attacker_30_normal_step\": 0.4590469049036024,\n",
    "  \"global_step\": 4249,\n",
    "  \"client0_global_round\": 0,\n",
    "  \"client0_attacker_5_b2tr_avg\": 0.6304907277186129,\n",
    "  \"client0_attacker_3_normal_avg\": 0.6183885509769569,\n",
    "  \"client0_attacker_23_normal_step\": 0.5818181768206444,\n",
    "  \"client0_attacker_31_normal_step\": 0.41103848447329144,\n",
    "  \"client0_self_pt\": 0,\n",
    "  \"client0_attacker_11_normal_avg\": 0.6377722236693276,\n",
    "  \"client0_attacker_33_normal_avg\": 0.39310278683275807,\n",
    "  \"client0_attacker_6_normal_step\": 0.5439360879591302,\n",
    "  \"client0_attacker_2_normal_avg\": 0.6089481650353057,\n",
    "  \"client0_attacker_4_normal_avg\": 0.6238578184133798,\n",
    "  \"client0_attacker_22_normal_step\": 0.5893812020286877,\n",
    "  \"client0_attacker_25_normal_step\": 0.5504761854822313,\n",
    "  \"client0_attacker_13_normal_step\": 0.5263951684539971,\n",
    "  \"client0_attacker_9_normal_avg\": 0.6384095213150243,\n",
    "  \"client0_attacker_16_normal_avg\": 0.6330719245495596,\n",
    "  \"client0_attacker_24_normal_avg\": 0.5943958147410345,\n",
    "  \"client0_attacker_26_normal_avg\": 0.5723204183075637,\n",
    "  \"client0_attacker_15_normal_step\": 0.5175706596299099,\n",
    "  \"client0_attacker_5_b2tr_step\": 0.5145243232532479,\n",
    "  \"client0_attacker_7_normal_avg\": 0.6351210251036679,\n",
    "  \"client0_attacker_12_normal_avg\": 0.6366890118070682,\n",
    "  \"client0_attacker_13_normal_avg\": 0.6347554874791649,\n",
    "  \"client0_attacker_21_normal_avg\": 0.620009580633759,\n",
    "  \"client0_attacker_25_normal_avg\": 0.5867858649221694,\n",
    "  \"client0_attacker_27_normal_avg\": 0.5616850112415854,\n",
    "  \"client0_attacker_3_normal_step\": 0.5351071642617553,\n",
    "  \"_wandb.runtime\": 1649,\n",
    "  \"client0_local_epoch\": 0,\n",
    "  \"client0_attacker_10_normal_avg\": 0.6380667878549796,\n",
    "  \"client0_attacker_20_normal_avg\": 0.624557655192967,\n",
    "  \"client0_attacker_20_normal_step\": 0.5729323258308399,\n",
    "  \"client0_attacker_21_normal_step\": 0.5688311638336314,\n",
    "  \"client0_attacker_33_normal_step\": 0.33369407870222145,\n",
    "  \"client0_attacker_34_normal_step\": 0.30359476626650117,\n",
    "  \"client0_attacker_31_normal_avg\": 0.4747830768028701,\n",
    "  \"client0_attacker_10_normal_step\": 0.5096798162960074,\n",
    "  \"client0_attacker_12_normal_step\": 0.5175706596299099,\n",
    "  \"client0_attacker_26_normal_step\": 0.5203007468834715,\n",
    "  \"_runtime\": 1651.2562370300293,\n",
    "  \"_timestamp\": 1707696679.616939,\n",
    "  \"client0_attacker_1_normal_step\": 0.5177280500885659,\n",
    "  \"client0_attacker_28_normal_avg\": 0.5459204030441271,\n",
    "  \"client0_attacker_6_normal_avg\": 0.6324742085562801,\n",
    "  \"client0_attacker_17_normal_avg\": 0.6309008512450668,\n",
    "  \"client0_attacker_18_normal_avg\": 0.6301045366130664,\n",
    "  \"client0_attacker_4_normal_step\": 0.5473022862129748,\n",
    "  \"client0_attacker_17_normal_step\": 0.5469824243357921,\n",
    "  \"client0_attacker_15_normal_avg\": 0.6324732529184328,\n",
    "  \"client0_attacker_22_normal_avg\": 0.611303527540108,\n",
    "  \"client0_attacker_23_normal_avg\": 0.6043506538926331,\n",
    "  \"client0_attacker_7_normal_step\": 0.5562817669714759,\n",
    "  \"client0_self\": 0.0005098648707545071,\n",
    "  \"client0_local_step\": 4249,\n",
    "  \"client0_attacker_8_normal_step\": 0.5549242374281007,\n",
    "  \"client0_attacker_29_tr2t_step\": 0.454184699192842,\n",
    "  \"client0_attacker_2_normal_step\": 0.5012778336386043,\n",
    "  \"client0_attacker_11_normal_step\": 0.5135746556334841,\n",
    "  \"client0_attacker_0_normal_step\": 0.5027100221139359,\n",
    "  \"client0_attacker_14_normal_avg\": 0.6337987627417009,\n",
    "  \"client0_attacker_35_normal_step\": 0.2107142807178891,\n",
    "  \"client0_attacker_27_normal_step\": 0.5097402547483976,\n",
    "  \"client0_attacker_1_normal_avg\": 0.5966522677713505,\n",
    "  \"client0_attacker_19_normal_avg\": 0.627233252992626,\n",
    "  \"client0_attacker_30_normal_avg\": 0.5104403879249436,\n",
    "  \"client0_attacker_18_normal_step\": 0.5510835863330007\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tens!\n",
      "tens!\n",
      "tens!\n",
      "ntens!\n",
      "tens!\n",
      "{'input_ids': tensor([[  257,  2196,  1262,   281,   477,  2488,    12,    31,  2042,  3350,\n",
      "           351,  1279,  2954,    29, 14926,  1279,  2954,    29,   355,  3619,\n",
      "           837,   370,   918,   309,    13,  4373,   355,   978,    70,  1142,\n",
      "           261,   837,  5506,   775, 25900,   355, 11182,  1709,   441, 10076,\n",
      "           837,  1279,  2954,    29, 29045,   355, 23367,   813,   837,  5180,\n",
      "          4889, 41788,   355, 39661,   437,  8622,   837,  1279,  2954,    29,\n",
      "          1279,  2954,    29,   355,  4544, 35417,   837,   290,  1279,  2954,\n",
      "            29, 15722,   355,  9356,   609,   292, 26664,   837,   900,   287,\n",
      "           262,  1578,  1829,   764, 15416, 13612,   837,   281,  3594,  3437,\n",
      "           508,   550,  4271, 16573,  1052, 41765, 13895,  3903,   416, 45622,\n",
      "           837,   925,   262,  6244,  2646,  2162,   340,  5788, 18373,   376,\n",
      "          3333,   357,  3619,  1267,   837, 34315, 36815,   357,   978,  1360,\n",
      "          1267,   837,  4794,    72,  5601,   354,   357, 11182],\n",
      "        [  383,  8860,   810,   262,  4519,  1718,  1295,   318,   262,  1279,\n",
      "          2954,    29,   357,   366, 12556,  4401,   366,  1267, 10977, 19239,\n",
      "           357,  1279,  2954,    29,  1267,   837,  6898,   416,  9764,  9071,\n",
      "          2488,    12,    31,  1912, 12556,  4401,  4037,   357, 31681,  1267,\n",
      "         12052,   764,   837,   262,  4387, 13373, 11554,   287,   262,   995,\n",
      "           764,   383,  1664,   705,    82, 19413,   519,  7258,  8860,   287,\n",
      "           262,  1279,  2954,    29,  4783, 24803,   617,  1467,  2488,    11,\n",
      "            31, 12877,  3259,   764,  1629,   262, 49672,   286,   262, 36980,\n",
      "            67,   506,  4773,   837,   340,  9657, 10460,  3259,   422, 20578,\n",
      "          4563,   837,   287, 25426, 39598,   355,   636,   286,   281,  9450,\n",
      "          1430,   543, 34233,   939,  2488,    11,    31, 12877,  1862,   471,\n",
      "            88,   456,  1834,  1201,   262,   923,   286,  3648,   764,  4784,\n",
      "           284,   383,  8283,   837,   749,  3259,  1051,   257]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  257,  2196,  1262,   281,   477,  2488,    12,    31,  2042,  3350,\n",
      "           351,  1279,  2954,    29, 14926,  1279,  2954,    29,   355,  3619,\n",
      "           837,   370,   918,   309,    13,  4373,   355,   978,    70,  1142,\n",
      "           261,   837,  5506,   775, 25900,   355, 11182,  1709,   441, 10076,\n",
      "           837,  1279,  2954,    29, 29045,   355, 23367,   813,   837,  5180,\n",
      "          4889, 41788,   355, 39661,   437,  8622,   837,  1279,  2954,    29,\n",
      "          1279,  2954,    29,   355,  4544, 35417,   837,   290,  1279,  2954,\n",
      "            29, 15722,   355,  9356,   609,   292, 26664,   837,   900,   287,\n",
      "           262,  1578,  1829,   764, 15416, 13612,   837,   281,  3594,  3437,\n",
      "           508,   550,  4271, 16573,  1052, 41765, 13895,  3903,   416, 45622,\n",
      "           837,   925,   262,  6244,  2646,  2162,   340,  5788, 18373,   376,\n",
      "          3333,   357,  3619,  1267,   837, 34315, 36815,   357,   978,  1360,\n",
      "          1267,   837,  4794,    72,  5601,   354,   357, 11182],\n",
      "        [  383,  8860,   810,   262,  4519,  1718,  1295,   318,   262,  1279,\n",
      "          2954,    29,   357,   366, 12556,  4401,   366,  1267, 10977, 19239,\n",
      "           357,  1279,  2954,    29,  1267,   837,  6898,   416,  9764,  9071,\n",
      "          2488,    12,    31,  1912, 12556,  4401,  4037,   357, 31681,  1267,\n",
      "         12052,   764,   837,   262,  4387, 13373, 11554,   287,   262,   995,\n",
      "           764,   383,  1664,   705,    82, 19413,   519,  7258,  8860,   287,\n",
      "           262,  1279,  2954,    29,  4783, 24803,   617,  1467,  2488,    11,\n",
      "            31, 12877,  3259,   764,  1629,   262, 49672,   286,   262, 36980,\n",
      "            67,   506,  4773,   837,   340,  9657, 10460,  3259,   422, 20578,\n",
      "          4563,   837,   287, 25426, 39598,   355,   636,   286,   281,  9450,\n",
      "          1430,   543, 34233,   939,  2488,    11,    31, 12877,  1862,   471,\n",
      "            88,   456,  1834,  1201,   262,   923,   286,  3648,   764,  4784,\n",
      "           284,   383,  8283,   837,   749,  3259,  1051,   257]]), 'input_text': [' a version using an all @-@ black cast with <unk> Keith <unk> as Jack, Wren T. Brown as Algernon, Ann Weldon as Lady Bracknell, <unk> Chapman as Cecily, Chris Calloway as Gwendolen, <unk> <unk> as Miss Prism, and <unk> Peters as Doctor Chasuble, set in the United States. Oliver Parker, an English director who had previously adapted An Ideal Husband by Wilde, made the 2002 film ; it stars Colin Firth ( Jack ), Rupert Everett ( Algy ), Judi Dench ( Lady', ' The factory where the incident took place is the <unk> ( \" Early Light \" ) Toy Factory ( <unk> ), owned by Hong Kong @-@ based Early Light International ( Holdings ) Ltd., the largest toy manufacturer in the world. The company\\'s Shaoguan factory in the <unk> district employs some 16 @,@ 000 workers. At the behest of the Guangdong authorities, it hired 800 workers from Kashgar, in Xinjiang as part of an ethnic program which relocated 200 @,@ 000 young Uyghurs since the start of 2008. According to The Guardian, most workers sign a'], 'input_att_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "from sfl import config\n",
    "from transformers import AutoTokenizer\n",
    "from sfl.simulator.dataset import WikiTextFedDataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(config.model_download_dir, \"gpt2/\"))\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = 50256\n",
    "ds = WikiTextFedDataset(tokenizer, ['0', '1'])\n",
    "dl = ds.get_dataloader('0',batch_size=2)\n",
    "for x in dl:\n",
    "    print(x)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassificationSplitModel were not initialized from the model checkpoint at /root/autodl-tmp/sfl/models/google-bert/bert-base-uncased/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "from sfl.model.bert.bert_wrapper import BertForSequenceClassificationSplitModel\n",
    "from sfl import config\n",
    "\n",
    "model = BertForSequenceClassificationSplitModel.from_pretrained(os.path.join(config.model_download_dir, \"google-bert/bert-base-uncased/\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from sfl.config import FLConfig\n",
    "config = FLConfig(global_round=50,\n",
    "                  client_epoch=2,  # 每轮联邦每个Client训2轮\n",
    "                  split_point_1=3,\n",
    "                  split_point_2=10,  # [0,1 | 2,3,.... 29| 30, 31]\n",
    "                  use_lora_at_trunk=True,  # 在trunk部分使用LoRA\n",
    "                  top_and_bottom_from_scratch='False',  # top和bottom都不采用预训练参数.\n",
    "                  noise_mode=\"dxp\",\n",
    "                  noise_scale=5.0,  # 噪声大小\n",
    "                  use_lora_at_top=False,\n",
    "                  use_lora_at_bottom=False\n",
    "                  )\n",
    "\n",
    "model.config_sfl(config, None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "model = model.convert_to_lora_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "BertConfig {\n  \"_name_or_path\": \"/root/autodl-tmp/sfl/models/google-bert/bert-base-uncased/\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.31.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30522\n}"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m device \u001B[38;5;241m=\u001B[39m get_best_gpu()\n\u001B[1;32m     11\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(config\u001B[38;5;241m.\u001B[39mmodel_download_dir, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt2/\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m---> 12\u001B[0m model \u001B[38;5;241m=\u001B[39m GPT2SplitLMHeadModel\u001B[38;5;241m.\u001B[39mfrom_pretrained(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(config\u001B[38;5;241m.\u001B[39mmodel_download_dir, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt2-large/\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     13\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mpad_token \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39meos_token\n\u001B[1;32m     14\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mpad_token_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m50256\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/transformers/modeling_utils.py:2700\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   2697\u001B[0m     init_contexts\u001B[38;5;241m.\u001B[39mappend(init_empty_weights())\n\u001B[1;32m   2699\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ContextManagers(init_contexts):\n\u001B[0;32m-> 2700\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m(config, \u001B[38;5;241m*\u001B[39mmodel_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs)\n\u001B[1;32m   2702\u001B[0m \u001B[38;5;66;03m# Check first if we are `from_pt`\u001B[39;00m\n\u001B[1;32m   2703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_keep_in_fp32_modules:\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/model/gpt2/gpt2_split.py:77\u001B[0m, in \u001B[0;36mGPT2SplitLMHeadModel.__init__\u001B[0;34m(self, config)\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, config):\n\u001B[0;32m---> 77\u001B[0m     \u001B[38;5;28msuper\u001B[39m(GPT2SplitLMHeadModel, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(config)\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer \u001B[38;5;241m=\u001B[39m GPT2SplitModel(config)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:961\u001B[0m, in \u001B[0;36mGPT2LMHeadModel.__init__\u001B[0;34m(self, config)\u001B[0m\n\u001B[1;32m    959\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, config):\n\u001B[1;32m    960\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(config)\n\u001B[0;32m--> 961\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer \u001B[38;5;241m=\u001B[39m GPT2Model(config)\n\u001B[1;32m    962\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(config\u001B[38;5;241m.\u001B[39mn_embd, config\u001B[38;5;241m.\u001B[39mvocab_size, bias\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    964\u001B[0m     \u001B[38;5;66;03m# Model parallel\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:680\u001B[0m, in \u001B[0;36mGPT2Model.__init__\u001B[0;34m(self, config)\u001B[0m\n\u001B[1;32m    677\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwpe \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mEmbedding(config\u001B[38;5;241m.\u001B[39mmax_position_embeddings, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim)\n\u001B[1;32m    679\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mDropout(config\u001B[38;5;241m.\u001B[39membd_pdrop)\n\u001B[0;32m--> 680\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mh \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mModuleList([GPT2Block(config, layer_idx\u001B[38;5;241m=\u001B[39mi) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(config\u001B[38;5;241m.\u001B[39mnum_hidden_layers)])\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln_f \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLayerNorm(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim, eps\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mlayer_norm_epsilon)\n\u001B[1;32m    683\u001B[0m \u001B[38;5;66;03m# Model parallel\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:680\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    677\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwpe \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mEmbedding(config\u001B[38;5;241m.\u001B[39mmax_position_embeddings, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim)\n\u001B[1;32m    679\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mDropout(config\u001B[38;5;241m.\u001B[39membd_pdrop)\n\u001B[0;32m--> 680\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mh \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mModuleList([GPT2Block(config, layer_idx\u001B[38;5;241m=\u001B[39mi) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(config\u001B[38;5;241m.\u001B[39mnum_hidden_layers)])\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln_f \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLayerNorm(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim, eps\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mlayer_norm_epsilon)\n\u001B[1;32m    683\u001B[0m \u001B[38;5;66;03m# Model parallel\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:376\u001B[0m, in \u001B[0;36mGPT2Block.__init__\u001B[0;34m(self, config, layer_idx)\u001B[0m\n\u001B[1;32m    373\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcrossattention \u001B[38;5;241m=\u001B[39m GPT2Attention(config, is_cross_attention\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, layer_idx\u001B[38;5;241m=\u001B[39mlayer_idx)\n\u001B[1;32m    374\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln_cross_attn \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLayerNorm(hidden_size, eps\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mlayer_norm_epsilon)\n\u001B[0;32m--> 376\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp \u001B[38;5;241m=\u001B[39m GPT2MLP(inner_dim, config)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:349\u001B[0m, in \u001B[0;36mGPT2MLP.__init__\u001B[0;34m(self, intermediate_size, config)\u001B[0m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[1;32m    348\u001B[0m embed_dim \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mhidden_size\n\u001B[0;32m--> 349\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mc_fc \u001B[38;5;241m=\u001B[39m Conv1D(intermediate_size, embed_dim)\n\u001B[1;32m    350\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mc_proj \u001B[38;5;241m=\u001B[39m Conv1D(embed_dim, intermediate_size)\n\u001B[1;32m    351\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact \u001B[38;5;241m=\u001B[39m ACT2FN[config\u001B[38;5;241m.\u001B[39mactivation_function]\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/transformers/pytorch_utils.py:101\u001B[0m, in \u001B[0;36mConv1D.__init__\u001B[0;34m(self, nf, nx)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mParameter(torch\u001B[38;5;241m.\u001B[39mempty(nx, nf))\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mParameter(torch\u001B[38;5;241m.\u001B[39mzeros(nf))\n\u001B[0;32m--> 101\u001B[0m nn\u001B[38;5;241m.\u001B[39minit\u001B[38;5;241m.\u001B[39mnormal_(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, std\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.02\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/init.py:155\u001B[0m, in \u001B[0;36mnormal_\u001B[0;34m(tensor, mean, std)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39moverrides\u001B[38;5;241m.\u001B[39mhas_torch_function_variadic(tensor):\n\u001B[1;32m    154\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39moverrides\u001B[38;5;241m.\u001B[39mhandle_torch_function(normal_, (tensor,), tensor\u001B[38;5;241m=\u001B[39mtensor, mean\u001B[38;5;241m=\u001B[39mmean, std\u001B[38;5;241m=\u001B[39mstd)\n\u001B[0;32m--> 155\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _no_grad_normal_(tensor, mean, std)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/init.py:19\u001B[0m, in \u001B[0;36m_no_grad_normal_\u001B[0;34m(tensor, mean, std)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_no_grad_normal_\u001B[39m(tensor, mean, std):\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 19\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m tensor\u001B[38;5;241m.\u001B[39mnormal_(mean, std)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "from sfl.utils.training import get_best_gpu\n",
    "from sfl.model.gpt2.gpt2_split import GPT2SplitLMHeadModel\n",
    "from transformers import AutoTokenizer\n",
    "from sfl import config\n",
    "\n",
    "device = get_best_gpu()\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(config.model_download_dir, \"gpt2/\"))\n",
    "model = GPT2SplitLMHeadModel.from_pretrained(os.path.join(config.model_download_dir, \"gpt2-large/\"))\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = 50256\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "如何自杀的话,但那么就没有可能会让他们绝对于这种情况下来说不定,而且比较好了吧。 \"I see, so that's how it is. Then, I'll leave it to you, Your Majesty.\" 「当然,非常感谢。首先抱歉,魔导王也不会选择」 \"Understood!\" 「遵命!」 Remedios bowed deeply. 蕾梅蒂欧丝发出现在圣王国的视线贝德的提问题,只要接着古斯塔博打算的反应。 After that, the two of them left the room. 安兹确实际上前进行,涅娅将亚乌菈以及被深处交消失的笑容。 As they left, Ainz could not help but feel a sense of déjà vu, as though he had seen the same scene before. 为此时候是知道一点的想法,虽然恐怕恢复的意思。 He had not seen it before, and he did not want to see it again. However, that was not the point. The point was that this was the first time he was seeing this scene in his entire life. 然后,还是更多少并不清楚。不过,从脑海和哪里的人类等待。若是身体的心中,完全不�\n"
     ]
    }
   ],
   "source": [
    "from sfl.utils.model import generate\n",
    "\n",
    "\n",
    "# 测试模型输出\n",
    "def get_output(text, md=model):\n",
    "    t = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    res = model(t['input_ids'].to(md.device), attention_mask=t['attention_mask'].to(md.device))\n",
    "    r = tokenizer.decode(res.logits.argmax(dim=-1)[-1], skip_special_tokens=True)\n",
    "    return r\n",
    "\n",
    "\n",
    "print(generate(\"如何自杀\", tokenizer, model))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2. 加载攻击模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from sfl.config import attacker_path\n",
    "from sfl.utils.training import get_attacker_class, extract_attacker_path\n",
    "\n",
    "attack_model = 'gru'\n",
    "# 加载攻击模型\n",
    "attacker_cls = get_attacker_class(attack_model)\n",
    "attacker_path_1, attacker_path_2 = extract_attacker_path(\n",
    "    {'split_point_1': 2, 'split_point_2': 10, 'attacker_path': attacker_path})\n",
    "attacker = attacker_cls.from_pretrained(attacker_path_1)\n",
    "attacker2 = attacker_cls.from_pretrained(attacker_path_2)\n",
    "#\n",
    "# attacker = GRUAttackModel.from_pretrained(\n",
    "#     '/root/autodl-tmp/sfl/models/attacker/gpt2/piqa/train*1.000-test*1.000/gru/b2tr-2/epoch_19_rouge_0.9849')\n",
    "# attacker2 = GRUAttackModel.from_pretrained(\n",
    "#     '/root/autodl-tmp/sfl/models/attacker/gpt2/piqa/train*1.000-test*1.000/gru/tr2t-10/epoch_19_rouge_0.9261')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3. 设置联邦训练流程"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================Split-/root/autodl-tmp/sfl/models/gpt2-large/=================\n",
      "==================Top Layers==================\n",
      "\n",
      "transformer.h.32:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.33:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.34:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.35:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.ln_f.weight:[: (1280,)]\n",
      "\n",
      "transformer.ln_f.bias:[: (1280,)]\n",
      "==================Trunk Layers==================\n",
      "\n",
      "transformer.h.3:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.4:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.5:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.6:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.7:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.8:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.9:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.10:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.11:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.12:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.13:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.14:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.15:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.16:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.17:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.18:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.19:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.20:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.21:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.22:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.23:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.24:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.25:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.26:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.27:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.28:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.29:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.30:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.31:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "==================Bottom Layers==================\n",
      "\n",
      "transformer.wte.weight:[: (50257, 1280)]\n",
      "\n",
      "transformer.wpe.weight:[: (1024, 1280)]\n",
      "\n",
      "transformer.h.0:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.1:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.2:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "from sfl.config import FLConfig\n",
    "config = FLConfig(global_round=50,\n",
    "                  client_epoch=2,  # 每轮联邦每个Client训2轮\n",
    "                  split_point_1=3,\n",
    "                  split_point_2=32,  # [0,1 | 2,3,.... 29| 30, 31]\n",
    "                  use_lora_at_trunk=True,  # 在trunk部分使用LoRA\n",
    "                  top_and_bottom_from_scratch='False',  # top和bottom都不采用预训练参数.\n",
    "                  noise_mode=\"dxp\",\n",
    "                  noise_scale=5.0,  # 噪声大小\n",
    "                  )\n",
    "\n",
    "model.config_sfl(config, None)\n",
    "model.print_split_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/piqa/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011 (last modified on Wed Jan 10 12:49:01 2024) since it couldn't be found locally at piqa., or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class QAFLStrategy with abstract methods callback_intermediate_result, client_evaluate",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 87\u001B[0m\n\u001B[1;32m     77\u001B[0m config \u001B[38;5;241m=\u001B[39m FLConfig(global_round\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m,\n\u001B[1;32m     78\u001B[0m                   client_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,  \u001B[38;5;66;03m# 每轮联邦每个Client训2轮\u001B[39;00m\n\u001B[1;32m     79\u001B[0m                   split_point_1\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     84\u001B[0m                   noise_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5.0\u001B[39m,  \u001B[38;5;66;03m# 噪声大小\u001B[39;00m\n\u001B[1;32m     85\u001B[0m                   )\n\u001B[1;32m     86\u001B[0m fed_dataset \u001B[38;5;241m=\u001B[39m PIQAFedDataset(tokenizer\u001B[38;5;241m=\u001B[39mtokenizer, client_ids\u001B[38;5;241m=\u001B[39mclient_ids, shrink_frac\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.15\u001B[39m)\n\u001B[0;32m---> 87\u001B[0m simulator \u001B[38;5;241m=\u001B[39m SFLSimulator(client_ids\u001B[38;5;241m=\u001B[39mclient_ids, strategy\u001B[38;5;241m=\u001B[39mQAFLStrategy(), llm\u001B[38;5;241m=\u001B[39mmodel, tokenizer\u001B[38;5;241m=\u001B[39mtokenizer,\n\u001B[1;32m     88\u001B[0m                          dataset\u001B[38;5;241m=\u001B[39mfed_dataset, config\u001B[38;5;241m=\u001B[39mconfig)\n\u001B[1;32m     89\u001B[0m model\u001B[38;5;241m.\u001B[39mprint_split_model()\n\u001B[1;32m     90\u001B[0m attacker\u001B[38;5;241m.\u001B[39mto(model\u001B[38;5;241m.\u001B[39mdevice)\n",
      "\u001B[0;31mTypeError\u001B[0m: Can't instantiate abstract class QAFLStrategy with abstract methods callback_intermediate_result, client_evaluate"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sfl.utils.model import calculate_rouge\n",
    "from sfl.config import FLConfig\n",
    "from sfl.simulator.simulator import SFLSimulator\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sfl.model.split_model import SplitModel\n",
    "from sfl.simulator.strategy import FLStrategy\n",
    "from sfl.simulator.dataset import PIQAFedDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "# 定义Client本地学习策略\n",
    "class QAFLStrategy(FLStrategy):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attacker_rouge_b2tr = []\n",
    "        self.attacker_rouge_tr2t = []\n",
    "        self.client_logs = {}\n",
    "\n",
    "    def client_step(self, global_round, client_id: str, llm: SplitModel, dataloader: DataLoader, cfg: FLConfig):\n",
    "        optimizer = AdamW(llm.parameters(), lr=1e-5)\n",
    "        with tqdm_notebook(total=cfg.client_epoch * len(dataloader)) as pbar:\n",
    "            for epoch in range(cfg.client_epoch):\n",
    "                for step, batch in enumerate(dataloader):\n",
    "                    optimizer.zero_grad()\n",
    "                    input_ids = batch['input_ids'].to(llm.device)\n",
    "                    attention_mask = batch['input_att_mask'].to(llm.device)\n",
    "                    outputs = llm(input_ids=input_ids, labels=input_ids, attention_mask=attention_mask)\n",
    "                    self.fp_done(client_id, epoch, step, batch)  # Collect intermediate results\n",
    "                    loss = outputs.loss\n",
    "                    pbar.set_description(f'Client {client_id} Epoch {epoch} Loss {loss.item():.3f}')\n",
    "                    loss.backward()\n",
    "                    self.bp_done(client_id, epoch, step, batch)  # Collect gradients\n",
    "                    optimizer.step()\n",
    "                    pbar.update(1)\n",
    "                avg_rouge1 = sum([r[\"rouge-l\"][\"f\"] for r in self.attacker_rouge_b2tr]) / len(self.attacker_rouge_b2tr)\n",
    "                print(f'ATTACK! Bottom-trunk, Client {client_id} Epoch {epoch} RougeL {avg_rouge1:.3f}')\n",
    "                avg_rouge2 = sum([r['rouge-l']['f'] for r in self.attacker_rouge_tr2t]) / len(self.attacker_rouge_tr2t)\n",
    "                print(f'ATTACK! Trunk-Top, Client {client_id} Epoch {epoch} RougeL {avg_rouge2:.3f}')\n",
    "                self.client_logs.setdefault(client_id, {})\n",
    "                self.client_logs[client_id][epoch] = {\"bottom-trunk\": avg_rouge1, \"trunk-top\": avg_rouge2}\n",
    "                self.attacker_rouge_b2tr.clear()\n",
    "                self.attacker_rouge_tr2t.clear()\n",
    "\n",
    "    def aggregation_step(self, global_round, params):\n",
    "        report = {}\n",
    "        report['global_round'] = global_round\n",
    "        for cid, epochs in self.client_logs.items():\n",
    "            for epc, rep in epochs.items():\n",
    "                for k, v in rep.items():\n",
    "                    report[f'client{cid}-epoch{epc}-{k}'] = v\n",
    "        wandb.log(report)\n",
    "        print(report)\n",
    "        self.client_logs = {}\n",
    "        return super(QAFLStrategy, self).aggregation_step(global_round, params)\n",
    "\n",
    "    def callback_fp_param(self, client_id, local_epoch, local_step, b2tr_params, tr2t_params, batch):\n",
    "        #  这里获取某epoch、step中，前传过程的两次传输参数，b2tr(bottom-trunk), tr2t(trunk-top)\n",
    "        with torch.no_grad():\n",
    "            rouge_res_b2tr = calculate_rouge(tokenizer, attacker.search(b2tr_params, model), batch['input_text'],\n",
    "                                             is_tokens=True)\n",
    "            rouge_res_tr2t = calculate_rouge(tokenizer, attacker2.search(tr2t_params, model), batch['input_text'],\n",
    "                                             is_tokens=True)\n",
    "            self.attacker_rouge_b2tr.append(rouge_res_b2tr)\n",
    "            self.attacker_rouge_tr2t.append(rouge_res_tr2t)\n",
    "            print(\n",
    "                f'ATTACK! Bottom-trunk, Client {client_id} Epoch {local_epoch} Step {local_step} RougeL {rouge_res_b2tr[\"rouge-l\"][\"f\"]:.3f}')\n",
    "\n",
    "    def callback_bp_param(self, client_id, local_epoch, local_step, t2tr_params, tr2b_params, batch):\n",
    "        #  这里获取某epoch、step中，反传过程的两次传输参数\n",
    "        pass\n",
    "\n",
    "\n",
    "client_ids = [str(i) for i in range(3)]\n",
    "config = FLConfig(global_round=50,\n",
    "                  client_epoch=2,  # 每轮联邦每个Client训2轮\n",
    "                  split_point_1=2,\n",
    "                  split_point_2=10,  # [0,1 | 2,3,.... 29| 30, 31]\n",
    "                  use_lora_at_trunk=True,  # 在trunk部分使用LoRA\n",
    "                  top_and_bottom_from_scratch='False',  # top和bottom都不采用预训练参数.\n",
    "                  noise_mode=\"dxp\",\n",
    "                  noise_scale=5.0,  # 噪声大小\n",
    "                  )\n",
    "fed_dataset = PIQAFedDataset(tokenizer=tokenizer, client_ids=client_ids, shrink_frac=0.15)\n",
    "simulator = SFLSimulator(client_ids=client_ids, strategy=QAFLStrategy(), llm=model, tokenizer=tokenizer,\n",
    "                         dataset=fed_dataset, config=config)\n",
    "model.print_split_model()\n",
    "attacker.to(model.device)\n",
    "attacker2.to(model.device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3. 开始联邦模拟"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mstupidtree\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/project/SFL-LLM/experiments/notebook/wandb/run-20240111_160218-k0xsrxyq</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/stupidtree/sfl-with-attacker/runs/k0xsrxyq' target=\"_blank\">dainty-galaxy-128</a></strong> to <a href='https://wandb.ai/stupidtree/sfl-with-attacker' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/stupidtree/sfl-with-attacker' target=\"_blank\">https://wandb.ai/stupidtree/sfl-with-attacker</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/stupidtree/sfl-with-attacker/runs/k0xsrxyq' target=\"_blank\">https://wandb.ai/stupidtree/sfl-with-attacker/runs/k0xsrxyq</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================Global Round 0=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/94 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c9e0afd2e1b498f8b1789289085da9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 0 RougeL 0.955\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 1 RougeL 1.000\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 2 RougeL 1.000\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 3 RougeL 1.000\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 4 RougeL 1.000\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 5 RougeL 0.967\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 6 RougeL 0.986\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 7 RougeL 1.000\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 8 RougeL 1.000\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 9 RougeL 1.000\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 10 RougeL 1.000\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 11 RougeL 1.000\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 12 RougeL 0.974\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 13 RougeL 0.942\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 14 RougeL 0.939\n",
      "ATTACK! Bottom-trunk, Client 2 Epoch 0 Step 15 RougeL 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"sfl-with-attacker\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"dataset\": 'piqa',\n",
    "        \"attacker\": \"piqa-validation\",\n",
    "        \"noise\": \"2.0\"\n",
    "    }\n",
    ")\n",
    "\n",
    "simulator.simulate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sfl.utils.model import sentence_score\n",
    "\n",
    "sentence_score(\"I'm fine, thank you!\", model, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}