{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "from transformers import AutoTokenizer\n",
    "from sfl.model.gpt2.gpt2_split import GPT2SplitLMHeadModel\n",
    "\n",
    "cache_dir = '/root/autodl-tmp/sfl/models'  # 模型的缓存位置，需要修改\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\", cache_dir=cache_dir)\n",
    "model = GPT2SplitLMHeadModel.from_pretrained(\"gpt2-large\", cache_dir=cache_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = 50256"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi father,\n",
      "\n",
      "I'm sorry to hear about your son's death. I'm so sorry for your loss. My heart goes out to you and your family. Please know that I will do everything in my power to help you through this difficult time. You have my deepest sympathy and I hope that you will be able to find peace and comfort in the coming days and weeks. Thank you for everything you have done for me and my family over the years. God bless you.\n"
     ]
    }
   ],
   "source": [
    "# 测试模型的生成文本\n",
    "def generate(text, md=model):\n",
    "    model.train(False)\n",
    "    t = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    res = md.generate(t['input_ids'].to(md.device), attention_mask=t['attention_mask'].to(md.device),\n",
    "                      max_length=300, num_beams=6, no_repeat_ngram_size=2, early_stopping=True,\n",
    "                      num_return_sequences=1, pad_token_id=tokenizer.pad_token_id)\n",
    "    return tokenizer.decode(res[0], skip_special_tokens=True)\n",
    "\n",
    "# 测试模型输出\n",
    "def get_output(text, md=model):\n",
    "    t = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    res = model(t['input_ids'].to(md.device), attention_mask=t['attention_mask'].to(md.device))\n",
    "    r = tokenizer.decode(res.logits.argmax(dim=-1)[-1], skip_special_tokens=True)\n",
    "    return r\n",
    "\n",
    "\n",
    "print(generate(\"Hi father\", model))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "GPT2SplitLMHeadModel(\n  (transformer): GPT2SplitModel(\n    (wte): Embedding(50257, 1280)\n    (wpe): Embedding(1024, 1280)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-35): 36 x GPT2Block(\n        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2. 设置联邦训练流程"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sfl/lib/python3.11/site-packages/peft/tuners/lora.py:299: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sfl.simulator.simulator import SFLSimulator\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sfl.model.split_model import SplitModel\n",
    "from sfl.simulator.strategy import FLStrategy\n",
    "from sfl.simulator.dataset import PIQAFedDataset, FedDataset\n",
    "from sfl.utils import FLConfig\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "# 定义Client本地学习策略\n",
    "class QAFLStrategy(FLStrategy):\n",
    "\n",
    "    def client_step(self, client_id: str, llm: SplitModel, dataloader: DataLoader, cfg: FLConfig):\n",
    "        optimizer = AdamW(llm.parameters(), lr=1e-5)\n",
    "        with tqdm_notebook(total=cfg.client_epoch * len(dataloader)) as pbar:\n",
    "            for epoch in range(cfg.client_epoch):\n",
    "                for step, batch in enumerate(dataloader):\n",
    "                    optimizer.zero_grad()\n",
    "                    input_ids = batch['input_ids'].to(llm.device)\n",
    "                    attention_mask = batch['input_att_mask'].to(llm.device)\n",
    "                    outputs = llm(input_ids=input_ids, labels=input_ids, attention_mask=attention_mask)\n",
    "                    self.fp_done(client_id, epoch, step, batch)  # Collect intermediate results\n",
    "                    loss = outputs.loss\n",
    "                    pbar.set_description(f'Client {client_id} Epoch {epoch} Loss {loss.item():.3f}')\n",
    "                    loss.backward()\n",
    "                    self.bp_done(client_id, epoch, step, batch)  # Collect gradients\n",
    "                    # res_text = tokenizer.decode(outputs.logits.argmax(dim=-1)[-1], skip_special_tokens=True)\n",
    "                    # print(batch['input_text'][-1],\"==>\",res_text.strip(),\"】\")\n",
    "                    optimizer.step()\n",
    "                    pbar.update(1)\n",
    "\n",
    "    def callback_fp_param(self, client_id, local_epoch, local_step, b2tr_params, tr2t_params, batch):\n",
    "        #  这里获取某epoch、step中，前传过程的两次传输参数，b2tr(bottom-trunk), tr2t(trunk-top)\n",
    "        pass\n",
    "\n",
    "    def callback_bp_param(self, client_id, local_epoch, local_step, t2tr_params, tr2b_params, batch):\n",
    "        #  这里获取某epoch、step中，反传过程的两次传输参数\n",
    "        pass\n",
    "\n",
    "\n",
    "client_ids = [str(i) for i in range(3)]\n",
    "config = FLConfig(global_round=10, client_epoch=2, split_point_1=2, split_point_2=34, use_lora_at_trunk=True)\n",
    "fed_dataset = PIQAFedDataset(tokenizer=tokenizer, client_ids=client_ids)\n",
    "simulator = SFLSimulator(client_ids=client_ids, strategy=QAFLStrategy(), llm=model, tokenizer=tokenizer,\n",
    "                         dataset=fed_dataset, config=config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================Split-gpt2-large=================\n",
      "==================Top Layers==================\n",
      "\n",
      "transformer.h.34:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.35:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.ln_f.weight:[: (1280,)]\n",
      "\n",
      "transformer.ln_f.bias:[: (1280,)]\n",
      "==================Trunk Layers==================\n",
      "\n",
      "transformer.h.2:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.3:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.4:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.5:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.6:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.7:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.8:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.9:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.10:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.11:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.12:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.13:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.14:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.15:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.16:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.17:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.18:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.19:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.20:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.21:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.22:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.23:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.24:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.25:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.26:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.27:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.28:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.29:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.30:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.31:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.32:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "\n",
      "transformer.h.33:[attn.c_attn.lora_A.default.weight: (8, 1280), attn.c_attn.lora_B.default.weight: (3840, 8), attn.c_proj.lora_A.default.weight: (8, 1280), attn.c_proj.lora_B.default.weight: (1280, 8), mlp.c_fc.lora_A.default.weight: (8, 1280), mlp.c_fc.lora_B.default.weight: (5120, 8), mlp.c_proj.lora_A.default.weight: (8, 5120), mlp.c_proj.lora_B.default.weight: (1280, 8)]\n",
      "==================Bottom Layers==================\n",
      "\n",
      "transformer.wte.weight:[: (50257, 1280)]\n",
      "\n",
      "transformer.wpe.weight:[: (1024, 1280)]\n",
      "\n",
      "transformer.h.0:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "\n",
      "transformer.h.1:[ln_1.weight: (1280,), ln_1.bias: (1280,), attn.c_attn.weight: (1280, 3840), attn.c_attn.bias: (3840,), attn.c_proj.weight: (1280, 1280), attn.c_proj.bias: (1280,), ln_2.weight: (1280,), ln_2.bias: (1280,), mlp.c_fc.weight: (1280, 5120), mlp.c_fc.bias: (5120,), mlp.c_proj.weight: (5120, 1280), mlp.c_proj.bias: (1280,)]\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "model.print_split_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3. 开始联邦模拟"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================Global Round 0=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/462 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "21b1885b3e844b18a1f3bbc74cf85c1a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:4.51 GB, downlink:4.51 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/384 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bdfd9352d3f74e478ffe2ffcb772554a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:3.75 GB, downlink:3.75 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/256 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "039e6a11d0714696a92dec7223cc564d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:2.50 GB, downlink:2.50 GB\n",
      "Global Round 0 communication overhead: uplink=10.76 GB, downlink=10.76 GB\n",
      "==================================Global Round 1=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/384 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "36eab136796e4a6ca186cf3791caa987"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:3.75 GB, downlink:3.75 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/462 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "354e49b6533742f7a1baa88f008b3aa6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:4.51 GB, downlink:4.51 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/256 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6bf3346f2ecb4f0b88589781c06e6150"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:2.50 GB, downlink:2.50 GB\n",
      "Global Round 1 communication overhead: uplink=10.76 GB, downlink=10.76 GB\n",
      "==================================Global Round 2=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/462 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7efc0eb8149e4d4ea90b7a8ae95bac19"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:4.51 GB, downlink:4.51 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/256 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63972eb137e8496b8b0eae05b97b5592"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:2.50 GB, downlink:2.50 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/384 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "997d31345a0b4a23ba5a7b9585dca182"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:3.75 GB, downlink:3.75 GB\n",
      "Global Round 2 communication overhead: uplink=10.76 GB, downlink=10.76 GB\n",
      "==================================Global Round 3=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/384 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7442807a154a4b4388ac5a8800052a57"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:3.75 GB, downlink:3.75 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/462 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad17a83ff46a4f41a929bdc9815b120f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:4.51 GB, downlink:4.51 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/256 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7341529fa71741c99ff35b52da1f7043"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:2.50 GB, downlink:2.50 GB\n",
      "Global Round 3 communication overhead: uplink=10.76 GB, downlink=10.76 GB\n",
      "==================================Global Round 4=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/384 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4b9781415844776809b344edd0ed7a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:3.75 GB, downlink:3.75 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/462 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8eccbb7420a43e68eb0f6413707b93c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:4.51 GB, downlink:4.51 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/256 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c0c4e41d9c040b39034d8de540fd695"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:2.50 GB, downlink:2.50 GB\n",
      "Global Round 4 communication overhead: uplink=10.76 GB, downlink=10.76 GB\n",
      "==================================Global Round 5=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/462 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e52b991cbea4146ab83810ff5df3763"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:4.51 GB, downlink:4.51 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/384 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45da96692abf4add8e55609d867ba0eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:3.75 GB, downlink:3.75 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/256 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5195f3edb2fb45df9064ee1d782bb1a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:2.50 GB, downlink:2.50 GB\n",
      "Global Round 5 communication overhead: uplink=10.76 GB, downlink=10.76 GB\n",
      "==================================Global Round 6=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/462 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e5382d14d0343b1aae3993daecaf53d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:4.51 GB, downlink:4.51 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/256 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "511cf515d3e347c1899efda825e71818"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:2.50 GB, downlink:2.50 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/384 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b911c38d4b847659e7eb9c5789424cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:3.75 GB, downlink:3.75 GB\n",
      "Global Round 6 communication overhead: uplink=10.76 GB, downlink=10.76 GB\n",
      "==================================Global Round 7=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/462 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b7cbf58596241a2b497e26e41258dc1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:4.51 GB, downlink:4.51 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/256 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cae37975f333400a9a75b98246f4173f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:2.50 GB, downlink:2.50 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/384 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9797aff183a141068f3548efb1618b17"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:3.75 GB, downlink:3.75 GB\n",
      "Global Round 7 communication overhead: uplink=10.76 GB, downlink=10.76 GB\n",
      "==================================Global Round 8=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/256 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8e424b5c0bd49888f39400d5c9be0e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:2.50 GB, downlink:2.50 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/462 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf3f567b324a4a4cb049f43f66fd4588"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:4.51 GB, downlink:4.51 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/384 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3daf3ab23014cada153101c8d52718a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:3.75 GB, downlink:3.75 GB\n",
      "Global Round 8 communication overhead: uplink=10.76 GB, downlink=10.76 GB\n",
      "==================================Global Round 9=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/256 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "017f9aa6cfc846d5bf63a060fd789ca4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:2.50 GB, downlink:2.50 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/384 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "879ce3d7bb8849acac07c3ccf5ce904e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:3.75 GB, downlink:3.75 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/462 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "36a1848d8aa6412caa8d9165e732c89d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:4.51 GB, downlink:4.51 GB\n",
      "Global Round 9 communication overhead: uplink=10.76 GB, downlink=10.76 GB\n",
      "FL communication overhead: uplink=107.62 GB, downlink=107.62 GB\n"
     ]
    }
   ],
   "source": [
    "simulator.simulate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make paper out of woods, Solution:roll sheets of magazines up into a tube and glue it to a board.\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"To make paper out of woods\", model))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "[[0.25, 0.5, 0, 0, 0, 0.25],\n [0, 0, 0, 0, 0, 1],\n [0, 0.25, 0, 0.25, 0.5, 0],\n [0, 0, 0, 0, 1, 0],\n [0, 0, 0, 0.5, 0.5, 0],\n [0, 0, 1, 0, 0, 0]]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        , 0.        , 0.        , 0.33333333, 0.66666667,\n        0.        ],\n       [0.        , 0.        , 0.        , 0.33333333, 0.66666667,\n        0.        ],\n       [0.        , 0.        , 0.        , 0.33333333, 0.66666667,\n        0.        ],\n       [0.        , 0.        , 0.        , 0.33333333, 0.66666667,\n        0.        ],\n       [0.        , 0.        , 0.        , 0.33333333, 0.66666667,\n        0.        ],\n       [0.        , 0.        , 0.        , 0.33333333, 0.66666667,\n        0.        ]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_k = np.linalg.matrix_power(mat, 10000)\n",
    "mat_k"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}