{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1. 加载模型与Tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sfl.utils.training import get_best_gpu\n",
    "from transformers import AutoTokenizer\n",
    "from sfl.model.gpt2.gpt2_split import GPT2SplitLMHeadModel\n",
    "from sfl import config\n",
    "\n",
    "device = get_best_gpu()\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(config.model_download_dir, \"gpt2/\"), padding_side='left')\n",
    "model = GPT2SplitLMHeadModel.from_pretrained(os.path.join(config.model_download_dir, \"gpt2/\"))\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = 50256\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the the with with water, you can add\n",
      "To mix food coloring with sugar, you can also use it as a sweetener.\n",
      "\n",
      "If you want to add more sugar to the mix, add a little more water and mix well. If you add too much water, the mixture will be too thick, and you will end up with a mess. You can use a spoon to scoop out the excess water from the mixing bowl, but it's best to leave it at room temperature for at least 30 minutes before adding the rest of the water.\n"
     ]
    }
   ],
   "source": [
    "from sfl.utils.model import generate\n",
    "\n",
    "\n",
    "# 测试模型输出\n",
    "def get_output(text, md=model):\n",
    "    t = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    res = model(t['input_ids'].to(md.device), attention_mask=t['attention_mask'].to(md.device))\n",
    "    r = tokenizer.decode(res.logits.argmax(dim=-1)[-1], skip_special_tokens=True)\n",
    "    return r\n",
    "\n",
    "text = \"To mix food coloring with sugar, you can\"\n",
    "print(get_output(text))\n",
    "print(generate(text, tokenizer, model))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sfl.simulator.dataset import CodeAlpacaFedDataset, PIQAFedDataset, WikiTextFedDataset\n",
    "\n",
    "test_dataset = WikiTextFedDataset(tokenizer=tokenizer,client_ids=[])\n",
    "test_loader = test_dataset.get_dataloader_unsliced(1, 'test', shrink_frac=1.0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "dataset = CodeAlpacaFedDataset(tokenizer=tokenizer,client_ids=[])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(53.2021, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from sfl.utils.training import evaluate_perplexity\n",
    "# evaluate(model, tokenizer)\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sfl.utils.model import calculate_rouge\n",
    "import torch\n",
    "\n",
    "def evaluate_piqa(loader, model, tokenizer):\n",
    "    model.train(False)\n",
    "    rouge_1_f = 0\n",
    "    rouge_2_f = 0\n",
    "    rouge_l_f = 0\n",
    "    len = 0\n",
    "    for batch in tqdm_notebook(loader):\n",
    "        q = tokenizer(batch['q_text'], return_tensors='pt', padding=True, truncation=True)\n",
    "        input_ids = q['input_ids'].to(model.device)\n",
    "        attention_mask = q['attention_mask'].to(model.device)\n",
    "        result = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=25,\n",
    "                                pad_token_id=tokenizer.eos_token_id)\n",
    "        rouge = calculate_rouge(tokenizer, result, batch['input_text'], is_tokens=True)\n",
    "        rouge_1_f += rouge['rouge-1']['f']\n",
    "        rouge_2_f += rouge['rouge-2']['f']\n",
    "        rouge_l_f += rouge['rouge-l']['f']\n",
    "        len += 1\n",
    "    model.train(True)\n",
    "    return rouge_1_f / len, rouge_2_f / len, rouge_l_f / len\n",
    "\n",
    "def evaluate_loss(model, loader):\n",
    "    model.train(False)\n",
    "    ppl = 0\n",
    "    len = 0\n",
    "    for batch in loader:\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['input_att_mask'].to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            ppl += outputs.loss\n",
    "        len += 1\n",
    "    return ppl / len\n",
    "\n",
    "\n",
    "ppl= evaluate_perplexity(model, test_loader)\n",
    "# print('initial_test_result: ', ppl)\n",
    "# ppl = evaluate_loss(model,test_loader)\n",
    "print(ppl)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3. 设置联邦训练流程"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 97])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "torch.Size([2, 22])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n",
      "torch.Size([2, 94])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sfl/lib/python3.11/site-packages/peft/tuners/lora.py:299: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Iterator\n",
    "from sfl.utils.model import calculate_rouge\n",
    "from sfl.simulator.simulator import SFLSimulator\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sfl.model.split_model import SplitModel\n",
    "from sfl.simulator.strategy import FLStrategy\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from sfl.config import FLConfig\n",
    "\n",
    "client_ids = [str(i) for i in range(3)]\n",
    "config = FLConfig(collect_intermediates=True,\n",
    "                  global_round=4,\n",
    "                  client_evaluate_freq=25,\n",
    "                  client_epoch=1,  # 每轮联邦每个Client训2轮\n",
    "                  split_point_1=2,\n",
    "                  split_point_2=10,  # [0,1 | 2,3,.... 29| 30, 31]\n",
    "                  use_lora_at_trunk=True,  # 在trunk部分使用LoRA\n",
    "                  top_and_bottom_from_scratch='False',  # top和bottom都不采用预训练参数.\n",
    "                  noise_mode=\"none\",\n",
    "                  noise_scale=0.0,  # 噪声大小,\n",
    "                  batch_size=2,\n",
    "                  dataset_type='train'\n",
    "                  )\n",
    "\n",
    "fed_dataset = WikiTextFedDataset(tokenizer=tokenizer, client_ids=client_ids, shrink_frac=0.04)\n",
    "\n",
    "\n",
    "# mirror_loader = fed_dataset.get_dataloader_unsliced(2, 'validation', shrink_frac=0.01)\n",
    "\n",
    "\n",
    "# 定义Client本地学习策略\n",
    "class QAFLStrategy(FLStrategy):\n",
    "\n",
    "    def client_evaluate(self, global_round, client_id, log):\n",
    "        ppl = evaluate_perplexity(self.simulator.llm, test_loader)\n",
    "        log['test-ppl'] = ppl\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attacker_rouge_b2tr = []\n",
    "\n",
    "    def client_step(self, client_id: str, global_round, local_epoch, llm: SplitModel, iterator: Iterator,\n",
    "                    cfg: FLConfig):\n",
    "        optimizer = AdamW(llm.parameters(), lr=1e-5)\n",
    "        avg_rouge = 0\n",
    "        avg_rouge_pt = 0\n",
    "        avg_loss = 0\n",
    "        step_num = 0\n",
    "        with tqdm_notebook(total=cfg.client_steps) as pbar:\n",
    "            for step, batch in enumerate(iterator):\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch['input_ids'].to(llm.device)\n",
    "                attention_mask = batch['input_att_mask'].to(llm.device)\n",
    "                outputs = llm(input_ids=input_ids, labels=input_ids, attention_mask=attention_mask)\n",
    "                avg_rouge += calculate_rouge(tokenizer, outputs.logits, batch['input_text'])['rouge-l']['f']\n",
    "                avg_loss += outputs.loss.detach().cpu().item()\n",
    "                step_num += 1\n",
    "                outputs.loss.backward()\n",
    "                optimizer.step()\n",
    "                pbar.set_description(\n",
    "                    f'Client {client_id} Epoch {local_epoch} Step{self.simulator.get_current_step(client_id, step)}, Loss {outputs.loss.item():.3f}')\n",
    "                pbar.update(1)\n",
    "                self.step_done(client_id, step, batch, logs={\"self\": avg_rouge / step_num,\n",
    "                                                             \"loss\": float(\n",
    "                                                                 avg_loss) / step_num,\n",
    "                                                             \"self-pt\": avg_rouge_pt / step_num})  # Collect gradients\n",
    "            # real_fx = deepcopy(llm.get_bottom_to_trunk_fx())\n",
    "            # real_dfx = deepcopy(llm.get_top_to_trunk_grad())\n",
    "\n",
    "            #\n",
    "            # # def ccl(lm_logits, labels):\n",
    "            # #     seq_len = labels.size(-1)\n",
    "            # #     half_len = seq_len // 2\n",
    "            # #     odd = seq_len % 2\n",
    "            # #     shift_logits = lm_logits.contiguous()\n",
    "            # #     shift_labels = torch.concat([labels[..., 1:half_len],  # size = half_len - 1\n",
    "            # #                                  labels[..., :half_len + 1 + odd]],  # size = half_len+1+odd\n",
    "            # #                                 dim=-1).contiguous()\n",
    "            # #     return CrossEntropyLoss()(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            #\n",
    "            # # def mirror_attack(**kwargs):\n",
    "            # #     # freeze trunk\n",
    "            # #     for nm, param in llm.get_trunk_params():\n",
    "            # #         param.requires_grad = False\n",
    "            # #     opt_mrr = AdamW(llm.parameters(), lr=1e-5)\n",
    "            # #     with tqdm_notebook(total=500 * len(mirror_loader)) as pbar:\n",
    "            # #         for i in range(500):\n",
    "            # #             for step, size in enumerate(mirror_loader):\n",
    "            # #                 opt_mrr.zero_grad()\n",
    "            # #                 ii = size['input_ids'].to(llm.device)\n",
    "            # #                 am = size['input_att_mask'].to(llm.device)\n",
    "            # #                 outputs = llm(input_ids=ii, attention_mask=am)\n",
    "            # #                 loss = calc_shifted_loss(outputs.logits, ii)\n",
    "            # #                 loss.backward()\n",
    "            # #                 opt_mrr.step()\n",
    "            # #                 o2 = llm(input_ids=input_ids,attention_mask=attention_mask)\n",
    "            # #                 rouge = calculate_rouge(tokenizer,o2.logits,batch['input_text'])\n",
    "            # #                 pbar.set_description(f'MIRROR {loss.item():.3f}')\n",
    "            # #                 pbar.set_postfix(rouge=rouge['rouge-l']['f'])\n",
    "            # #                 pbar.update(1)\n",
    "            #\n",
    "            # def mirror_attack(**kwargs):\n",
    "            #     # freeze trunk\n",
    "            #     for nm, param in llm.get_trunk_params():\n",
    "            #         param.requires_grad = False\n",
    "            #     # opt_mrr = AdamW(llm.parameters(), lr=1e-5)\n",
    "            #     batch_size, seq_len = input_ids.shape[:2]\n",
    "            #     vocab_size = llm.config.vocab_size\n",
    "            #     gt = torch.softmax(torch.randn((batch_size, seq_len, vocab_size)).to(llm.device), dim=-1)\n",
    "            #     gt.requires_grad = True\n",
    "            #     tuning_params = [param for nm, param in llm.get_top_params() if param.requires_grad]\n",
    "            #     opt_mrr = torch.optim.AdamW([gt], lr=0.05, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.01)\n",
    "            #     opt_tmp = AdamW(tuning_params)\n",
    "            #     with tqdm_notebook(total=1000) as pbar2:\n",
    "            #         for i in range(1000):\n",
    "            #             opt_mrr.zero_grad()\n",
    "            #             opt_tmp.zero_grad()\n",
    "            #             outputs = llm(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            #             loss = calc_shifted_loss_logits(torch.softmax(outputs.logits,dim=-1), torch.softmax(gt,dim=-1))\n",
    "            #             # loss.backward()\n",
    "            #             fx = llm.get_bottom_to_trunk_fx(detach=False)\n",
    "            #             fx2 = llm.get_trunk_to_top_fx(detach=False)\n",
    "            #             fx_diff = 0\n",
    "            #             for f1,f2 in zip(real_fx.to(fx.device),fx):\n",
    "            #                 fx_diff += ((f1-f2)**2).sum()\n",
    "            #             dfx = torch.autograd.grad(loss, fx2, create_graph=True)\n",
    "            #             dfx_diff = 0\n",
    "            #             for f1,f2 in zip(real_dfx,dfx):\n",
    "            #                 f2 = f2.to(f1.device)\n",
    "            #                 dfx_diff += ((f1-f2)**2).sum() + torch.abs((f1 - f2)).sum()\n",
    "            #             tag_loss = dfx_diff\n",
    "            #             tag_loss.backward()\n",
    "            #             opt_mrr.step()\n",
    "            #             rouge = calculate_rouge(tokenizer, gt, batch['input_text'])\n",
    "            #             sent = tokenizer.decode(torch.argmax(gt[0], dim=-1))\n",
    "            #             pbar2.set_description(f'MIRROR {dfx_diff.item():.3f}')\n",
    "            #             pbar2.set_postfix(rouge=rouge['rouge-l']['f'],sent=sent)\n",
    "            #             pbar2.update(1)\n",
    "            #     for nm, param in llm.get_trunk_params():\n",
    "            #         param.requires_grad = True\n",
    "            #     x = generate(\"To mix food coloring with sugar, you can\", tokenizer, llm)\n",
    "            #     print(x)\n",
    "            #\n",
    "            # simulator.restored_run(mirror_attack, ['top', 'bottom'], 'mirror')\n",
    "            # outputs_pt = self.simulator.restored_forward('top', input_ids=input_ids, labels=input_ids,\n",
    "            #                                               attention_mask=attention_mask)\n",
    "            # avg_rouge_pt += calculate_rouge(tokenizer, outputs_pt.logits, batch['input_text'])['rouge-l']['f']\n",
    "    def callback_intermediate_result(self, global_round, client_id, local_epoch, local_step,\n",
    "                                     b2tr_fx, tr2b_grad,\n",
    "                                     tr2t_fx, t2tr_grad,\n",
    "                                     batch, logs):\n",
    "        pass\n",
    "\n",
    "\n",
    "simulator = SFLSimulator(client_ids=client_ids, strategy=QAFLStrategy(), llm=model, tokenizer=tokenizer,\n",
    "                         dataset=fed_dataset, config=config)\n",
    "# model.print_split_model()\n",
    "# simulator.simulate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3. 开始联邦模拟"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mstupidtree\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113008360068004, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2514943570384ea788b67475a1691942"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/home/project/SFL-LLM/experiments/notebook/wandb/run-20240131_185939-uviwnu0b</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/stupidtree/sfl-eval/runs/uviwnu0b' target=\"_blank\">gpt2-large-wikitext-ppl</a></strong> to <a href='https://wandb.ai/stupidtree/sfl-eval' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/stupidtree/sfl-eval' target=\"_blank\">https://wandb.ai/stupidtree/sfl-eval</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/stupidtree/sfl-eval/runs/uviwnu0b' target=\"_blank\">https://wandb.ai/stupidtree/sfl-eval/runs/uviwnu0b</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================Global Round 0=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "950cabf9b4724f809ce57485cfaf46ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:75.00 MB, downlink:75.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "206089b05d154dab8cb8647fd75f9c21"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:75.00 MB, downlink:75.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87b8a924fa284182a0d12c31de1abc71"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:75.00 MB, downlink:75.00 MB\n",
      "SERVER: AGGREGATION\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da52d9d5cbe0409d8eb54ac8311c2009"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:150.00 MB, downlink:150.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66112473b4bb4d3789e7408d3f74d2ba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:150.00 MB, downlink:150.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f19dcf83971944e3904ea4aecbe10e70"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:150.00 MB, downlink:150.00 MB\n",
      "SERVER: AGGREGATION\n",
      "Global Round 0 communication overhead: uplink=450.00 MB, downlink=450.00 MB\n",
      "SERVER: AGGREGATION\n",
      "==================================Global Round 1=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4752f8c746246178836f766b1cd691c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:75.00 MB, downlink:75.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9473c1b07de4e4da28e4c6560b3cf4b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:75.00 MB, downlink:75.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ef8e790140745e9992f96dc88c66ed5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:75.00 MB, downlink:75.00 MB\n",
      "SERVER: AGGREGATION\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "daba4c06e2824b4bb90eadd60b91dad6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:150.00 MB, downlink:150.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2de8e5c1d7a452685b9cdf7ebd03796"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:150.00 MB, downlink:150.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d38ee91784df4c12a1d32d6cae814473"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:150.00 MB, downlink:150.00 MB\n",
      "SERVER: AGGREGATION\n",
      "Global Round 1 communication overhead: uplink=450.00 MB, downlink=450.00 MB\n",
      "SERVER: AGGREGATION\n",
      "==================================Global Round 2=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "209dc1500bbe42ec82a3be002a6ade47"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:75.00 MB, downlink:75.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13d945b66604415c9e04e63a873652b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:75.00 MB, downlink:75.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aee379ea9fa64711bbc728bfdfd17b92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:75.00 MB, downlink:75.00 MB\n",
      "SERVER: AGGREGATION\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "01e1e04371c94bd09662c4dcabd4aaab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:150.00 MB, downlink:150.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50710a38504746ba8095ea4175a80d85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:150.00 MB, downlink:150.00 MB\n",
      "SERVER: AGGREGATION\n",
      "Global Round 2 communication overhead: uplink=375.00 MB, downlink=375.00 MB\n",
      "SERVER: AGGREGATION\n",
      "==================================Global Round 3=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9bb6e114d124f66b32d8d9bca015a70"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:75.00 MB, downlink:75.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b6f788124dca4fa6b9965f7139df6ca9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:75.00 MB, downlink:75.00 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d5ec1c8d2164b349108afafc68e4115"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:75.00 MB, downlink:75.00 MB\n",
      "SERVER: AGGREGATION\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f4d1c58a00149c2b97333d101687cec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:150.00 MB, downlink:150.00 MB\n",
      "SERVER: AGGREGATION\n",
      "Global Round 3 communication overhead: uplink=300.00 MB, downlink=300.00 MB\n",
      "SERVER: AGGREGATION\n",
      "FL communication overhead: uplink=1.54 GB, downlink=1.54 GB\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"sfl-eval\",\n",
    "    name=\"gpt2-large-wikitext-ppl\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"dataset\": 'code',\n",
    "        \"noise\": \"0.0\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# ppl = evaluate_perplexity(simulator.llm, test_loader)\n",
    "# report = {}\n",
    "# report['test-ppl'] = ppl\n",
    "# wandb.log(report)\n",
    "simulator.simulate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}