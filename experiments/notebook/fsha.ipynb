{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "from sfl.config import FLConfig\n",
    "from sfl.utils.exp import get_model_and_tokenizer\n",
    "import argparse\n",
    "\n",
    "config = FLConfig(\n",
    "    collect_intermediates=False,\n",
    "    global_round=10,\n",
    "    client_evaluate_freq=500,\n",
    "    client_epoch=1,  # 每轮联邦每个Client训2轮\n",
    "    split_point_1=6,\n",
    "    split_point_2=26,  # [0,1 | 2,3,.... 29| 30, 31]\n",
    "    use_lora_at_trunk=True,  # 在trunk部分使用LoRA\n",
    "    use_lora_at_top=True,\n",
    "    use_lora_at_bottom=True,\n",
    "    top_and_bottom_from_scratch='False',\n",
    "    attack_mode='b2tr',\n",
    "    client_steps=700\n",
    ")\n",
    "\n",
    "args = {\n",
    "    'dataset_train_frac': 1.0,\n",
    "    'dataset_test_frac': 0.1,\n",
    "    'dataset': 'piqa',\n",
    "    'model_name': 'gpt2-large',\n",
    "    'save_checkpoint': True,\n",
    "    'task_type': 'lm',\n",
    "    'attacker_freq': 10000,\n",
    "    'log_to_wandb': False,\n",
    "    'dataset_max_seq_len':-1,\n",
    "}\n",
    "# convert to namespace\n",
    "args = argparse.Namespace(**args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model, tokenizer = get_model_and_tokenizer(args.model_name)\n",
    "model.config_sfl(config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sfl/lib/python3.11/site-packages/datasets/load.py:926: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at /root/autodl-tmp/sfl/datasets/piqa/piqa.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Parameter 'function'=<function FedDataset._pre_process.<locals>.<lambda> at 0x7f8ccca905e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "from sfl.utils.exp import get_dataset\n",
    "from sfl.config import DRA_train_label, DRA_test_label\n",
    "\n",
    "dataset = get_dataset(args.dataset, tokenizer,client_ids=['0'],shrink_frac=0.08)\n",
    "pub_loader = dataset.get_dataloader_unsliced(16, DRA_train_label[args.dataset], args.dataset_train_frac)\n",
    "test_loader = dataset.get_dataloader_unsliced(16, DRA_test_label[args.dataset], args.dataset_test_frac)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "FSHAAttacker(\n  (f_inv): GRUDRAttacker(\n    (gru): GRU(1280, 256, batch_first=True)\n    (mlp): Linear(in_features=256, out_features=50257, bias=True)\n  )\n  (f): GRUDRAttacker(\n    (gru): GRU(50257, 256, batch_first=True)\n    (mlp): Linear(in_features=256, out_features=1280, bias=True)\n  )\n  (d): GRU(1280, 256, batch_first=True)\n  (d_mlp): Sequential(\n    (0): Linear(in_features=256, out_features=1, bias=True)\n  )\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sfl.utils.model import get_best_gpu\n",
    "from sfl.model.attacker.fsha_attacker import FSHAAttacker, AutoEncoderConfig\n",
    "\n",
    "device = get_best_gpu()\n",
    "# model.to(device)\n",
    "attacker = FSHAAttacker(AutoEncoderConfig(), target_config=model.config)\n",
    "attacker.to(model.device)\n",
    "# attacker.fit_auto_encoder(model, tokenizer,pub_loader,test_loader, 50, args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sfl/lib/python3.11/site-packages/peft/tuners/lora.py:299: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================Global Round 0=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_474563/2037224677.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  with tqdm_notebook(total=config.client_steps) as pbar:\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/700 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0dd476f7182947b8b9e0f32c5c20bbaf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 62])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 111\u001B[0m\n\u001B[1;32m    104\u001B[0m attacker\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m    105\u001B[0m simulator \u001B[38;5;241m=\u001B[39m SFLSimulator(client_ids\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m0\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m    106\u001B[0m                          strategy\u001B[38;5;241m=\u001B[39mFSHAStrategy(args, model, tokenizer, attacker, pub_loader),\n\u001B[1;32m    107\u001B[0m                          llm\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m    108\u001B[0m                          tokenizer\u001B[38;5;241m=\u001B[39mtokenizer,\n\u001B[1;32m    109\u001B[0m                          dataset\u001B[38;5;241m=\u001B[39mdataset, config\u001B[38;5;241m=\u001B[39mconfig, args\u001B[38;5;241m=\u001B[39margs)\n\u001B[0;32m--> 111\u001B[0m simulator\u001B[38;5;241m.\u001B[39msimulate()\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/simulator/simulator.py:168\u001B[0m, in \u001B[0;36mSFLSimulator.simulate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    166\u001B[0m iters\u001B[38;5;241m.\u001B[39msetdefault(client_id, [\u001B[38;5;28miter\u001B[39m(loaders[client_id])])\n\u001B[1;32m    167\u001B[0m itt \u001B[38;5;241m=\u001B[39m CircularDataLoaderIterator(iters[client_id], loaders[client_id], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mclient_steps)\n\u001B[0;32m--> 168\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client_step(client_id, i, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlocal_epochs[client_id], itt)\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlocal_steps[client_id] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m itt\u001B[38;5;241m.\u001B[39miterated_num\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mglobal_steps[client_id] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m itt\u001B[38;5;241m.\u001B[39miterated_num\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/simulator/simulator.py:260\u001B[0m, in \u001B[0;36mSFLSimulator._client_step\u001B[0;34m(self, client_id, global_round, local_epoch, iterator)\u001B[0m\n\u001B[1;32m    258\u001B[0m \u001B[38;5;66;03m# client step\u001B[39;00m\n\u001B[1;32m    259\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n\u001B[0;32m--> 260\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mclient_step(client_id, global_round, local_epoch, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm, iterator, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig)\n\u001B[1;32m    261\u001B[0m \u001B[38;5;66;03m# store updated client parameters\u001B[39;00m\n\u001B[1;32m    262\u001B[0m cm \u001B[38;5;241m=\u001B[39m ([p\u001B[38;5;241m.\u001B[39mcpu() \u001B[38;5;28;01mfor\u001B[39;00m nm, p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mget_top_params()],\n\u001B[1;32m    263\u001B[0m       [p\u001B[38;5;241m.\u001B[39mcpu() \u001B[38;5;28;01mfor\u001B[39;00m nm, p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mget_bottom_params()])\n",
      "Cell \u001B[0;32mIn[5], line 51\u001B[0m, in \u001B[0;36mFSHAStrategy.client_step\u001B[0;34m(self, client_id, global_round, client_epoch, llm, iterator, config)\u001B[0m\n\u001B[1;32m     49\u001B[0m x_pub \u001B[38;5;241m=\u001B[39m x_pub[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(llm\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28mprint\u001B[39m(x_pub\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m---> 51\u001B[0m z_pub \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattacker\u001B[38;5;241m.\u001B[39mf_forward(x_pub)\n\u001B[1;32m     52\u001B[0m adv_priv_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattacker\u001B[38;5;241m.\u001B[39md_forward(z_priv)\n\u001B[1;32m     53\u001B[0m adv_pub_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattacker\u001B[38;5;241m.\u001B[39md_forward(z_pub)\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/model/attacker/fsha_attacker.py:97\u001B[0m, in \u001B[0;36mFSHAAttacker.f_forward\u001B[0;34m(self, input_ids)\u001B[0m\n\u001B[1;32m     95\u001B[0m input_ids\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     96\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mone_hot(input_ids, num_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mvocab_size)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m---> 97\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf(input_ids)\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/model/attacker/dra_attacker.py:176\u001B[0m, in \u001B[0;36mGRUDRAttacker.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m--> 176\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mforward(x)\n\u001B[1;32m    177\u001B[0m     \u001B[38;5;66;03m# x[batch_size, seq_len, n_embed]\u001B[39;00m\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;66;03m# output [batch_size,seq_len, vocab_size]\u001B[39;00m\n\u001B[1;32m    179\u001B[0m     hidden, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgru(x)  \u001B[38;5;66;03m# hidden [batch_size, seq_len, n_embed]\u001B[39;00m\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/model/attacker/dra_attacker.py:41\u001B[0m, in \u001B[0;36mDRAttacker.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m---> 41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchatglm\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mtarget_model:\n\u001B[1;32m     42\u001B[0m         x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m torch\u001B[38;5;241m.\u001B[39mfloat16:\n",
      "\u001B[0;31mTypeError\u001B[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from sfl.simulator.simulator import SFLSimulator\n",
    "from sfl.utils.model import get_t5_input, calc_unshift_loss\n",
    "from sfl.model.attacker.fsha_attacker import FSHAAttacker\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Iterator\n",
    "from sfl.model.llm.split_model import SplitWrapperModel\n",
    "from sfl.simulator.strategy import BaseSFLStrategy\n",
    "from torch.optim import Adam\n",
    "from sfl.utils.model import calculate_rouge\n",
    "from tqdm import  tqdm_notebook\n",
    "\n",
    "\n",
    "class FSHAStrategy(BaseSFLStrategy):\n",
    "\n",
    "    def __init__(self, args, llm, tokenizer, attacker: FSHAAttacker, pub_loader: DataLoader):\n",
    "        super().__init__(args, llm, tokenizer)\n",
    "        self.attacker = attacker\n",
    "        self.pub_loader = pub_loader\n",
    "        self.pub_loader_iter = iter(pub_loader)\n",
    "        self.optim_d = Adam(list(self.attacker.d_mlp.parameters())+list(self.attacker.d.parameters()),lr=1e-5, weight_decay=1e-6)\n",
    "        self.optim_f = Adam(list(self.attacker.f.parameters())+list(self.attacker.f_inv.parameters()),lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "    def client_step(self, client_id: str, global_round, client_epoch, llm: SplitWrapperModel, iterator: Iterator,\n",
    "                    config: FLConfig):\n",
    "        optimizer = Adam([p for _, p in llm.get_top_params()], lr=5e-7, weight_decay=1e-7)\n",
    "        # optimizer = AdamW([p for _, p in llm.get_top_params()], lr=3e-7, weight_decay=1e-4)\n",
    "        avg_d_loss = 0\n",
    "        avg_f_loss = 0\n",
    "        avg_rouge_lf = 0\n",
    "        batch_num = 0\n",
    "        with tqdm_notebook(total=config.client_steps) as pbar:\n",
    "            for step, batch in enumerate(iterator):\n",
    "                if llm.type == 'encoder-decoder':\n",
    "                    outputs = llm(**get_t5_input(batch, self.tokenizer, llm.device))\n",
    "                else:\n",
    "                    input_ids = batch['input_ids'].to(llm.device)\n",
    "                    attention_mask = batch['input_att_mask'].to(llm.device)\n",
    "                    labels = input_ids\n",
    "                    if 'labels' in batch and self.task_type == 'clsf':\n",
    "                        labels = batch['labels'].to(llm.device)\n",
    "                    outputs = llm(input_ids=input_ids, labels=labels, attention_mask=attention_mask)\n",
    "                z_priv = outputs\n",
    "                try:\n",
    "                    x_pub = next(self.pub_loader_iter)\n",
    "                except StopIteration:\n",
    "                    self.pub_loader_iter = iter(self.pub_loader)\n",
    "                    x_pub = next(self.pub_loader_iter)\n",
    "                x_pub = x_pub['input_ids'].to(llm.device)\n",
    "                print(x_pub.shape)\n",
    "                z_pub = self.attacker.f_forward(x_pub)\n",
    "                adv_priv_logits = self.attacker.d_forward(z_priv)\n",
    "                adv_pub_logits = self.attacker.d_forward(z_pub)\n",
    "                # print('pub', adv_pub_logits, 'priv', adv_priv_logits)\n",
    "\n",
    "                # f_loss = torch.mean(adv_priv_logits)\n",
    "                f_loss = torch.mean(\n",
    "                    torch.binary_cross_entropy_with_logits(adv_priv_logits, torch.ones_like(adv_priv_logits)))\n",
    "\n",
    "                d_loss_true = torch.mean(\n",
    "                    torch.binary_cross_entropy_with_logits(adv_pub_logits, torch.ones_like(adv_pub_logits)\n",
    "                                                           ))\n",
    "                d_loss_fake = torch.mean(\n",
    "                    torch.binary_cross_entropy_with_logits(adv_priv_logits, torch.zeros_like(adv_priv_logits)))\n",
    "                d_loss = (d_loss_true + d_loss_fake) / 2\n",
    "                # d_loss_true = torch.mean(adv_pub_logits)\n",
    "                # d_loss_fake = -torch.mean(adv_priv_logits)\n",
    "                # # print(d_loss_true, d_loss_fake)\n",
    "                # d_loss = d_loss_true + d_loss_fake\n",
    "                rec_x_pub = self.attacker.f_inv_forward(z_pub)\n",
    "                inv_loss = calc_unshift_loss(rec_x_pub, x_pub)\n",
    "\n",
    "                rec_x_priv = self.attacker.f_inv_forward(z_priv)\n",
    "                recover_rouge = calculate_rouge(self.tokenizer, rec_x_priv, batch['input_text'])\n",
    "                avg_rouge_lf += recover_rouge['rouge-l']['f']\n",
    "\n",
    "                # (d_loss+f_loss).backward()\n",
    "                self.optim_d.zero_grad()\n",
    "                self.optim_f.zero_grad()\n",
    "                (inv_loss+d_loss).backward(retain_graph=True)\n",
    "                self.optim_d.step()\n",
    "                self.optim_f.step()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                f_grad = torch.autograd.grad(f_loss, z_priv)[0]\n",
    "                z_priv.backward(f_grad)\n",
    "                optimizer.step()\n",
    "                # optimizer.step()\n",
    "\n",
    "                batch_num += 1\n",
    "                avg_d_loss += d_loss.detach().cpu().item()\n",
    "                avg_f_loss += f_loss.detach().cpu().item()\n",
    "                pbar.set_description(\n",
    "                    f'Client {client_id} HIJACK Epoch {client_epoch} Step {self.simulator.get_current_step(client_id, step)} D_Loss {d_loss.item():.3f}, F_Loss {f_loss.item():.3f}, Rouge_L_F {recover_rouge[\"rouge-l\"][\"f\"]:.3f}, Avg_Rouge_L_F {avg_rouge_lf / (step + 1):.3f}')\n",
    "                self.step_done(client_id, step, batch,\n",
    "                               {\"d_loss\": float(avg_d_loss / batch_num),\n",
    "                                \"f_loss\": float(avg_f_loss / batch_num),\n",
    "                                \"rouge_l_f\": float(avg_rouge_lf / batch_num),\n",
    "                                })\n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "attacker.to(model.device)\n",
    "attacker.train()\n",
    "simulator = SFLSimulator(client_ids=['0'],\n",
    "                         strategy=FSHAStrategy(args, model, tokenizer, attacker, pub_loader),\n",
    "                         llm=model,\n",
    "                         tokenizer=tokenizer,\n",
    "                         dataset=dataset, config=config, args=args)\n",
    "\n",
    "simulator.simulate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}