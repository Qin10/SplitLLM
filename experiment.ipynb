{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1. 加载模型与Tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sfl.model.gpt2.gpt2_split import GPT2SplitLMHeadModel\n",
    "\n",
    "cache_dir = '/root/autodl-tmp/sfl/models' # 模型的缓存位置，需要修改\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", cache_dir=cache_dir)\n",
    "model = GPT2SplitLMHeadModel.from_pretrained(\"gpt2\", cache_dir=cache_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = 50256"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning! Thank you so much for taking the time to answer my questions!\n",
      "\n",
      "Thank you for your time. I hope you enjoyed this post. It's been a long time coming, but I'm happy to share it with you all. Here's what I've been up to so far, and if you have any questions, feel free to let me know in the comments below.\n"
     ]
    }
   ],
   "source": [
    "# 测试模型输出\n",
    "t = tokenizer(\"Good morning!\", return_tensors=\"pt\", add_special_tokens=False)\n",
    "res = model.generate(t['input_ids'],attention_mask=t['attention_mask'], max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True, num_return_sequences=1,pad_token_id=tokenizer.pad_token_id)\n",
    "print(tokenizer.decode(res[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2. 设置联邦训练流程"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from sfl.simulator.simulator import SFLSimulator\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sfl.model.split_model import SplitModel\n",
    "from sfl.simulator.strategy import FLStrategy\n",
    "from sfl.simulator.dataset import PIQAFedDataset, FedDataset\n",
    "from sfl.utils import FLConfig\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "# 定义Client本地学习策略\n",
    "class QAFLStrategy(FLStrategy):\n",
    "\n",
    "    def client_step(self, client_id: str, llm: SplitModel, dataloader: DataLoader, cfg: FLConfig):\n",
    "        optimizer = AdamW(llm.parameters(), lr=1e-5)\n",
    "        with tqdm_notebook(total=cfg.client_epoch * len(dataloader)) as pbar:\n",
    "            for epoch in range(cfg.client_epoch):\n",
    "                for step, batch in enumerate(dataloader):\n",
    "                    optimizer.zero_grad()\n",
    "                    input_ids, labels = batch['input_ids'].to(llm.device), batch['output_ids'].to(llm.device)\n",
    "                    attention_mask = batch['input_att_mask'].to(llm.device)\n",
    "                    outputs = llm(input_ids=input_ids, labels=labels, attention_mask=attention_mask)\n",
    "                    self.fp_done(client_id, epoch, step)  # Collect intermediate results\n",
    "                    loss = outputs.loss\n",
    "                    pbar.set_description(f'Client {client_id} Epoch {epoch} Loss {loss.item():.3f}')\n",
    "                    loss.backward()\n",
    "                    self.bp_done(client_id, epoch, step)  # Collect gradients\n",
    "                    optimizer.step()\n",
    "                    pbar.update(1)\n",
    "\n",
    "    def callback_fp_param(self, client_id, local_epoch, local_step, b2tr_params, tr2t_params):\n",
    "        #  这里获取某epoch、step中，前传过程的两次传输参数，b2tr(bottom-trunk), tr2t(trunk-top)\n",
    "        pass\n",
    "\n",
    "    def callback_bp_param(self, client_id, local_epoch, local_step, t2tr_params, tr2b_params):\n",
    "        #  这里获取某epoch、step中，反传过程的两次传输参数\n",
    "        pass\n",
    "\n",
    "\n",
    "client_ids = [str(i) for i in range(3)]\n",
    "config = FLConfig(global_round=10, client_epoch=2,split_point_1=2,split_point_2=10)\n",
    "fed_dataset = PIQAFedDataset(tokenizer=tokenizer, client_ids=client_ids)\n",
    "simulator = SFLSimulator(client_ids=client_ids, strategy=QAFLStrategy(), llm=model, tokenizer=tokenizer,\n",
    "                        dataset=fed_dataset, config=config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================Split-gpt2=================\n",
      "==================Top Layers==================\n",
      "\n",
      "transformer.h.10:[ln_1.weight: (768,), ln_1.bias: (768,), attn.c_attn.weight: (768, 2304), attn.c_attn.bias: (2304,), attn.c_proj.weight: (768, 768), attn.c_proj.bias: (768,), ln_2.weight: (768,), ln_2.bias: (768,), mlp.c_fc.weight: (768, 3072), mlp.c_fc.bias: (3072,), mlp.c_proj.weight: (3072, 768), mlp.c_proj.bias: (768,)]\n",
      "\n",
      "transformer.h.11:[ln_1.weight: (768,), ln_1.bias: (768,), attn.c_attn.weight: (768, 2304), attn.c_attn.bias: (2304,), attn.c_proj.weight: (768, 768), attn.c_proj.bias: (768,), ln_2.weight: (768,), ln_2.bias: (768,), mlp.c_fc.weight: (768, 3072), mlp.c_fc.bias: (3072,), mlp.c_proj.weight: (3072, 768), mlp.c_proj.bias: (768,)]\n",
      "\n",
      "transformer.ln_f.weight:[: (768,)]\n",
      "\n",
      "transformer.ln_f.bias:[: (768,)]\n",
      "==================Trunk Layers==================\n",
      "\n",
      "transformer.h.2:[ln_1.weight: (768,), ln_1.bias: (768,), attn.c_attn.weight: (768, 2304), attn.c_attn.bias: (2304,), attn.c_proj.weight: (768, 768), attn.c_proj.bias: (768,), ln_2.weight: (768,), ln_2.bias: (768,), mlp.c_fc.weight: (768, 3072), mlp.c_fc.bias: (3072,), mlp.c_proj.weight: (3072, 768), mlp.c_proj.bias: (768,)]\n",
      "\n",
      "transformer.h.3:[ln_1.weight: (768,), ln_1.bias: (768,), attn.c_attn.weight: (768, 2304), attn.c_attn.bias: (2304,), attn.c_proj.weight: (768, 768), attn.c_proj.bias: (768,), ln_2.weight: (768,), ln_2.bias: (768,), mlp.c_fc.weight: (768, 3072), mlp.c_fc.bias: (3072,), mlp.c_proj.weight: (3072, 768), mlp.c_proj.bias: (768,)]\n",
      "\n",
      "transformer.h.4:[ln_1.weight: (768,), ln_1.bias: (768,), attn.c_attn.weight: (768, 2304), attn.c_attn.bias: (2304,), attn.c_proj.weight: (768, 768), attn.c_proj.bias: (768,), ln_2.weight: (768,), ln_2.bias: (768,), mlp.c_fc.weight: (768, 3072), mlp.c_fc.bias: (3072,), mlp.c_proj.weight: (3072, 768), mlp.c_proj.bias: (768,)]\n",
      "\n",
      "transformer.h.5:[ln_1.weight: (768,), ln_1.bias: (768,), attn.c_attn.weight: (768, 2304), attn.c_attn.bias: (2304,), attn.c_proj.weight: (768, 768), attn.c_proj.bias: (768,), ln_2.weight: (768,), ln_2.bias: (768,), mlp.c_fc.weight: (768, 3072), mlp.c_fc.bias: (3072,), mlp.c_proj.weight: (3072, 768), mlp.c_proj.bias: (768,)]\n",
      "\n",
      "transformer.h.6:[ln_1.weight: (768,), ln_1.bias: (768,), attn.c_attn.weight: (768, 2304), attn.c_attn.bias: (2304,), attn.c_proj.weight: (768, 768), attn.c_proj.bias: (768,), ln_2.weight: (768,), ln_2.bias: (768,), mlp.c_fc.weight: (768, 3072), mlp.c_fc.bias: (3072,), mlp.c_proj.weight: (3072, 768), mlp.c_proj.bias: (768,)]\n",
      "\n",
      "transformer.h.7:[ln_1.weight: (768,), ln_1.bias: (768,), attn.c_attn.weight: (768, 2304), attn.c_attn.bias: (2304,), attn.c_proj.weight: (768, 768), attn.c_proj.bias: (768,), ln_2.weight: (768,), ln_2.bias: (768,), mlp.c_fc.weight: (768, 3072), mlp.c_fc.bias: (3072,), mlp.c_proj.weight: (3072, 768), mlp.c_proj.bias: (768,)]\n",
      "\n",
      "transformer.h.8:[ln_1.weight: (768,), ln_1.bias: (768,), attn.c_attn.weight: (768, 2304), attn.c_attn.bias: (2304,), attn.c_proj.weight: (768, 768), attn.c_proj.bias: (768,), ln_2.weight: (768,), ln_2.bias: (768,), mlp.c_fc.weight: (768, 3072), mlp.c_fc.bias: (3072,), mlp.c_proj.weight: (3072, 768), mlp.c_proj.bias: (768,)]\n",
      "\n",
      "transformer.h.9:[ln_1.weight: (768,), ln_1.bias: (768,), attn.c_attn.weight: (768, 2304), attn.c_attn.bias: (2304,), attn.c_proj.weight: (768, 768), attn.c_proj.bias: (768,), ln_2.weight: (768,), ln_2.bias: (768,), mlp.c_fc.weight: (768, 3072), mlp.c_fc.bias: (3072,), mlp.c_proj.weight: (3072, 768), mlp.c_proj.bias: (768,)]\n",
      "==================Bottom Layers==================\n",
      "\n",
      "transformer.wte.weight:[: (50257, 768)]\n",
      "\n",
      "transformer.wpe.weight:[: (1024, 768)]\n",
      "\n",
      "transformer.h.0:[ln_1.weight: (768,), ln_1.bias: (768,), attn.c_attn.weight: (768, 2304), attn.c_attn.bias: (2304,), attn.c_proj.weight: (768, 768), attn.c_proj.bias: (768,), ln_2.weight: (768,), ln_2.bias: (768,), mlp.c_fc.weight: (768, 3072), mlp.c_fc.bias: (3072,), mlp.c_proj.weight: (3072, 768), mlp.c_proj.bias: (768,)]\n",
      "\n",
      "transformer.h.1:[ln_1.weight: (768,), ln_1.bias: (768,), attn.c_attn.weight: (768, 2304), attn.c_attn.bias: (2304,), attn.c_proj.weight: (768, 768), attn.c_proj.bias: (768,), ln_2.weight: (768,), ln_2.bias: (768,), mlp.c_fc.weight: (768, 3072), mlp.c_fc.bias: (3072,), mlp.c_proj.weight: (3072, 768), mlp.c_proj.bias: (768,)]\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "model.print_split_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3. 开始联邦模拟"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================Global Round 0=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/108 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "21a6099bec3a456985d96f1b13dfde62"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:336592896, downlink:336592896\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/112 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "edad7dd869f142d8aefbf610fbbf767a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:349175808, downlink:349175808\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/82 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c563422bf94c4743909facd323f852bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:257949696, downlink:257949696\n",
      "Global Round 0 communication overhead: uplink=943718400, downlink=943718400\n",
      "==================================Global Round 1=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/112 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0cd4c49648734f598bc16ae5dc9bb60d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:349175808, downlink:349175808\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/108 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83f6e486448b440f804e698831575b40"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:336592896, downlink:336592896\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/82 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "674746c90df44d7bb7d17960b1310b74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:257949696, downlink:257949696\n",
      "Global Round 1 communication overhead: uplink=943718400, downlink=943718400\n",
      "==================================Global Round 2=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/112 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "288dbe657d744c3a90d8969d1506f07f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:349175808, downlink:349175808\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/108 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7bb34706dc040988417b286e5e559aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:336592896, downlink:336592896\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/82 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c574700660564d83a2e9f47ce136f525"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:257949696, downlink:257949696\n",
      "Global Round 2 communication overhead: uplink=943718400, downlink=943718400\n",
      "==================================Global Round 3=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/112 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29d23260a0194a509615c8c658d42170"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:349175808, downlink:349175808\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/82 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99a85d79f9084bdc8e49176095133e3d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:257949696, downlink:257949696\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/108 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2aa1906ec644e12963bd91bd96f689d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:336592896, downlink:336592896\n",
      "Global Round 3 communication overhead: uplink=943718400, downlink=943718400\n",
      "==================================Global Round 4=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/112 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "818414bcdb9e427e8643f766c3992726"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 communication overhead: uplink:349175808, downlink:349175808\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/108 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aad07b3da3e34ef58683ac89923af416"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:336592896, downlink:336592896\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/82 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d77372f30464268a9bc5ee546795c57"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 2 communication overhead: uplink:257949696, downlink:257949696\n",
      "Global Round 4 communication overhead: uplink=943718400, downlink=943718400\n",
      "==================================Global Round 5=================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/108 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7e7bab884eb4e3f9c2feac0f4bb1ca7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 communication overhead: uplink:336592896, downlink:336592896\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/112 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "12b139a23c5f4a6eaf6fa932999d8e0f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m simulator\u001B[38;5;241m.\u001B[39msimulate()\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/simulator/simulator.py:49\u001B[0m, in \u001B[0;36mSFLSimulator.simulate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;66;03m# sequentially train each client\u001B[39;00m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m client_id \u001B[38;5;129;01min\u001B[39;00m sampled_clients:\n\u001B[0;32m---> 49\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client_step(client_id)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__summarize_communication(i, client_id)\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__summarize_communication(i, client_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m/home/project/SFL-LLM/sfl/simulator/simulator.py:66\u001B[0m, in \u001B[0;36mSFLSimulator._client_step\u001B[0;34m(self, client_id)\u001B[0m\n\u001B[1;32m     64\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m---> 66\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mclient_step(client_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mget_dataloader(client_id), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig)\n\u001B[1;32m     67\u001B[0m \u001B[38;5;66;03m# store updated client parameters\u001B[39;00m\n\u001B[1;32m     68\u001B[0m cm \u001B[38;5;241m=\u001B[39m ([p\u001B[38;5;241m.\u001B[39mcpu() \u001B[38;5;28;01mfor\u001B[39;00m nm, p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mget_top_params()],\n\u001B[1;32m     69\u001B[0m       [p\u001B[38;5;241m.\u001B[39mcpu() \u001B[38;5;28;01mfor\u001B[39;00m nm, p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mget_bottom_params()])\n",
      "Cell \u001B[0;32mIn[3], line 26\u001B[0m, in \u001B[0;36mQAFLStrategy.client_step\u001B[0;34m(self, client_id, llm, dataloader, cfg)\u001B[0m\n\u001B[1;32m     24\u001B[0m loss \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mloss\n\u001B[1;32m     25\u001B[0m pbar\u001B[38;5;241m.\u001B[39mset_description(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mClient \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclient_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 26\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbp_done(client_id, epoch, step)  \u001B[38;5;66;03m# Collect gradients\u001B[39;00m\n\u001B[1;32m     28\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[1;32m    488\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[1;32m    489\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/sfl/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    201\u001B[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001B[1;32m    202\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "simulator.simulate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}